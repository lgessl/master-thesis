\documentclass[10pt, aspectratio=169]{beamer}

\usetheme{metropolis}
\definecolor{alertc}{HTML}{008cd9}
\definecolor{myDarkTeal}{HTML}{1c2d30}
\setbeamercolor{alerted text}{fg=alertc}
\setbeamercolor{normal text}{fg=myDarkTeal,bg=black!2}
\metroset{block=fill}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}m{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}m{#1}}

\usepackage{siunitx}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\usepackage{amsmath}
\def\RR{\mathbb{R}}
\def\Pr{\mathrm{Pr}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{numbers,square}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}

\definecolor{mDarkTeal}{HTML}{23373b}
\definecolor{mLightBrown}{HTML}{EB811B}
\definecolor{mDarkBrown}{HTML}{B85002}
\definecolor{mLightGreen}{HTML}{14B03D}
\setbeamertemplate{itemize/enumerate subbody begin}{\normalsize}

\title{Beating the International Prognostic Index for high-risk DLBCL patients}
\subtitle{Master thesis final report}
\date{July 9th, 2024}
\author{Lukas Ge√ül}
\institute{Chair of Statistical Bioinformatics, Regensburg University}

\begin{document}

\maketitle

\section{Recap: The goal of this thesis}

\begin{frame}{}
  \begin{itemize}
    \item MMML-Predict: develop a cost-efficient classifier 
    that filters DLBCL patients with progression-free survival $\leq 2$ years more reliably 
    than the International Prognostic Index for non-Hodgkin's lymphoma (IPI).
    \pause
    \item The IPI \cite{ipi93} is a simple risk score (0--5) based on five clinical features. 
    The cohorts IPI $\geq i, i = 0, 1, \ldots, 5$, lack precision ($< 50 \%$) or 
    are too small to be clinically relevant (prevalence $< 10 \%$).
    \pause
    \item Our classifier should label at least 15\% of patients as high-risk with a precision
    of at least $\max(50\%, \text{precision of IPI} \geq 4)$.
    \pause
    \item Unlike the IPI, the new classifier can incorporate the whole range of 
    modern features (like transcriptomic, genetic, clinical data, already-existent 
    signatures) measured at diagnosis and even dynamic features measured during 
    the treatment.
  \end{itemize}
\end{frame}

\begin{frame}
  MMML-Predict will enroll 300 DLBCL patients in a prospective trial.
  \begin{itemize}
    \item Data for the first 200 patients \textit{will} arrive here and will be 
      our sole foundation to train classifiers and finally submit a single one.
    \item A group in Leipzig will test the submitted classifier on the remaining 
      100 patients.
  \end{itemize}
  \pause
  For this thesis, we also play by these rules, but on already existing data.
\end{frame}

\section{How to find and sell the best model}

\begin{frame}{A two-step approach}
  \begin{description}
    \item[Validation] Of those models we have trained, we want to find and choose
       the model that performs best on new data to the best of \alert{our} knowledge.
    \item[Testing] We need to demonstrate the performance of the chosen model to 
      \alert{outside} people on new, independent data.
  \end{description}

  \pause
  To this end, we split the data $(X, y)$ into a train cohort 
  $(X_{\text{train}}, y_{\text{train}})$ (also for validation) and test 
  cohort $(X_{\text{test}}, y_{\text{test}})$ (no more repeated splitting).
\end{frame}

\begin{frame}{Validation}
  We start with a set of tuples of hyperparameters $H$, where every $h \in H$ 
  defines a model up to its parameters.

  For every hyperparameter tuple $h \in H$, we
  \pause

  \begin{enumerate}
    \item fit the model to the train cohort 
    in a cross-validation, yielding a vector of cross-validated predictions 
    $\hat{y}_\text{train} = \text{cv}(h)$.
    \pause
    \item We use the cross-validated predictions to calculate the 
      cross-validated error $\text{err}(y_{\text{train}}, \hat{y}_\text{train})$.
  \end{enumerate}

  We select the model $m^*$ with hyperparameter tuple
  \begin{align*}
    h^* = \argmin_{h \in H} \ \text{err}(y_\text{train}, \text{cv}(h)).
  \end{align*}
\end{frame}

\begin{frame}{Testing}
  We calculate $m^*$'s predictions $m^*(X_\text{test}) = \hat{y}_{\text{test}}$ 
  on the test cohort and estimate its performance on independent data via
  \begin{align*}
    \text{err}(y_{\text{test}}, \hat{y}_{\text{test}}).
  \end{align*}

  \pause
  For our problem, we choose $\text{err}(y, \hat{y})$ as the minimum of the 
  negative precisions with a prevalence of at least 17\% (model output usually 
  needs thresholding).

  \pause
  Strictly speaking, the threshold for the model output is another hyperparameter, 
  but it is a platform-dependent one \citep{transplatform17}. On a new data set, 
  one might take the 17\% quantile of the model output as the threshold.
\end{frame}

\section{Let's talk about $H$: candidate models}

\begin{frame}{Model-agnostic hyperparameters \ldots}
  \ldots apply for every model. In our case, they concern the predictor matrix 
  $X \in \RR^{n \times p}$ and the response vector $y \in \{ 0, 1 \}^n \cup 
  (\RR \times \{0, 1 \})^n$.

  \begin{itemize}
    \item We add all combinations of at most $n_{\text{combi}}$ discrete features 
      that are positive in a share of at least $s_{\text{min}}$ patients to $X$; e.g. 
      we add a column ``female and ABC-type tumor'' if at least 5\% of patients 
      have this property.
    \pause
    \item For $T > 0$, we provide the fitting algorithm a modified reponse $y$, namely
      \begin{itemize}
        \item for the binary response, we set $y_i = 1$ if the patient's progression-free 
          survival is $< T$, $y_i = 0$ otherwise,
        \item for the Cox response, we censor all samples with time to event 
          exceeding $T$ at $T$.
      \end{itemize}
    \pause
    \item A-priori feature selection: which features do we include in $X$ in the 
      first place?
  \end{itemize}
\end{frame}

\begin{frame}{The most model-specific hyperparameter: model class}
  At the core, our models consist of 
  \begin{itemize}
    \item Cox proportional-hazards,
    \item logistic regression and 
    \item ordinary linear (or Gauss) regression 
  \end{itemize}
  models \cite{zerosumR}, \pause with a loss function optionally equipped with
  \begin{itemize}
    \item $\ell_1$ or $\ell_2$ regularization,
    \item the zero-sum constraint on a subset of features \cite{transplatform17},
    \item standardization of the predictor.
  \end{itemize}

  \pause
  Moreover, we deploy random forests \cite{ranger-gh}.
\end{frame}

\begin{frame}{Nested models}
  Given some ``early'' models $f_i: \RR^p \to \RR, i = 1, \ldots, m$, we can nest 
  them into another, ``late'' model $f: \RR^m \to \RR$ and get a new model 
  $f \circ (f_1, \ldots, f_m)$.

  \pause
  \begin{itemize}
    \item Often, the early models have been trained on another data set, so we 
      observe their output as features in our data set (like the Lamis 
      signature): such $f_i$ are merely projections onto a feature.
    \item If we need to fit some of the early models to our data, how can we 
      get reliable cross-validated predictions for $f$? See next slide.
  \end{itemize}
  \pause

  Typically, we train the early  model on the high-dimensional 
  part of the data (like gene expression) and use its output together with the 
  remaining features as input for the late model.
\end{frame}

\begin{frame}{}
  \begin{algorithm}[H]
    \caption{Nested pseudo cross validation} \label{alg:nested-pcv}
    \begin{algorithmic}[1]
      \State \textbf{Input:} Predictor matrix $X$, response $y$, 
        hyperparameter tuple $h = (h_1, h_2)$
      \State Fit $f_1$ to $(X; y)$ subject to $h_1$ in a $k$-fold cross-validation, 
        yielding cross-validated predictions $\hat{y}^{(1)}$.
      \State Fit $f$ to $(\hat{y}^{(1)}, f_2(X), f_3(X), \ldots, f_m(X); y)$ 
        subject to $h_2$ in a $k$-fold cross-validation, yielding 
        cross-validated predictions $\hat{y}$.
      \State $g \gets f \circ (f_1, \ldots, f_n)$
      \State \textbf{Output:} $(\hat{y}, g)$
    \end{algorithmic}
  \end{algorithm}

The \text{pseudo} cross-validated prediction for every sample in $\hat{y}$ slightly 
depends on the sample itself. Benefit: save a factor $k$ in time complexity.

Procede greedily (first tune $h_1$, then $h_2$) to avoid 
overfitting of cross-validated predictions to the training cohort.
\end{frame}

% \section{From theory to practice: software}

\begin{frame}{The R package patroklos}
  \begin{minipage}{0.15\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/logo.png}
  \end{minipage}
  \hfill
  \begin{minipage}{0.8\textwidth}
    \raggedright
    patroklos~\citep{patroklos} solves this and analogous problems with the 
    presented methods.
  \end{minipage}
\end{frame}

\section{How this plays out on real data}

\begin{frame}{Meet the data}
  \input{tables/data_sets.tex}
  \footnotetext[1]{High risk is defined as overall survival < 2.5 years.}
  \footnotetext[2]{All datasets include the IPI features in thresholded form, 
    gender, cell of origin, and the LAMIS signature.}
\end{frame}

\begin{frame}{Intra-trial: Validate and test on the same data set}
  \input{tables/intra_trial.tex}
\end{frame}

\begin{frame}{$m^*$'s architecture in a nutshell}
  \small
  \begin{columns}[T]
    \column{.3\textwidth}
      \textbf{Schmitz}\par
      Nested model as in Alg. \ref{alg:nested-pcv} with
      \begin{itemize}
        \item the early model (Gauss) trained on the 
            RNA-seq features,
        \item the late model (Cox) trained on the early model's output plus the 
          remaining features (IPI in all verions), $n_\text{combi} = 2$.
      \end{itemize}
    \column{.3\textwidth}
      \textbf{Reddy}\par
      Nested model as in Alg. \ref{alg:nested-pcv} with
      \begin{itemize}
        \item the early model (Gauss) trained on the 
            RNA-seq features,
        \item the late model (Cox) trained on the early model's output plus the 
          remaining features (five IPI features discretized), 
          $n_\text{combi} = 3$.
      \end{itemize}
    \column{.3\textwidth}
      \textbf{Lamis test}\par
      A logistic model trained on all features except for the NanoString gene 
      counts, $n_\text{combi} = 1$.
  \end{columns} 
\end{frame}

\begin{frame}{$m^*$ seems to the winner of a lottery: Schmitz}
  \begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{../../results/schmitz/meta_gauss-glm.jpeg}
  \end{figure}
\end{frame}

\begin{frame}{$m^*$ seems to the winner of a lottery: Reddy}
  \begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{../../results/reddy/meta_gauss-glm.jpeg}
  \end{figure}
\end{frame}

\begin{frame}{$m^*$ seems to the winner of a more predictable lottery: Lamis test}
  \begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{../../results/lamis_test2/meta_gauss-glm.jpeg}
  \end{figure}
\end{frame}

\begin{frame}{Inter-trial: Train and validate on one data set, test on another}
  \input{tables/inter_trial.tex}
\end{frame}

\begin{frame}{A closer look at $m^*$ for Reddy $\to$ Lamis test}
  We train a logistic model with $\ell_1$ penalty and standardization of 
  the predictor, for $T = \num{2.3}$ and 
  $n_\text{combi} = 2$, providing as features
  \begin{itemize}
    \item LAMIS score,
    \item cell of origin,
    \item IPI group: low (0--1), intermediate (2--3), high (4--5),
    \item the five thresholded IPI features,
    \item gender.
  \end{itemize}
\end{frame}

\begin{frame}{A closer look at $m^*$ for Reddy $\to$ Lamis test}
  \input{tables/signature_reddy_to_lamis.tex}
\end{frame}

\begin{frame}{A strong link between validation and test error: Reddy $\to$ Lamis test}
  \begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{../../results/all/reddy_vs_lamis.jpeg}
  \end{figure}
\end{frame}

\begin{frame}{A strong link between validation and test error: Lamis test $\to$ Schmitz}
  \begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{../../results/all/lamis_vs_schmitz.jpeg}
  \end{figure}
\end{frame}

\section{Conclusions and discussion}
\begin{frame}{Take aways}
  We wanted to deliver a classifier that defines a high-risk group of DLBCL 
  patients which is larger and more precise than that defined by the IPI.
  \begin{itemize}
    \item In intra-trial experiments, we could deliver on this promise for three 
      data sets. Inter-trial experiments worked even better.
    \pause
    \item While simple, $\ell_1$-penalized models predicting from 
      high-dimensional gene expression levels only sometimes already beat 
      the IPI, one usually needs to integrate more features.
    \pause
    \item Integrating \textit{already-existent} transcriptomic and genetic 
      signatures and the IPI features into another model reliably beats the IPI.
    \pause
    \item Transferring these models from one data set (and platform) to another
      works very well (especially Reddy $\to$ Lamis test). Apparently, the
      size of the data set matters most.
  \end{itemize}
\end{frame}

\begin{frame}{Discussion}
  \begin{itemize}
    \item \textbf{Validation:} Ensure a reliable link between validated and 
      tested performance. How? \pause
      \begin{itemize}
        \item Validating a smaller $H$ (proceding more greedily, relying on 
          prior, general knowledge).
        \item A refined cross validation following \citep{nested-cv-hastie} to 
          estimate the generalization error more reliably.
        \item Is our choice of $\text{err}$ too unstable? ROC-AUC isn't any 
          more stable.
        \item More samples.
      \end{itemize}
    \pause
    \item \textbf{Training:} Deploy other, more complex models in the 
      integration step like boosted trees or neural networks. Balance 
      classification problem via sample weights in loss function.
    \pause
    \item For MMML-Predict: rather more samples, less features.
  \end{itemize}
\end{frame}

\begin{frame}[standout]
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figs/pumuckl.png}
  \end{minipage}
  \hfill
  \begin{minipage}{0.50\textwidth}
      \centering
      Thank you! \par Questions?
  \end{minipage}
\end{frame}

\appendix

\begin{frame}[allowframebreaks]{References}
  \small
  \bibliography{lit}
  The ultra cute Pumuckl is taken from 
  \url{https://irp-cdn.multiscreensite.com/08191d67/dms3rep/multi/Pumuckl_Rennend.png}.
\end{frame}

\end{document}