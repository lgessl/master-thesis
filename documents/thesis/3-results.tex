\chapter{Results} \label{chap:results}

In this triune chapter, we start with introducing three DLBCL data sets that include 
survival and gene expression features. Next, in intra-trial experiments, we split every of these 
three data sets into 
a train and test cohort, fit models for a variety of hyperparameter tuples in $H$ to the training 
cohort, validate them and test the best on the test cohort; this is less about presenting a 
high-performing model, but more about analyzing validated and tested errors to make $H$ slimmer and 
better for the future. This future plays out in the last part as we train and validate on one of 
the three data sets and test on another and deal with cross-platform variability in inter-trial 
experiments.

\section{The data}

Our DLBCL data sets are taken from papers by Schmitz et al.\ \cite{schmitz18}, Reddy et al.\ 
\cite{reddy17} and Staiger et al.\ \cite{staiger20}; see Table \ref{table:data} for key properties 
and comparison. We will refer to them as Schmitz, Reddy and Lamis test data, respectively.

\input{figs/data.tex}

\paragraph{Schmitz data}
The data by Schmitz et al.\ includes the five IPI features in their continuous form. 
By heuristically optimizing a novel genetic distinctiveness metric, Schmitz et al.\ 
clustered 574 DLBCL biopsy samples into four subtypes, called MCD, BN2, N1, EZB and ``other''. They 
unblinded the clinical data only after the clustering was complete and the 
model was frozen \cite[Appendix 1, pp. 16--18]{schmitz18}. The following, independent survival analysis 
unveiled significantly differing progression-free survival between the four subtypes (excluding 
``other'') according to a logrank test; BN2 and EZB subtypes have far better prognosis than MCD and 
N1 (2-year PFS rate of \num{81}\% and \num{75}\% as opposed to \num{39}\% and 
\num{20}\%, respectively). The IPI score did not vary significantly between the subtypes, 
indicating that the new classifier gives us additional information to predict survival.

There are two caveats: First, the genetic classifier saw the entire data set during training and 
this runs afoul of a strict train-test regime even if training was 
survival-agnostic. This only affects the intra-trial experiments as in the inter-trial experiments 
we do not include the genetic subtype as a feature. As a more important take away, we always need 
to carefully use features in a data set that are the output of some model trained on this data set. 
Second, this data set is not the result of a 
prospective, representative trial, but opportunistically collected samples from highly renowned 
U.S. hospitals, which preferably treat 
difficult cases. As a result, the high-risk proportion in the Schmitz data is at \num{36.6}\% -- 
compared to \num{24.3}\% in the prospective Lamis test data -- and the IPI reaches a precision of 
\num{65.2}\% for classifying high-risk patients at a prevalence of \num{12.9}\% -- compared to 
\num{38.2}\% at \num{17.0}\% in the Lamis test data. The IPI already meets the MMML-Predict goals, 
but we want to see if we can do even better in a such high-risk regime.

\paragraph{Reddy data}

Compared to a prospective study, also the Reddy data is enriched for high-risk patients and the 
IPI boasts a performance that already satisfies the MMML-Predict goals, even if a bit less 
convincingly than on the Schmitz data. After identifying \num{150} DLBCL driver genes, Reddy et al.\ 
trained a Cox model, termed genomic risk model,
that predicts overall survival (OS) from combinations of genetic events and gene-expression markers 
(cell of origin, MYC, and BCL2) and thresholded it into low, intermediate and low risk. This is a
model whose predictions we cannot use because their split into the train and test cohort is not 
clear, but that inspired us to use combinations of discrete features, cf.\ subsection 
\ref{subsec:model-agnostic}. Nevertheless, with high expression and translocation of MYC, BCL2 and 
BCL6, the data provides some of the input features of the genomic risk model as well as three binary 
clinical features: B symptoms at diagnosis, testicular and central-nervous-system involvement. B 
symptoms refer to the presence of the triad fever, night sweats and unintential weight 
loss.

\paragraph{Lamis test data}

The Lamis test data is composed of \num{466} patients enrolled in prospective clinical trials. 
Staiger et al.\ first determined \num{731} gene pairs with highly correlated gene expression levels 
between their train cohort -- \num{233} DLBCLs with gene expression levels built from the Affymetrix 
GeneChip technology -- and the Lamis test data -- \num{466} DLBCLs with gene expression levels built 
from the NanoString nCounter technology -- with the aid of six paired nCounter-GeneChip samples, cf.\ 
\cite[Supplementary Methods]{staiger20}.
Next, they learned a Cox model on the differences of the (logarithmized) gene expression levels 
from these gene pairs and the five thresholded IPI features using LASSO regularization. 
Afterwards, they removed the five IPI features from the model aiming to make it independent of the 
IPI. One can expand the differences of gene expression levels in the signature to obtain an ordinary 
gene-expression signature with coefficients corresponding to single genes, the Lamis. It is 
based on \num{17} genes, but dominated by just two genes, CSF1 and CPT1A. 

By dichotomizing the 
Lamis scores at the 75\%-quantile into their Lamis group (low or high), Staiger et al.\ present two 
groups on the Lamis test data with significantly differing PFS and OS; 
meanwhile, the IPI features, breaks in MYC, BCL2, BCL6, and cell of origin remained prognostic 
indicators independently of the Lamis group.

Since the Lamis coefficients fulfill the 
zero-sum property, we apply the Lamis unchanged on the two other data sets; according to Eq.\ 
\eqref{eq:inter-tech}, the Lamis scores have a data-set-dependent shift. We also add the Lamis 
group by dichotomizing the Lamis scores at the 75\%-quantile of the respective data set.

\section{Intra-trial experiments}

To gain first insights on our methods, we conduct intra-trial experiments sepratetly on the 
Schmitz, Reddy and Lamis test data. To this end, we split every data set into a train and test 
cohort. We do so uniformly at random, with two constraints: first, a ratio of 3 to 1 between train 
and test cohort and, second, the ratio between high-risk and low-risk patients in train cohort, 
test cohort and overall data set is the same. As shown in Table \ref{table:intra-trial}, the 
performance of the IPI score thresholded at 4 to classify high risk notably differ between the whole
data set and the subsampled test cohort. 

\input{figs/intra_trial.tex}

\subsection{Model architectures}

We want to give a brief summary of the models we send into the race and take a closer look at the 
best one, $m^*$, on every data set.

\subsubsection{Candidates}

\paragraph{Gene expression levels only}
Models trained in a leave-one-out cross-validation only on the gene expression levels include the 
Gaussian, logistic and Cox model. Regarding noteworthy hyperparameter decisions, we both apply and 
do not apply standardization of the predictor; we do not demand the zero-sum constraint 
because we do not want to transfer our model to other data sets and want to safe computation time;
we regularize with elastic-net penalty factor $\alpha \in \{ \num{0.1}, 1 \}$; as throughout this 
chapter, we have the \texttt{zeroSum} package calculate a sequence of \num{100} regularization strengths 
$\lambda$ for us and stop early if the cross-validated error does not improve for \num{10} 
consecutive decreasing $\lambda$ values; we try out a whole bunch of time cutoffs $T$.

We train and validate a model for every combination of these hyperparameters.
Not taking into account the values of $\lambda$ (which are hard to foresee due to early stopping), 
this adds up to \num{88} models.

Looking at the models with top validated performance (cf. projection upon validation error in Fig. 
\ref{fig:intra-val-test-geo}), for the following we narrow down $T$ to one or 
two values (usually at or slighly below two years) and $\alpha$ to $1$ as the more complex models 
trained for $\alpha = \num{0.1}$ cannot clearly outperform the spare LASSO-trained models. For 
models only predicting from gene expression levels, we do not standardize the predictor any more.

\paragraph{Core models with other features}
We now add all available remaining features: for the 
IPI, we either add the five continuous features (if available), the five threholded features, the 
score as a continuous feature or all of these threee; for the Lamis, % here and for the rest of thesis 
we add the score as a continuous and the group as a discrete feature. For $s_\text{min} = \num{0.05}$ 
and $n_\text{combi} \in \{1, 2, 3, 4 \}$, we add combinations of discrete features to the predictor. 
We both include and exclude gene expression levels in the predictor.

As for the model class, we build on our experience gained from the gene-expression-only models and 
just use the top performing model class of these models for such models that also use gene 
expression levels. For models not predicting from gene-expression levels, we have no prior 
knowledge and thus deploy the full range of core models: Gaussian, logistic, Cox models and random 
forests. As all of these models need to deal with features on different scales, we always 
standardize the predictor.


\paragraph{Nested models}
We nest models according to Alg. \ref{alg:nested-pcv}, where the early model $f_1$ is the best 
validated model among the gene-expression-only models and $f$ has the model class of $f_1$ or is 
a random forest. If $f$ has the same model class as $f_1$, we still tune the LASSO regularization 
strengtht for $f_1$ because we may end up adding several hundreds combined disrete featuers to the 
predictor. As for a-priori feature selection, dito as above.

\subsubsection{Best validated models}

\paragraph{Schmitz and Reddy}
For both the Schmitz and Reddy data, the best model according to the validation error is a nested 
model as in Alg. \ref{alg:nested-pcv} with the early model $f_1$, a Gauss model, predicting from 
the gene-expression levels and the late model $f$ being a Cox model. They only differ in their 
features: For the Schmitz data, $X$ holds the IPI in all its three formats as opposed to the five 
discretized IPI features for the Reddy data. We have $n_\text{combi} = 2$ for Schmitz as opposed to 
$n_\text{combi} = 3$ for Reddy.

On the Schmitz data, $m^*$'s validated precision of \num{89.3}\% drops by more than \num{20} points 
to \num{68.4}\% on the test set. In an even bigger drop, $m^*$'s validated precision of \num{78.6}\% 
declines to \num{55.6}\% on the Reddy test set. These way too optimistic cross-validated errors 
demand further investigation.

\paragraph{Lamis test}
On the Lamis test data, $m^*$ is a very simple model: a logistic model that neither uses gene 
expression levels nor combinations of discrete features ($n_\text{combi} = 1$). Notably, the model
incorporates the Lamis score and group with decisively non-zero coefficient. Here, $m^*$'s 
validated precision of \num{58.2}\% is more in line with the \num{45.9}\% on the test set.

\subsection{Meta analysis}

Now that we have frozen all models and do not add further ones, we can unlock the test set and 
evaluate the test performance of more models. Our main focus rests on the discrepancy between 
validated and test error; as we laid out in section \ref{sec:train-val-test}, the models with the 
best validated errors most likely have an underestimated test error and it is hard to fight this 
effect qualitatively. However, we use several methods to calculate a validated error -- cross 
validation, the pseudo cross validatio of Alg. \ref{alg:nested-pcv} and the OOB error of random 
forests -- and for every model architecture we tune different hyperparameters and a different 
number of hyperparameter tuples. Both validation error and test error of a model are random 
variables and it is hard to infer statistical properties of them based on a single realization of 
these random variables; by grouping the models according to their hyperparameters we gain 
statisticial power and look for patterns in the validation and test error as well as 
discrepancies between them.

\subsubsection{Models predicting from gene expression levels only}

\input{figs/intra_val_test_geo.tex}

\paragraph{Lamis test}
Looking at Fig. \ref{fig:intra-val-test-geo}, we see that, for Lamis test, the models only using 
gene expression levels cannot compete with IPI $\geq 4$. While the validated errors of these models are 
below the \num{-36.4}\% negative precision of the IPI, the test errors all are above \num{-30}\%. 
In all cases, the test error is at least 10 points higher than the validated error, a discrepancy 
also indicated the fact that the identity line is outside the plot area. It is hard to imagine that 
these models even if nested into another model can help outperform the IPI. Hence, for the 
following, the analysis focuses on the other two data sets.

\paragraph{Schmitz and Reddy}
Also for the Schmitz and Reddy data, the dependence between validated and test error is far from 
being monotonic; in both cases, the plot looks noisy and the correlation between validated and test 
error is negative such that the model with the lowest validated error happens to have the highest 
(Schmitz) or second highest (Reddy) test error. The plots do not show errors for all tried out 
hyperparameter tuples in $H$, but only for a high-performing subset with $\lambda$ and $T$ already 
optimized; this optimization was successful in the sense that even the worst models boast a test 
error below that of IPI $\geq 4$.

There is no model class reliably outperforming the others. Both data sets suggest that ridge 
regularization yields models with a test error lower than that of LASSO regularization; validation, 
however, does not reveal this difference. The picture on standardizing $X$ or not is mixed: on the 
Schmitz data, models with non-standardized predictor fluctuate somewhat around the identiy line than 
those with standardized predictor, meaning validation is more reliable for the former model group;
on the Reddy data, meanwhile, we the same phenomenon with roles swapped.

\subsubsection{Models with the full range of features}

\input{figs/intra_val_test_more.tex}

All analysis here is based on Fig. \ref{fig:intra-val-test-more}.

\paragraph{Schmitz and Reddy}
Again, we analyze the non-prospective, high-risk heavy Schmitz and Reddy data together. As we apply
the full range of a priori selected features, validation and test error for these two data sets 
stay out of touch. Partioning according to the model architecture reveals some patterns. 

Models 
nesting a Gauss model into a GLM as in Alg. \ref{alg:nested-pcv} all have low validated errors, on 
the Reddy data, they even have lower validated error than any other model; the test error is 
always higher than the validated error and usually it is \textit{much} higher. Alg. \ref{alg:nested-pcv} 
yields way too optimistic validated errors if the late model is a GLM, prompting us to no longer 
fit such models in the following experiments. Discarding Alg. \ref{alg:nested-pcv} once and for all
would go too far: when nesting a Gauss model into a random forest, we get validated errors more in 
line with test errors, especially on the Schmitz data. On both data sets, OOB predictions for 
models consisting of a random forest alone very well estimate the test error. In all but one case 
they even underestimate the test error. All in all, OOB predictions seem superior to 
cross-validated predictions, which comes as no surprise when having a closer look at both 
methods: A sample's cross-validated predictions come from one model and we have one model per 
sample in the best case, a leave-one-out cross-validation (which we did for all models involved in 
nested models). In contrast, a sample's OOB prediction comes from a whole forest of models with 
expected size roughly $B/3$ because the probability of a sample not being in a bootrap sample 
is
\begin{align}
    \left( 1 - \frac{1}{n} \right)^n \to \frac{1}{e} \approx \frac{1}{3} \quad \text{as } n \to 
    \infty,
\end{align}
and we can scale it by scaling $B$, which is cheap as we have laid out in subsection 
\ref{subsec:elastic-net}.

As for feature selection, we see that including gene expression levels does not lead to greatly 
improved models. Even the high-performing gene-expression-only models do not benefit very much if 
we train them with additional features. On the Reddy data, the points belonging to models not using 
gene-expression data are all close to or underneath the identity line, meaning the validation errors 
catch the test errors well, their test errors are less widespread and on average lower than those 
of the remaining models. This does not mean that gene expression levels do not provide important 
information: all of the models that do not use the gene expression levels directly from our data 
do so indirectly thanks to the Lamis. We have simply outsourced developing a gene-expression based 
model to the Lamis authors, thereby getting rid of $p \gg n$ and ending up with a more accurate 
validation.

\paragraph{Lamis test}

On the Lamis test data, the validated errors align better with the test errros. As with the two other 
datasets, OOB-based test-error estimates are conservative leading to underestimated test errors 
for models incorporating a random forest. As a result of highly correlated validation and test 
errors, the best validated model is also the best tested model. Compared to above, the validated errors 
of models nesting a Cox model into a GLM overestimate the test errors less; since the early Cox 
model yields pretty inaccurate cross-validated predictions, we hypothesize that the training 
algorithm of the late GLM is less inclined to put as much weight on these predictions as with the 
two other data sets.

Models profit a lot from not just predicting from the gene expression levels: all the 
gene-expression-only cluster into a bulk with test errors higher than those of all other models. 
Key to the success is again the molecular information condensed in the Lamis, which all the 
high-performing models put heavy weight on.

\section{Inter-trial experiments}

The results of the intra-trial experiments are encouraging: we beat the precision of the IPI by 
at least \num{9.5} points on three data sets, including two where the IPI already did a good job. 
In the following analysis, we saw validated errors often being detached from test errors. To tackle 
this issue, we decided to no longer train certain models. We also noticed that the Lamis is at least 
as capable of catching the molecular information in the gene expression levels as the models
\textit{we} trained for that purpose, but allows training more precise models and validating them 
more accurately. Another way to close the validation-test gap is to increase the number of samples 
in both the train and test cohort: it both makes overfitting the validated predictions to the 
training cohort harder and makes it less likely that the test cohort includes cases biologically 
not covered at all in the train cohort. We will now go along this path as we use every of the 
three data sets in their entirety for training and validation and then test on the other two.

\input{figs/inter_trial.tex}