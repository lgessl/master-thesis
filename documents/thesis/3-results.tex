\chapter{Results} \label{chap:results}

In this triune chapter, we start with introducing three DLBCL data sets that include 
survival and gene expression features. Next, in intra-trial experiments, we split every of these 
three data sets into 
a train and test cohort, fit models for a variety of hyperparameter tuples in $H$ to the training 
cohort, validate them and test the best on the test cohort; this is less about presenting a 
high-performing model, but more about analyzing validated and tested errors to make $H$ slimmer and 
better for the future. This future plays out in the last part as we train and validate on one of 
the three data sets and test on another and deal with cross-platform variability in inter-trial 
experiments.

\section{The data}

Our DLBCL data sets are taken from papers by Schmitz et al.\ \cite{schmitz18}, Reddy et al.\ 
\cite{reddy17} and Staiger et al.\ \cite{staiger20}; see Table \ref{table:data} for key properties 
and comparison. We will refer to them as Schmitz, Reddy and Lamis test data, respectively.

\input{figs/data.tex}

\paragraph{Schmitz data}
The data by Schmitz et al.\ includes the five IPI features in their continuous form. 
By heuristically optimizing a novel genetic distinctiveness metric, Schmitz et al.\ 
clustered 574 DLBCL biopsy samples into four subtypes, called MCD, BN2, N1, EZB and ``other''. They 
unblinded the clinical data only after the clustering was complete and the 
model was frozen \cite[Appendix 1, pp. 16--18]{schmitz18}. The following, independent survival analysis 
unveiled significantly differing progression-free survival between the four subtypes (excluding 
``other'') according to a logrank test; BN2 and EZB subtypes have far better prognosis than MCD and 
N1 (2-year PFS rate of \num{81}\% and \num{75}\% as opposed to \num{39}\% and 
\num{20}\%, respectively). The IPI score did not vary significantly between the subtypes, 
indicating that the new classifier gives us additional information to predict survival.

There are two caveats: First, the genetic classifier saw the entire data set during training and 
this runs afoul of a strict train-test regime even if training was 
survival-agnostic. This only affects the intra-trial experiments as in the inter-trial experiments 
we do not include the genetic subtype as a feature. As a more important take away, we always need 
to carefully use features in a data set that are the output of some model trained on this data set. 
Second, this data set is not the result of a 
prospective, representative trial, but opportunistically collected samples from highly renowned 
U.S. hospitals, which preferably treat 
difficult cases. As a result, the high-risk proportion in the Schmitz data is at \num{36.6}\% -- 
compared to \num{24.3}\% in the prospective Lamis test data -- and the IPI reaches a precision of 
\num{65.2}\% for classifying high-risk patients at a prevalence of \num{12.9}\% -- compared to 
\num{38.2}\% at \num{17.0}\% in the Lamis test data. The IPI already meets the MMML-Predict goals, 
but we want to see if we can do even better in a such high-risk regime.

\paragraph{Reddy data}

Compared to a prospective study, also the Reddy data is enriched for high-risk patients and the 
IPI boasts a performance that already satisfies the MMML-Predict goals, even if a bit less 
convincingly than on the Schmitz data. After identifying \num{150} DLBCL driver genes, Reddy et al.\ 
trained a Cox model, termed genomic risk model,
that predicts overall survival (OS) from combinations of genetic events and gene-expression markers 
(cell of origin, MYC, and BCL2) and thresholded it into low, intermediate and low risk. This is a
model whose predictions we cannot use because their split into the train and test cohort is not 
clear, but that inspired us to use combinations of discrete features, cf.\ subsection 
\ref{subsec:model-agnostic}. Nevertheless, with high expression and translocation of MYC, BCL2 and 
BCL6, the data provides some of the input features of the genomic risk model as well as three binary 
clinical features: B symptoms at diagnosis, testicular and central-nervous-system involvement. B 
symptoms refer to the presence of the triad fever, night sweats and unintential weight 
loss.

\paragraph{Lamis test data}

The Lamis test data is composed of \num{466} patients enrolled in prospective clinical trials. 
Staiger et al.\ first determined \num{731} gene pairs with highly correlated gene expression levels 
between their train cohort -- \num{233} DLBCLs with gene expression levels built from the Affymetrix 
GeneChip technology -- and the Lamis test data -- \num{466} DLBCLs with gene expression levels built 
from the NanoString nCounter technology -- with the aid of six paired nCounter-GeneChip samples, cf.\ 
\cite[Supplementary Methods]{staiger20}.
Next, they learned a Cox model on the differences of the (logarithmized) gene expression levels 
from these gene pairs and the five thresholded IPI features using LASSO regularization. 
Afterwards, they removed the five IPI features from the model aiming to make it independent of the 
IPI. One can expand the differences of gene expression levels in the signature to obtain an ordinary 
gene-expression signature with coefficients corresponding to single genes, the Lamis. It is 
based on \num{17} genes, but dominated by just two genes, CSF1 and CPT1A. 

By dichotomizing the 
Lamis scores at the 75\%-quantile into their Lamis group (low or high), Staiger et al.\ present two 
groups on the Lamis test data with significantly differing PFS and OS; 
meanwhile, the IPI features, breaks in MYC, BCL2, BCL6, and cell of origin remained prognostic 
indicators independently of the Lamis group.

Since the Lamis coefficients fulfill the 
zero-sum property, we apply the Lamis unchanged on the two other data sets; according to Eq.\ 
\eqref{eq:inter-tech}, the Lamis scores have a data-set-dependent shift. We also add the Lamis 
group by dichotomizing the Lamis scores at the 75\%-quantile of the respective data set.

\section{Intra-trial experiments}

To gain first insights on our methods, we conduct intra-trial experiments sepratetly on the 
Schmitz, Reddy and Lamis test data. To this end, we split every data set into a train and test 
cohort. We do so uniformly at random, with two constraints: first, a ratio of 3 to 1 between train 
and test cohort and, second, the ratio between high-risk and low-risk patients in train cohort, 
test cohort and overall data set is the same. As shown in Table \ref{table:intra-trial}, the 
performance of the IPI score thresholded at 4 to classify high risk notably differ between the whole
data set and the subsampled test cohort. 

\input{figs/intra_trial.tex}

\subsection{Model architectures}

We want to give a brief summary of the models we send into the race and take a closer look at the 
best one, $m^*$, on every data set.

\subsubsection{Candidates}

\paragraph{Gene expression levels only}
Models trained in a leave-one-out cross-validation only on the gene expression levels include the 
Gaussian, logistic and Cox model. Regarding noteworthy hyperparameter decisions, we both apply and 
do not apply standardization of the predictor; we do not demand the zero-sum constraint 
because we do not want to transfer our model to other data sets and want to safe computation time;
we regularize with elastic-net penalty factor $\alpha \in \{ \num{0.1}, 1 \}$; as throughout this 
chapter, we have the \texttt{zeroSum} package calculate a sequence of \num{100} regularization strengths 
$\lambda$ for us and stop early if the cross-validated error does not improve for \num{10} 
consecutive decreasing $\lambda$ values; we try out a whole bunch of time cutoffs $T$.

We train and validate a model for every combination of these hyperparameters.
Not taking into account the values of $\lambda$ (which are hard to foresee due to early stopping), 
this adds up to \num{88} models.

Looking at the models with top validated performance, for the following we narrow down $T$ to one or 
two values (usually at or slighly below two years) and $\alpha$ to $1$ as the more complex models 
trained for $\alpha = \num{0.1}$ cannot clearly outperform the spare LASSO-trained models. For 
models only predicting from gene expression levels, we do not standardize the predictor any more.

\paragraph{Core models with other features}
We now add all available remaining features: for the 
IPI, we either add the five continuous features (if available), the five threholded features, the 
score as a continuous feature or all of these threee; for the Lamis, % here and for the rest of the thesis 
we add the score as a continuous and the group as a discrete feature. For $s_\text{min} = \num{0.05}$ 
and $n_\text{combi} \in \{1, 2, 3, 4 \}$, we add combinations of discrete features to the predictor. 
We both include and exclude gene expression levels in the predictor.

As for the model class, we build on our experience gained from the gene-expression-only models and 
just use the top performing model class of these models for such models that also use gene 
expression levels. For models not predicting from gene-expression levels, we have no prior 
knowledge and thus deploy the full range of core models: Gaussian, logistic, Cox models and random 
forests. As all of these models need to deal with features on different scales, we always 
standardize the predictor.


\paragraph{Nested models}
We nest models according to Alg. \ref{alg:nested-pcv}, where the early model $f_1$ is the best 
validated model among the gene-expression-only models and $f$ has the model class of $f_1$ or is 
a random forest. If $f$ has the same model class as $f_1$, we still tune the LASSO regularization 
strengtht for $f_1$ because we may end up adding several hundreds combined disrete featuers to the 
predictor. As for a-priori feature selection, dito as above.

\subsubsection{Best validated models}

\paragraph{Schmitz and Reddy}
For both the Schmitz and Reddy data, the best model according to the validation error is a nested 
model as in Alg. \ref{alg:nested-pcv} with the early model $f_1$, a Gauss model, predicting from 
the gene-expression levels and the late model $f$ being a Cox model. They only differ in their 
features: For the Schmitz data, $X$ holds the IPI in all its three formats as opposed to the five 
discretized IPI features for the Reddy data. We have $n_\text{combi} = 2$ for Schmitz as opposed to 
$n_\text{combi} = 3$ for Reddy.

On the Schmitz data, $m^*$'s validated precision of \num{89.3}\% drops by more than \num{20} points 
to \num{68.4}\% on the test set. In an even bigger drop, $m^*$'s validated precision of \num{78.6}\% 
declines to \num{55.6}\% on the Reddy test set. These way too optimistic cross-validated errors 
demand further investigation.

\paragraph{Lamis test}
On the Lamis test data, $m^*$ is a very simple model: a logistic model that neither uses gene 
expression levels nor combinations of discrete features ($n_\text{combi} = 1$). Here, $m^*$'s 
validated precision of \num{58.2}\% is more in line with the \num{45.9}\% on the test set.

\subsection{Meta analysis}

Now that we have frozen all models and do not add further ones, we can unlock the test set and 
evaluate the test performance of more models. Our main focus rests on the discrepancy between 
validated and test error; as we laid out in section \ref{sec:train-val-test}, the models with the 
best validated errors most likely have an underestimated test error and it is hard to fight this 
effect qualitatively. However, we use several methods to calculate a validated error -- cross 
validation, the pseudo cross validatio of Alg. \ref{alg:nested-pcv} and the OOB error of random 
forests -- for every model architecture we tune different hyperparameters and a different number of 
hyperparameter tuples. Hence, we want to investigate if there are systematic dependencies between 
validation-test error discrepancy and the model architecture.

\section{Inter-trial experiments}

\input{figs/inter_trial.tex}