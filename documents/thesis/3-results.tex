\chapter{Results} \label{chap:results}

In this three-part chapter, we start with introducing three DLBCL data sets that include 
survival, clinical and molecular features. Next, in \textit{intra-trial experiments}, we split 
every of these three data sets into a train and test cohort, fit a variety of candidate models to 
the training cohort, validate them on the same and test the best on the test cohort. A purpose of 
this is analyzing validated and tested errors to make the set of tried out hyperparameter tuples 
$H$ better for the future. This future 
plays out in the last part as we train and validate on one of the three data sets and test on 
the two remaining data sets while dealing with cross-platform variability in 
\textit{inter-trial experiments}.

\section{The data}\label{sec:data}

Our DLBCL data sets are taken from papers by Schmitz et al.\ \cite{schmitz18}, Reddy et al.\ 
\cite{reddy17} and Staiger et al.\ \cite{staiger20}. See Table \ref{table:data} for key properties 
and comparison. We will refer to them as Schmitz, Reddy and Staiger data, respectively.

\input{figs/data.tex}

\paragraph{Schmitz data}
The data by Schmitz et al.\ includes the five IPI features in their continuous form. 
By heuristically optimizing a novel genetic distinctiveness metric, Schmitz et al.\ 
clustered \num{574} DLBCL biopsy samples into four genetic subtypes. They 
unblinded the survival data only after the clustering was complete and the 
model was frozen \cite[Appendix 1, pp. 16--18]{schmitz18}. The following survival analysis 
on a subset of \num{229} patients, for who survival data was available and who had undergone R-CHOP 
or CHOP-like treatment, unveiled significantly differing PFS between the four genetic subtypes. The 
IPI score did not vary significantly between the subtypes, 
indicating that their new classifier provides additional information to predict survival.

There are two caveats: First, their genetic classifier saw the entire data set during training and 
this runs afoul of a strict train-test regime even if training was 
survival-agnostic. This only affects our intra-trial experiments as in the inter-trial experiments 
we do not include the genetic subtype as a feature. More importantly, we always need 
to be careful with using features in a data set that are the output of some model trained on this 
data set. Second, this data set is not the result of a 
prospective, representative trial, but consists of opportunistically collected patients from highly renowned 
U.S. hospitals, which preferably treat 
difficult cases. As a result, the high-risk proportion in the Schmitz data is at \num{36.6}\% -- 
compared to \num{24.3}\% in the prospective Staiger data -- and the tIPI reaches a precision of 
\num{65.2}\% for classifying high-risk patients at a prevalence of \num{12.9}\% -- compared to 
\num{38.2}\% precision at \num{17.0}\% prevalence in the Staiger data. The tIPI already meets the 
MMML-Predict goals, but we want to find out if our models can prevail even in such a high-risk 
environment.

\paragraph{Reddy data}

Compared to a prospective study, also the Reddy data, comprised of \num{604} patients treated with 
rituximab-containing regimens, is enriched for high-risk patients. The 
tIPI has a performance that already satisfies the MMML-Predict goals. After identifying \num{150} 
DLBCL driver genes, Reddy et al.\ trained a Cox model, named genomic risk model,
that predicts \textit{overall survival} (OS) from combinations of genetic events and gene-expression markers 
(cell of origin, MYC and BCL2 overexpression) and thresholded its output into low, intermediate and 
high risk. OS is the time from the start of the treatment to death from any cause \cite{pazdur08}. 
We cannot use the predictions of the model because the authors do not tell us which 
samples they assigned into the train and test set, but the model inspired us to use combinations of 
discrete features, cf.\ subsection 
\ref{subsec:model-agnostic}. Nevertheless, with high expression and translocation of MYC, BCL2 and 
BCL6, the data provides some of the input features of the genomic risk model as well as three binary 
clinical features: B symptoms at diagnosis, testicular and central-nervous-system involvement. B 
symptoms refer to the triad of fever, night sweats and unintentional weight 
loss being simultaneously present \cite{carbone71}.

\paragraph{Staiger data}

The Staiger data is composed of \num{466} patients treated with CHOP-like and R-CHOP-like regimens 
and enrolled in prospective clinical trials. 
Staiger et al.\ first determined \num{731} gene pairs with highly correlated gene-expression levels 
between their training cohort -- \num{233} DLBCLs with gene-expression levels built from the Affymetrix 
GeneChip technology -- and the Staiger data -- \num{466} DLBCLs with gene-expression levels built 
from the NanoString nCounter technology -- with the help of six paired nCounter-GeneChip samples, cf.\ 
\cite[Supplementary Methods]{staiger20}.
Next, they learned a LASSO-regularized Cox model on the differences of the (logarithmized) 
gene-expression levels from these gene pairs and the five thresholded IPI features.
Afterwards, they removed the five IPI features from the model aiming to make it independent of the 
IPI. Using distributivity, one can expand the differences of gene-expression levels in the 
signature to obtain an ordinary 
gene-expression signature with coefficients corresponding to single genes: the LAMIS 
(lymphoma-associated macrophage interaction signature). It is 
based on \num{17} genes, but dominated by just two genes, CSF1 and CPT1A. 

By thresholding the 
continuous output of the LAMIS, which we call the \textit{LAMIS score}, at the 75\%-quantile into 
their \textit{LAMIS group} (low or high), Staiger et al.\ present two 
groups on the Staiger data with significantly differing PFS and OS; 
meanwhile, the IPI features, breaks in MYC, BCL2, BCL6, and cell of origin remain prognostic 
indicators independently of the LAMIS group.

Since the LAMIS coefficients fulfill the 
zero-sum property, we apply the LAMIS unchanged on the two other data sets, cf.\ Eq.\ 
\eqref{eq:inter-tech}. We also add the LAMIS group by thresholding the LAMIS score at its 
75\%-quantile of the respective data set.

\paragraph{Combined data}

We will conduct inter-trial experiments on a large data set consisting of the samples from the 
Schmitz, Reddy and Staiger data. As for the gene expression-levels, we map the Ensemble gene IDs 
used in the Reddy data to HGNC gene symbols with the help of the \texttt{BiomaRt} R package 
\cite{biomart09}
to end up with the same gene nomenclature in all three data sets. Intersecting the gene-expression 
features leaves us with expression levels for \num{119} genes and the features gender, age, 
the five thresholded IPI features, the IPI score, the IPI group, the LAMIS score and the LAMIS 
group. The resulting combined data set is \num{1299} samples strong.

\section{Intra-trial experiments}\label{sec:intra-trial}

To gain first insights into our methods, we conduct intra-trial experiments separately on the 
Schmitz, Reddy and Staiger data. To this end, we split every data set into a train and test 
cohort. We do so uniformly at random, with two constraints: first, a ratio of 3 to 1 between train 
and test cohort and, second, the ratio between high-risk and low-risk patients in training cohort, 
test cohort and overall data set is the same. As shown in Table \ref{table:intra-trial}, the 
performance of the tIPI can noticeably differ between the whole data set and the subsampled test cohort. 

\input{figs/intra_trial.tex}

\subsection{Model architectures}

We want to give a brief summary of the models we send into the race and take a closer look at the 
best validated one, $m^*$, on every data set.

\subsubsection{Candidates}

\paragraph{Gene-expression levels only}
Models trained in a leave-one-out cross-validation only on the gene-expression levels include the 
Gauss, logistic and Cox model. Regarding noteworthy hyperparameter decisions, we both apply and 
do not apply standardization of the predictor; we do not demand the zero-sum constraint, i.e., all 
zero-sum weights are \num{0},
because, at this point, we do not want to transfer our models to other data sets;
we regularize with elastic-net penalty factor $\alpha \in \{ \num{0.1}, 1 \}$. We choose the 
training survival cutoff $T \in \{ 1, \num{1.25}, \num{1.5}, \ldots, \num{2.5}\}$. For Cox models, 
we additionally set $T = \infty$. For the Reddy data, we add \num{0.5} to these values of $T$ to 
account for classifying OS $> \num{2.5}$ years as opposed to PFS $> \num{2}$ years.

We train and validate a model for every combination of these hyperparameters.
Not taking into account the values of the regularization strength $\lambda$ -- which are hard to 
foresee due to early stopping --, this adds up to \num{88} models.

Looking at the models with top validated performance (cf. projection upon validation error in Fig.\
\ref{fig:intra-val-test-geo}), for the following, we narrow down $T$ to one or 
two values -- usually at or slightly below two years -- and set $\alpha$ to $1$ as the more complex 
models trained for $\alpha = \num{0.1}$ cannot clearly outperform the sparse LASSO-regularized 
models. For models only predicting from gene-expression levels, we do not standardize the predictor 
anymore.

\paragraph{Core models with other features}
We now add more of the remaining features to the predictor. For the 
IPI, we either add the five IPI features in their thresholded format, the IPI score, the IPI group
or all of them at once. For $s_\text{min} = \num{0.05}$ 
and $n_\text{combi} \in \{1, 2, 3, 4 \}$, we add combinations of categorical features to the 
predictor. 
We both include and exclude gene-expression levels in the predictor.
As all of these models need to deal with features on different scales, we always 
standardize the predictor. We regularize all GLMs with the LASSO because we may end up up adding 
several hundred combined categorical features to the predictor. For random forests, we let 
$n_\text{min}$ and $m$ around their default values \num{1} and $\floor{\sqrt{p}}$, respectively, and 
set $B = \num{600}$.

\paragraph{Nested models}
We nest models according to Alg. \ref{alg:nested-pcv}, where the early model $f_1$ is the best 
validated model among the gene-expression-only models. If the late model $f$ is not a random 
forest, but a GLM, we regularize it with the LASSO. We select features a priori as in the previous 
paragraph.

\subsubsection{Best validated models}

\paragraph{Schmitz and Reddy}
For both the Schmitz and Reddy data, the best model $m^*$ according to the validation error is a nested 
model as in Alg. \ref{alg:nested-pcv} with the early model $f_1$, a Gauss model, predicting from 
the gene-expression levels and the late model $f$ being a Cox model. They only differ in their 
features: For the Schmitz data, the predictor holds IPI-related features in all of the 
above-mentioned formats as opposed to only the five IPI features in their thresholded format for the 
Reddy data. We have $n_\text{combi} = 2$ for Schmitz as opposed to $n_\text{combi} = 3$ for Reddy.

On the Schmitz data, $m^*$'s validated precision of \num{89.3}\% drops by more than \num{20} points 
to \num{68.4}\% on the test set. In an even bigger drop, $m^*$'s validated precision of \num{78.6}\% 
declines to \num{55.6}\% on the Reddy test set. We will investigate these discrepancies between 
validation and testing in subsection \ref{subsec:results-intra-meta}.

\paragraph{Staiger}
On the Staiger data, $m^*$ is a very simple model: a logistic model that neither uses 
gene-expression levels nor combinations of discrete features ($n_\text{combi} = 1$). Of note, the 
model incorporates the LAMIS score and group with non-zero coefficient. Here, $m^*$'s 
validated precision of \num{58.2}\% is more in line with the \num{45.9}\% precision on the test set.

\paragraph{Logrank test}
The last row of Table \ref{table:intra-trial} holds two-sided p-values for the logrank test
statistic as proposed by Mandel \cite{mantel66}, which is calculated under the null hypothesis 
that the positive and negative group according to $m^*$ have the same hazard. While $m^*$ for all 
three data sets fails to beat the precision of the tIPI significantly, all logrank test statistics 
are highly significant. In fact, all models picked as the winner in a validation in this chapter 
achieve a highly significant logrank test with a p-value far below the commonly used \num{0.05}. 
This suggests that significantly beating the IPI's precision is much harder than classifying patients 
into two groups with clearly different hazard. 

\subsection{Meta analysis}\label{subsec:results-intra-meta}

Now that we have frozen all models and do not add further ones, we can unlock the test set and 
evaluate the test performance of more models. We want to examine the discrepancy between 
validated and test error. As we laid out in section \ref{sec:train-val-test}, the models with the 
best validated errors most likely have an underestimated test error and it is hard to fight this 
effect qualitatively. However, we use several methods to calculate a validated error -- cross 
validation, the pseudo cross validation of Alg.\ \ref{alg:nested-pcv} and the OOB error of random 
forests -- and, for every model architecture, we tune different hyperparameters and a different 
number of hyperparameter tuples. Both validation error and test error of a model are random 
variables and it is hard to infer statistical properties of them based on a single realization of 
these random variables. By grouping the models according to their hyperparameters, we gain 
statistical power and can look for patterns in the validation and test error as well as 
discrepancies between them.

\subsubsection{Models predicting from gene-expression levels only}

\input{figs/intra_val_test_geo.tex}

\paragraph{Staiger}
Looking at Fig.\ \ref{fig:intra-val-test-geo}, we see that, for Staiger, the models only using 
gene-expression levels cannot compete with the $\text{tIPI}$. While the validated errors of these 
models are 
below the \num{-36.4}\% negative precision of the tIPI, the test errors all are above \num{-30}\%. 
In all cases, the test error is at least 10 points higher than the validated error.  

\paragraph{Schmitz and Reddy}
Also for the Schmitz and Reddy data, the dependence between validated and test error is far from 
being monotonic; in plots A and B in Fig.\ \ref{fig:intra-val-test-geo}, the
correlation between validated and test 
error is negative such that the model with the lowest validated error has the highest 
(Schmitz) or second highest (Reddy) test error. The plots do not show errors for all tried out 
hyperparameter tuples in $H$, but only for a high-performing subset with $T$, $n_\text{combi}$ 
and $\lambda$ already 
optimized according to validation; this optimization was successful in the sense that even the 
worst models post a test error below that of $\text{tIPI}$.

There is no model class reliably outperforming the others. On both data sets, ridge 
regularization yields models with a test error lower than that of LASSO regularization; validation, 
however, does not reveal this difference. The picture on standardizing the predictor matrix or not 
is mixed: on the 
Schmitz data, models with non-standardized predictor fluctuate less around the identity line than 
those with standardized predictor, meaning validation is more reliable for the former model group;
on the Reddy data, meanwhile, we observe the same phenomenon with roles swapped.

\subsubsection{All models}

\input{figs/intra_val_test_more.tex}

All analysis here is based on Fig. \ref{fig:intra-val-test-more}.

\paragraph{Schmitz and Reddy}
Again, we analyze the non-prospective, high-risk-heavy Schmitz and Reddy data together. When 
considering all candidate models, validation and test error for these two data sets 
stay out of touch. Partitioning according to the model architecture reveals some patterns. 

Models 
nesting a Gauss model into a GLM as in Alg. \ref{alg:nested-pcv} all have low validated errors; on 
the Reddy data, their validated errors are lower than those of all other models. The test error is 
always higher than the validated error and usually it is \textit{much} higher. Alg. \ref{alg:nested-pcv} 
yields way too optimistic validated errors if the late model is a GLM, prompting us to no longer 
fit such models in the following experiments. Discarding Alg. \ref{alg:nested-pcv} once and for all
might go too far: when nesting a Gauss model into a random forest, we get validated errors more in 
line with test errors, especially on the Schmitz data. On both data sets, OOB predictions for 
models consisting of a random forest alone very well estimate the test error. In all but one case 
they even underestimate the test error. 

As for feature selection, we see that including gene-expression levels does not clearly improve
models. Even the high-performing gene-expression-only models do not benefit noticeably if
we train them with additional features. On the Reddy data, the points belonging to models not using 
gene-expression data are all close to or underneath the identity line; this means the validation 
errors 
catch the test errors well, their test errors are less widespread and on average lower than those 
of the remaining models. This does not mean that gene-expression levels do not provide important 
information: all of the models that do not use the gene-expression levels directly from our data 
do so indirectly by means of the LAMIS and cell of origin. 

\paragraph{Staiger}

On the Staiger data, the validated errors align better with the test errors. As with the two other 
datasets, OOB-based test-error estimates are conservative leading to underestimated test errors 
for models incorporating a random forest. Reflecting highly correlated validation and test 
errors, the best validated model is also the best tested model. Compared to above, the validated 
errors of models nesting a Cox model into a GLM overestimate the test errors less.
% since the early Cox 
% model yields pretty poor cross-validated predictions, we hypothesize that the training 
% algorithm of the late GLM is less tempted to put as much weight on these predictions as with the 
% two other data sets. 
Models profit from not just predicting from the gene-expression levels: 
all the gene-expression-only models form a bulk with test errors higher than those of all other 
models. 
% Key to the success is again the molecular information condensed in the LAMIS, which all the 
% high-performing models put heavy weight on.

\section{Inter-trial experiments}\label{sec:inter-trial}

Looking back at the intra-trial experiments, we beat the precision of the tIPI by 
at least \num{7} points on three data sets, including two where the IPI with its prevalence 
and precision already fulfilled the MMML-Predict objectives. 
In the following analysis, we saw validated errors often being detached from test errors. To tackle 
this issue, we decided to no longer train certain models. Another way to close the 
validation-testing gap is to increase the number of samples 
in both the train and test cohort: it both makes overfitting the validated predictions to the 
training cohort harder and makes it less likely that the test cohort includes cases biologically 
not covered at all in the training cohort. We will now go along this path as we use every of the 
three data sets in their entirety for training and validation and then test on the other two.

\subsection{Model architectures}

We want to sketch the architectures of the candidate models and take a closer look at the 
best validated model $m_i^*$ trained on every data set.

\subsubsection{Candidates}

Even with combined discrete features, the number of features in the predictor $p$ 
never exceeds \num{300} in this section, meaning computation is comparatively cheap. 
Consequently, we can afford 
a leave-one-out cross-validation for every model that is not a random forest and $B = \num{1000}$ 
trees for every random forest.

\paragraph{Gene-expression levels only}
This time, we require every Gauss, logistic and Cox model to fulfill the zero-sum property for 
the gene-expression levels it deals with as features since every data set has its own protocol for 
measuring gene-expression levels. We have gene-expression levels for \num{119} genes in the 
combined data and these genes are a subset of the \num{145} genes for which the Staiger data 
includes gene-expression levels. We train LASSO-regularized Gauss, 
logistic and Cox models predicting only from gene-expression levels. In line with the zero-sum 
idea, we do not standardize the predictor and have $T$ range from \num{1} to \num{2.6} years in 
steps of \num{0.2} years for models trained on the Schmitz and Staiger samples; for the Reddy 
samples, here and for the rest of this section, we shift $T$ by
\num{0.5} years to the future accounting for the fact that on the Reddy data, absent PFS, we need 
to classify OS $< \num{2.5}$ years as opposed to PFS $< \num{2}$ years.

\paragraph{Core models with other features}
As in the intra-trial experiments, we now add more of the available features to the predictor. Only 
for the LAMIS, we vary its format and add just the score, just the group or both. We augment the 
predictor with combinations of discrete features according to $s_\text{min} = \num{0.05}$ and 
$n_\text{combi} \in \{ 1, 2, 3 \}$. We restrict the training survival cutoff $T$ to range between 
\num{1.4} and \num{2} years in steps of \num{0.2} years for the Schmitz and Staiger data. We 
both include and exclude the expression levels in the predictor of the Cox and logistic 
models, which we fit for all combinations of these hyperparameter values.

For random forests, which have trouble dealing with features systematically shifted between data 
sets (cf.\ \ref{subsec:nested-models}), we exclude the gene-expression levels from the predictor 
and, of the LAMIS formats, we only include the LAMIS group in the predictor. Moreover, random 
forests can realize combinations of categorical features themselves, so we set $n_\text{combi} = 1$ 
for them.

\subsubsection{Best validated models}

Table \ref{table:inter-trial} presents the performance of the best models validated and trained on 
every cohort, $m_i^*$, $i \in \{ \text{Schmitz}, \text{Reddy}, \text{Staiger} \}$. We now loop 
through $i$.

\input{figs/inter_trial.tex}

\paragraph{Schmitz}
$m^*_\text{Schmitz}$ is a Cox model predicting from only categorical features and combinations of up to 
three of them. In particular, no gene-expression levels are directly involved and the model confines 
itself with the LAMIS \textit{group}. All in all, the sparse model uses \num{7} features. During 
training, we provided the model with a low survival cutoff of $T = \num{1.4}$ years.

When tested on the Reddy data, $m^*_\text{Schmitz}$ beats the precision of the tIPI by 5 points. This 
is not enough to show significant superiority in the sense that the \num{95}\%-confidence interval 
of our model's precision does not include the precision of the tIPI.

On the Staiger set, meanwhile, $m^*_\text{Schmitz}$ accomplishes all objectives: with 
\num{50.7}\% precision, it overtops the precision of the tIPI by more than 12 points, narrowly 
surpasses the 
psychologically important \num{50}\% threshold, and the \num{95}\%-confidence interval of our model's 
precision excludes the precision of the tIPI. $m^*_\text{Schmitz}$, originating from 
the high-risk, non-prospective Schmitz data with the LAMIS measured with the 
RNA-seq technology, acclimatizes well to the prospective, NanoString regime of the Staiger data. 

\paragraph{Reddy}
$m^*_\text{Reddy}$ looks quite similar to $m^*_\text{Schmitz}$. Instead of the LAMIS group, the 
logistic model uses the LAMIS score making it its only continuous feature. Its predictor contains 
combinations of up to two categorical features. Hyperparameter tuning yielded a training survival 
cutoff of \num{2.3} years. 

On the Schmitz data, $m^*_\text{Reddy}$ is more precise than the tIPI, but fails to outperform the 
tIPI significantly. It yields a hazard ratio of \num{53.4} 
(\num{95}\%-CI \num{18.7}--\num{152.1}).

On the Staiger data, $m^*_\text{Reddy}$ posts a precision of \num{53.2} that is by \num{2.5} 
points higher than that of $m^*_\text{Schmitz}$ and by \num{15} points higher than that of the tIPI. 
Consequently, the lower limit of the \num{95}\%-confidence interval of its precision, at 
\num{41.5}\%, is 
clearly above the precision of the tIPI and the hazard ratio rises to \num{23.9}
(\num{95}\%-CI \num{9.9}--\num{57.6}). Even more than $m^*_\text{Schmitz}$, $m^*_\text{Reddy}$ 
defies systematic differences between data sets: comparing the Reddy to the Staiger data, we 
notice the contrasts non-prospective versus prospective trial, 
RNA-seq versus NanoString nCounter technology to measure gene-expression levels and, most strikingly, 
classifying OS below \num{2.5} years versus PFS below \num{2} years.

\paragraph{Staiger}
The simplest of the three picked models is $m^*_\text{Staiger}$. A Cox model, it predicts from 
the LAMIS score and five more categorical features -- no combinations of categorical features involved, 
$n_\text{combi} = 1$. With $T = \num{1.4}$, it uses the same low training survival cutoff as 
$m^*_\text{Schmitz}$.

On the Reddy data, our model's precision, at \num{50.7}\%, stays below that of the tIPI at \num{54.1}\%. 
Speaking for the whole thesis, this renders the Reddy data a challenging test cohort with a 
hard-to-beat IPI, but -- as we have just seen -- a very helpful training cohort.

Meanwhile, on the Schmitz data, the precision of $m^*_\text{Staiger}$ of \num{75.7}\% exceeds that 
of the tIPI by more than \num{10}\%, but falls short of outrivaling the tIPI significantly. Still, 
this demonstrates that transferring models between the Schmitz and LAMIS data works in both 
directions.

\subsection{Meta analysis}\label{subsec:results-inter-meta}

We again freeze all of our models and unlock the test sets for all of them to analyze the discrepancy 
between validated and test error and to gain insights into how stable thresholding the models on the 
test cohort is.

\subsubsection{Validated and test errors}

\input{figs/inter_trial_val_test.tex}

Fig.\ \ref{fig:inter-val-test} compares the validation to the test error for all candidate models 
participating in the inter-trial experiments; note that several hyperparameters have already 
been optimized in the validation. 

We have emphasized that the risk profiles of the three data sets differ, especially when comparing 
the Schmitz and Reddy to the Staiger data, so we cannot expect the validation-test-error tuples 
to perfectly align at the identity line. Indeed, Fig.\ 
\ref{fig:inter-val-test} shows test errors that are clearly more correlated with their validation 
errors than in the intra-trial experiments. As a result, the model with minimal validation error in all 
cases is close to being the one with minimal test error. In more detail, with the exception of 
the models trained on the Staiger and tested on the Reddy data, the tested precision of the best 
validated model and that of the best tested model deviate by no more than \num{2} points. In 
three of the six cases, the best validated and tested model coincide. 

As for the model class, the best tested model in all cases is a Cox model. In plots A.1 and C.1--2, 
this Cox model has a test error clearly below that of the second-best model class, the 
logistic model. Ignoring the tIPI, the random 
forest always finishes last in validation and never reaches a top position in testing. In fact, 
it claims the last testing place in four out of the six plots.

Regarding the question if the predictor should directly contain the gene-expression levels or not,
Fig.\ \ref{fig:inter-val-test} speaks a plain language. In testing, the top ranking models are 
always such models that do not use gene-expression levels. Often, the test errors of the models whose 
predictor excludes gene-expression features entirely separate from the rest (cf.\ plots A.1--2, B.1). 
In validation, however, this distinction can be less obvious, especially for those models trained on 
the Reddy data (cf.\ plots B.1, B.2). In summary, we gain nothing in terms of test 
performance, but risk choosing a low-performing model in the validation. E.g., the models that 
make the choice of the best validated model on the Reddy data 
close and fortunate with regard to testing all use gene-expression features. 
% As the LAMIS does a very 
% good job at capturing gene-expression information and was trained on an independent data set, we 
% conclude that we should just include the LAMIS -- and maybe other widely accepted molecular 
% signatures -- as an ordinary feature in the predictor and leave aside any other gene-expression 
% levels.

\subsubsection{Choosing the output threshold}

\input{figs/inter_output_prec.tex}
\input{figs/inter_output_prec_ci.tex}

For all $i$, the best validated model $m_i^*$ outputs a continuous score and we need to threshold 
these scores in a protocol-dependent manner to obtain a binary classifier. As a reminder, the errors 
we presented so far are all optimized on the respective cohort: the maximum precision under the 
constraint to have a prevalence of at least \num{17}\% for our models and of at least \num{10}\% 
for the tIPI. When given a new data set with blinded outcome, we suggested to threshold a model at 
the \num{17}\%-quantile of its continuous output on this data set. This, by definition, leads to 
higher test errors than with the optimization procedure. The question is by how much the test error 
of the optimally thresholded model and the \num{17}\%-quantile-thresholded model will differ.

If we rank patients by the scores a good model assigns them, high-risk patients accumulate at the 
top of the ranking and low-risk patients at the bottom. As a result, as we decrease the threshold, 
the group of patients with a score above a threshold tends to comprise a lower and lower proportion 
of high-risk patients. Stated differently, for good models the precision tends to drop as the 
threshold drops and the relationship between prevalence and precision is roughly monotonic. In the 
case of a perfectly monotonic dependence between prevalence and precision, the $\alpha$-quantile 
would be an optimal threshold with a prevalence of at least $\alpha$. 
In the case of a somewhat violated monotonicity, we still have reason to hope that the 
$\alpha$-quantile is a near-optimal threshold.

To check this out in practice, we plotted prevalence versus precision for every possibility to 
threshold $m^*_i$ for all $i$ and test cohorts as long as the prevalence is in $[\num{0.15}, 
\num{0.50}]$, in Fig.\ \ref{fig:inter-output-prec}. For a given 
cohort of $n$ samples, a GLM with a non-zero coefficient $\beta_j$ for a continuous feature almost 
surely outputs $n$ different scores. This leads to $n$ possible thresholds if we demand that at 
least one sample be in the positive group. Indeed, all 
plots reveal a clearly decreasing overall trend for prevalences $\geq \num{17}\%$. With the 
exception of plot C.2, this overall trend is never seriously compromised over smaller 
prevalence intervals. This means, the \num{17}\%-quantile of the model output is a near optimal 
choice in all six cases -- as expected.

In plots B.1--2 and C.1--2, the curve falls sharply for prevalences between \num{10}\% and \num{15}\%. 
We might wonder if the sharp increase in precision that we observe as we reduce the prevalence 
from \num{15}\% to \num{10}\% overcompensates for the lost statistical power that we need to 
significantly outperform the IPI. Fig.\ \ref{fig:inter-output-prec-ci} answers this question with a 
clear yes. With a reduced prevalence of \num{10}\%, we score two more significant victories over the 
tIPI, this time on the Schmitz data: a clear one in the case of $m^*_\text{Reddy}$ and a close one
for $m^*_\text{Staiger}$. 

This makes $m^*_\text{Reddy}$ a transferable model that is 
able to perform in both a representative and high-risk environment for predicting PFS despite being 
trained to predict OS. Vice versa, for $m^*_\text{Schmitz}$ and $m^*_\text{Staiger}$, the Reddy data 
stays challenging and the only data set where a best validated model $m^*_i$ fails to significantly 
surpass the precision of the tIPI.