\chapter{Results} \label{chap:results}

In this triune chapter, we start with introducing three DLBCL data sets that include 
survival and gene expression features. Next, in intra-trial experiments, we split every of these 
three data sets into 
a train and test cohort, fit models for a variety of hyperparameter tuples in $H$ to the training 
cohort, validate them and test the best on the test cohort; this is less about presenting a 
high-performing model, but more about analyzing validated and tested errors to make $H$ slimmer and 
better for the future. This future plays out in the last part as we train and validate on one of 
the three data sets and test on another and deal with cross-platform variability in inter-trial 
experiments.

\section{The data}

Our DLBCL data sets are taken from papers by Schmitz et al.\ \cite{schmitz18}, Reddy et al.\ 
\cite{reddy17} and Staiger et al.\ \cite{staiger20}; see Table \ref{table:data} for key properties 
and comparison. We will refer to them as Schmitz, Reddy and Lamis test data, respectively.

\input{figs/data.tex}

\paragraph{Schmitz data}
The data by Schmitz et al.\ includes the five IPI features in their continuous form. 
By heuristically optimizing a novel genetic distinctiveness metric, Schmitz et al.\ 
clustered 574 DLBCL biopsy samples into four subtypes, called MCD, BN2, N1, EZB and ``other''. They 
unblinded the clinical data only after the clustering was complete and the 
model was frozen \cite[Appendix 1, pp. 16--18]{schmitz18}. The following, independent survival analysis 
unveiled significantly differing progression-free survival between the four subtypes (excluding 
``other'') according to a logrank test; BN2 and EZB subtypes have far better prognosis than MCD and 
N1 (2-year PFS rate of \num{81}\% and \num{75}\% as opposed to \num{39}\% and 
\num{20}\%, respectively). The IPI score did not vary significantly between the subtypes, 
indicating that the new classifier gives us additional information to predict survival.

There are two caveats: First, the genetic classifier saw the entire data set during training and 
this runs afoul of a strict train-test regime even if training was 
survival-agnostic. This only affects the intra-trial experiments as in the inter-trial experiments 
we do not include the genetic subtype as a feature. As a more important take away, we always need 
to carefully use features in a data set that are the output of some model trained on this data set. 
Second, this data set is not the result of a 
prospective, representative trial, but opportunistically collected samples from highly renowned 
U.S. hospitals, which preferably treat 
difficult cases. As a result, the high-risk proportion in the Schmitz data is at \num{36.6}\% -- 
compared to \num{24.3}\% in the prospective Lamis test data -- and the IPI reaches a precision of 
\num{65.2}\% for classifying high-risk patients at a prevalence of \num{12.9}\% -- compared to 
\num{38.2}\% at \num{17.0}\% in the Lamis test data. The IPI already meets the MMML-Predict goals, 
but we want to see if we can do even better in a such high-risk regime.

\paragraph{Reddy data}

Compared to a prospective study, also the Reddy data is enriched for high-risk patients and the 
IPI boasts a performance that already satisfies the MMML-Predict goals, even if a bit less 
convincingly than on the Schmitz data. After identifying \num{150} DLBCL driver genes, Reddy et al.\ 
trained a Cox model, termed genomic risk model,
that predicts overall survival (OS) from combinations of genetic events and gene-expression markers 
(cell of origin, MYC, and BCL2) and thresholded it into low, intermediate and low risk. This is a
model whose predictions we cannot use because their split into the train and test cohort is not 
clear, but that inspired us to use combinations of discrete features, cf.\ subsection 
\ref{subsec:model-agnostic}. Nevertheless, with high expression and translocation of MYC, BCL2 and 
BCL6, the data provides some of the input features of the genomic risk model as well as three binary 
clinical features: B symptoms at diagnosis, testicular and central-nervous-system involvement. B 
symptoms refer to the presence of the triad fever, night sweats and unintential weight 
loss.

\paragraph{Lamis test data}

The Lamis test data is composed of \num{466} patients enrolled in prospective clinical trials. 
Staiger et al.\ first determined \num{731} gene pairs with highly correlated gene expression levels 
between their train cohort -- \num{233} DLBCLs with gene expression levels built from the Affymetrix 
GeneChip technology -- and the Lamis test data -- \num{466} DLBCLs with gene expression levels built 
from the NanoString nCounter technology -- with the aid of six paired nCounter-GeneChip samples, cf.\ 
\cite[Supplementary Methods]{staiger20}.
Next, they learned a Cox model on the differences of the (logarithmized) gene expression levels 
from these gene pairs and the five thresholded IPI features using LASSO regularization. 
Afterwards, they removed the five IPI features from the model aiming to make it independent of the 
IPI. One can expand the differences of gene expression levels in the signature to obtain an ordinary 
gene-expression signature with coefficients corresponding to single genes, the Lamis. It is 
based on \num{17} genes, but dominated by just two genes, CSF1 and CPT1A. 

By dichotomizing the 
Lamis scores at the 75\%-quantile into their Lamis group (low or high), Staiger et al.\ present two 
groups on the Lamis test data with significantly differing PFS and OS; 
meanwhile, the IPI features, breaks in MYC, BCL2, BCL6, and cell of origin remained prognostic 
indicators independently of the Lamis group.

Since the Lamis coefficients fulfill the 
zero-sum property, we apply the Lamis unchanged on the two other data sets; according to Eq.\ 
\eqref{eq:inter-tech}, the Lamis scores have a data-set-dependent shift. We also add the Lamis 
group by dichotomizing the Lamis scores at the 75\%-quantile of the respective data set.

\paragraph{Combined data}

We will conduct inter-trial experiments on a big data set consisting of the samples in the Schmitz, 
Reddy and Lamis test data. As for the gene expression features, we map the Ensemble gene IDs used 
in the Reddy data to HGNC gene symbols with the help of the \texttt{BiomaRt} R package \cite{biomart}
to end up with the same gene nomenclature in all three data sets. Intersecting the gene expression 
features leaves us with gene expression levels for \num{119} genes and the features gender, age, 
the five thresholded IPI features, the IPI score, the IPI group, the Lamis score and the Lamis 
group. The resulting data set is \num{1299} samples strong.

\section{Intra-trial experiments}

To gain first insights on our methods, we conduct intra-trial experiments sepratetly on the 
Schmitz, Reddy and Lamis test data. To this end, we split every data set into a train and test 
cohort. We do so uniformly at random, with two constraints: first, a ratio of 3 to 1 between train 
and test cohort and, second, the ratio between high-risk and low-risk patients in train cohort, 
test cohort and overall data set is the same. As shown in Table \ref{table:intra-trial}, the 
performance of the tIPI notably differs between the whole data set and the subsampled test cohort. 

\input{figs/intra_trial.tex}

\subsection{Model architectures}

We want to give a brief summary of the models we send into the race and take a closer look at the 
best one, $m^*$, on every data set.

\subsubsection{Candidates}

\paragraph{Gene expression levels only}
Models trained in a leave-one-out cross-validation only on the gene expression levels include the 
Gaussian, logistic and Cox model. Regarding noteworthy hyperparameter decisions, we both apply and 
do not apply standardization of the predictor; we do not demand the zero-sum constraint 
because we do not want to transfer our model to other data sets and want to safe computation time;
we regularize with elastic-net penalty factor $\alpha \in \{ \num{0.1}, 1 \}$; as throughout this 
chapter, we have the \texttt{zeroSum} package calculate a sequence of \num{100} regularization strengths 
$\lambda$ for us and stop early if the cross-validated error does not improve for \num{10} 
consecutive decreasing $\lambda$ values; we try out a whole bunch of time cutoffs $T$.

We train and validate a model for every combination of these hyperparameters.
Not taking into account the values of $\lambda$ (which are hard to foresee due to early stopping), 
this adds up to \num{88} models.

Looking at the models with top validated performance (cf. projection upon validation error in Fig. 
\ref{fig:intra-val-test-geo}), for the following we narrow down $T$ to one or 
two values (usually at or slighly below two years) and $\alpha$ to $1$ as the more complex models 
trained for $\alpha = \num{0.1}$ cannot clearly outperform the spare LASSO-trained models. For 
models only predicting from gene expression levels, we do not standardize the predictor any more.

\paragraph{Core models with other features}
We now add all available remaining features: for the 
IPI, we either add the five continuous features (if available), the five threholded features, the 
score as a continuous feature or all of these threee; for the Lamis, % here and for the rest of thesis 
we add the score as a continuous and the group as a discrete feature. For $s_\text{min} = \num{0.05}$ 
and $n_\text{combi} \in \{1, 2, 3, 4 \}$, we add combinations of discrete features to the predictor. 
We both include and exclude gene expression levels in the predictor.

As for the model class, we build on our experience gained from the gene-expression-only models and 
just use the top performing model class of these models for such models that also use gene 
expression levels. For models not predicting from gene-expression levels, we have no prior 
knowledge and thus deploy the full range of core models: Gaussian, logistic, Cox models and random 
forests. As all of these models need to deal with features on different scales, we always 
standardize the predictor.


\paragraph{Nested models}
We nest models according to Alg. \ref{alg:nested-pcv}, where the early model $f_1$ is the best 
validated model among the gene-expression-only models and $f$ has the model class of $f_1$ or is 
a random forest. If $f$ has the same model class as $f_1$, we still tune the LASSO regularization 
strengtht for $f_1$ because we may end up adding several hundreds combined disrete featuers to the 
predictor. As for a-priori feature selection, dito as above.

\subsubsection{Best validated models}

\paragraph{Schmitz and Reddy}
For both the Schmitz and Reddy data, the best model according to the validation error is a nested 
model as in Alg. \ref{alg:nested-pcv} with the early model $f_1$, a Gauss model, predicting from 
the gene-expression levels and the late model $f$ being a Cox model. They only differ in their 
features: For the Schmitz data, $X$ holds the IPI in all its three formats as opposed to the five 
discretized IPI features for the Reddy data. We have $n_\text{combi} = 2$ for Schmitz as opposed to 
$n_\text{combi} = 3$ for Reddy.

On the Schmitz data, $m^*$'s validated precision of \num{89.3}\% drops by more than \num{20} points 
to \num{68.4}\% on the test set. In an even bigger drop, $m^*$'s validated precision of \num{78.6}\% 
declines to \num{55.6}\% on the Reddy test set. These way too optimistic cross-validated errors 
demand further investigation.

\paragraph{Lamis test}
On the Lamis test data, $m^*$ is a very simple model: a logistic model that neither uses gene 
expression levels nor combinations of discrete features ($n_\text{combi} = 1$). Notably, the model
incorporates the Lamis score and group with decisively non-zero coefficient. Here, $m^*$'s 
validated precision of \num{58.2}\% is more in line with the \num{45.9}\% on the test set.

\subsection{Meta analysis}

Now that we have frozen all models and do not add further ones, we can unlock the test set and 
evaluate the test performance of more models. Our main focus rests on the discrepancy between 
validated and test error; as we laid out in section \ref{sec:train-val-test}, the models with the 
best validated errors most likely have an underestimated test error and it is hard to fight this 
effect qualitatively. However, we use several methods to calculate a validated error -- cross 
validation, the pseudo cross validatio of Alg. \ref{alg:nested-pcv} and the OOB error of random 
forests -- and for every model architecture we tune different hyperparameters and a different 
number of hyperparameter tuples. Both validation error and test error of a model are random 
variables and it is hard to infer statistical properties of them based on a single realization of 
these random variables; by grouping the models according to their hyperparameters we gain 
statisticial power and look for patterns in the validation and test error as well as 
discrepancies between them.

\subsubsection{Models predicting from gene expression levels only}

\input{figs/intra_val_test_geo.tex}

\paragraph{Lamis test}
Looking at Fig. \ref{fig:intra-val-test-geo}, we see that, for Lamis test, the models only using 
gene expression levels cannot compete with $\text{tIPI}$. While the validated errors of these models are 
below the \num{-36.4}\% negative precision of the IPI, the test errors all are above \num{-30}\%. 
In all cases, the test error is at least 10 points higher than the validated error, a discrepancy 
also indicated by the fact that the identity line is outside the plot area. It is hard to imagine that 
these models even if nested into another model can help outperform the IPI. Hence, for the 
following, the analysis focuses on the other two data sets.

\paragraph{Schmitz and Reddy}
Also for the Schmitz and Reddy data, the dependence between validated and test error is far from 
being monotonic; in both cases, the plot looks noisy and the correlation between validated and test 
error is negative such that the model with the lowest validated error happens to have the highest 
(Schmitz) or second highest (Reddy) test error. The plots do not show errors for all tried out 
hyperparameter tuples in $H$, but only for a high-performing subset with $\lambda$ and $T$ already 
optimized; this optimization was successful in the sense that even the worst models boast a test 
error below that of $\text{tIPI}$.

There is no model class reliably outperforming the others. Both data sets suggest that ridge 
regularization yields models with a test error lower than that of LASSO regularization; validation, 
however, does not reveal this difference. The picture on standardizing $X$ or not is mixed: on the 
Schmitz data, models with non-standardized predictor fluctuate less around the identiy line than 
those with standardized predictor, meaning validation is more reliable for the former model group;
on the Reddy data, meanwhile, we the same phenomenon with roles swapped.

\subsubsection{Models with the full range of features}

\input{figs/intra_val_test_more.tex}

All analysis here is based on Fig. \ref{fig:intra-val-test-more}.

\paragraph{Schmitz and Reddy}
Again, we analyze the non-prospective, high-risk heavy Schmitz and Reddy data together. As we apply
the full range of a priori selected features, validation and test error for these two data sets 
stay out of touch. Partioning according to the model architecture reveals some patterns. 

Models 
nesting a Gauss model into a GLM as in Alg. \ref{alg:nested-pcv} all have low validated errors, on 
the Reddy data, they even have lower validated error than any other model; the test error is 
always higher than the validated error and usually it is \textit{much} higher. Alg. \ref{alg:nested-pcv} 
yields way too optimistic validated errors if the late model is a GLM, prompting us to no longer 
fit such models in the following experiments. Discarding Alg. \ref{alg:nested-pcv} once and for all
would go too far: when nesting a Gauss model into a random forest, we get validated errors more in 
line with test errors, especially on the Schmitz data. On both data sets, OOB predictions for 
models consisting of a random forest alone very well estimate the test error. In all but one case 
they even underestimate the test error. All in all, OOB predictions seem superior to 
cross-validated predictions, which comes as no surprise when having a closer look at both 
methods: A sample's cross-validated predictions come from one model and we have one model per 
sample in the best case, a leave-one-out cross-validation (which we did for all models involved in 
nested models). In contrast, a sample's OOB prediction comes from a whole forest of models with 
expected size roughly $B/3$ because the probability of a sample not being in a bootrap sample 
is
\begin{align}
    \left( 1 - \frac{1}{n} \right)^n \to \frac{1}{e} \approx \frac{1}{3} \quad \text{as } n \to 
    \infty,
\end{align}
and we can scale it by scaling $B$, which is cheap as we have laid out in subsection 
\ref{subsec:elastic-net}.

As for feature selection, we see that including gene expression levels does not lead to greatly 
improved models. Even the high-performing gene-expression-only models do not benefit very much if 
we train them with additional features. On the Reddy data, the points belonging to models not using 
gene-expression data are all close to or underneath the identity line, meaning the validation errors 
catch the test errors well, their test errors are less widespread and on average lower than those 
of the remaining models. This does not mean that gene expression levels do not provide important 
information: all of the models that do not use the gene expression levels directly from our data 
do so indirectly thanks to the Lamis. We have simply outsourced developing a gene-expression based 
model to the Lamis authors, thereby getting rid of $p \gg n$ and ending up with a more accurate 
validation.

\paragraph{Lamis test}

On the Lamis test data, the validated errors align better with the test errros. As with the two other 
datasets, OOB-based test-error estimates are conservative leading to underestimated test errors 
for models incorporating a random forest. As a result of highly correlated validation and test 
errors, the best validated model is also the best tested model. Compared to above, the validated errors 
of models nesting a Cox model into a GLM overestimate the test errors less; since the early Cox 
model yields pretty inaccurate cross-validated predictions, we hypothesize that the training 
algorithm of the late GLM is less inclined to put as much weight on these predictions as with the 
two other data sets.

Models profit a lot from not just predicting from the gene expression levels: all the 
gene-expression-only cluster into a bulk with test errors higher than those of all other models. 
Key to the success is again the molecular information condensed in the Lamis, which all the 
high-performing models put heavy weight on.

\section{Inter-trial experiments}

The results of the intra-trial experiments are encouraging: we beat the precision of the IPI by 
at least \num{7} points on three data sets, including two where the IPI already did a good job. 
In the following analysis, we saw validated errors often being detached from test errors. To tackle 
this issue, we decided to no longer train certain models. We also noticed that the Lamis is at least 
as capable of catching the molecular information in the gene expression levels as the models
we trained for that purpose ourselves, but allows training more precise models and validating them 
more accurately. Another way to close the validation-test gap is to increase the number of samples 
in both the train and test cohort: it both makes overfitting the validated predictions to the 
training cohort harder and makes it less likely that the test cohort includes cases biologically 
not covered at all in the train cohort. We will now go along this path as we use every of the 
three data sets in their entirety for training and validation and then test on the other two.

\subsection{Model architectures}

Here, we want to sketch the architectures of the candidate models and take a closer look at the 
best validated model $m_i^*$ trained on every data set.

\subsubsection{Candidates}

Even with combined discrete features, the number of features the model in this section predict from 
never exceeds \num{300} meaning computation is comparatively cheap. Consequently, we can afford 
a leave-one-out cross-validation for every model that is not a random forest and \num{1000} trees 
for every random forest.

\paragraph{Gene-expression levels only}
This time, we demand that every Gauss, logistic and Cox models fulfill the zero-sum constraint for 
the gene-expression levels it deals with as features since every data set has its own protocol for 
measuring gene-expression levels. We have gene-expression levels for \num{119} genes in the 
combined data and these genes are a subset of the \num{145} genes for which the Lamis test data 
includes gene-expression levels; even though the gene-expression only models trained on the Lamis 
data disappointed in both validation and testing, we once more train LASSO-regularized Gauss, 
logistic and Cox models predicting only from gene-expression levels hoping they can exploit the 
improved data situation with more samples. In line with the zero-sum idea, we do not standardize 
the predictor and have $T$ range from \num{1} to \num{2.6} years in steps of \num{0.2} years for 
models trained on the Schmitz and Lamis-test samples; for the Reddy samples, here and for the rest 
of this section, we shift $T$ by
\num{0.5} years to the future accounting for the fact that on the Reddy data, absent PFS, we need 
to classify OS below \num{2.5} years as opposed to PFS below \num{2} years.

\paragraph{Core models with other features}
As in the intra-trial experiments, we generously add all remaining features to the predictor. Only 
for the Lamis, we vary its format and add just the score, just the group or both. We augment the 
predictor with combinations of discrete features according to $s_\text{min} = \num{0.05}$ and 
$n_\text{combi} \in \{ 1, 2, 3 \}$. We restrict the training survival cutoff $T$ to range between 
\num{1.4} and \num{2} years in steps of \num{0.2} years for the Schmitz and Lamis test data. We 
both include and exclude the expression levels in the predictor of the Cox and logistic 
models we fit for all combinations of these values for the hyperparameters.

Random forests have trouble dealing with features systematically shifted features between data sets, so 
we always exclude the gene-expression levels from the predictor and of the Lamis formats only 
include the Lamis group in the predictor. Moreover, random forests can realize combinations of 
categorical features themselves, so we set $n_\text{combi} = 1$ for them.

\subsubsection{Best validated models}

Table \ref{table:inter-trial} presents the performance of the best models validated and trained on 
every cohort, $m_i^*$, $i \in \{ \text{Schmitz}, \text{Reddy}, \text{Lamis test} \}$. We now cycle 
through $i$.

\input{figs/inter_trial.tex}

\paragraph{Schmitz}
$m^*_\text{Schmitz}$ is a Cox model predicting from discrete features and combinations of up to 
three of them; in particular, no gene-expression levels are directly involved and the model confines 
itself with the Lamis \text{group}. All in all, the sparse model uses \num{7} features. During 
training, we provided the model with a fairly low survival cutoff of \num{1.4} years.

When tested on the Reddy data, $m^*_\text{Schmitz}$ beats the IPI's precision by 5 points. This 
is not enough to show significant superiority in the sense that the \num{95}\%-confidence interval 
of our model's precision does not include the precision of the IPI.

On the Lamis test set, meanwhile, $m^*_\text{Schmitz}$ accomplishes all objectives: with 
\num{50.7}\% precision, it tops the IPI's precision by more than 12 points, narrowly surpassess the 
pychologically important \num{50}\% threshold, and the \num{95}\%-confidence interval of our model's 
precision excludes the IPI's precision. It is remarkable that the $m^*_\text{Schmitz}$, coming from 
the high-risk, non-prospective, RNA-seq environment of the Schmitz data with the Lamis measured with the 
RNA-seq technology, well adapts to the prospective, NanoString regime of the Lamis test data and 
does so better than the models we trained in the previous section on part of the Lamis test data.

\paragraph{Reddy}
$m^*_\text{Reddy}$ looks quite similar to $m^*_\text{Schmitz}$. Instead of the Lamis group, the 
logistic model uses the Lamis score making it its only continuous feature. Its predictor contains 
combinations of up to two discrete features. Hyperparameter tuning yielded a training survival 
cutoff of \num{2.3} years, somewhat below the \num{2.5} cutoff separating patients into high-risk 
and low-risk DLBCLs. 

On the Schmitz data, $m^*_\text{Reddy}$ is more precise than the IPI, but fails to outperform the 
IPI significantly. It yields a fairly high hazard ratio of \num{53.4} 
(\num{95}\%-CI \num{18.7}-\num{152.1}).

On the Lamis test set, $m^*_\text{Reddy}$ boasts a precision of \num{53.2} that is by \num{2.5} 
points higher than that of $m^*_\text{Schmitz}$ and by \num{15} points higher than that of the IPI. 
Consequently, the lower boundary \num{95}\%-confidence interval of its precision at \num{41.5}\% is 
clearly above the IPI's precision and the hazard ratio rises to a considerable \num{23.9}
(\num{95}\%-CI \num{9.9}-\num{57.6}). Even more than $m^*_\text{Schmitz}$, $m^*_\text{Reddy}$ 
defies systematic differences between data sets: comparing the Reddy to the Lamis test data, we 
notice the contrasts non-prospective versus prospective, 
RNA-seq versus NanoString nCounter technology to measure gene expression levels and, most strikingly, 
classifying OS below \num{2.5} years versus PFS below \num{2} years.

\paragraph{Lamis test}
The simplest of the three picked models is $m^*_\text{Lamis test}$. A Cox model, it predicts from 
the Lamis score and five more discrete features -- no combinations of discrete features involved, 
$n_\text{combi} = 1$. With $T = \num{1.4}$, it uses the same low training survival cutoff as 
$m^*_\text{Schmitz}$.

On the Reddy data, our model's precision at \num{50.7}\% stays below the IPI's at \num{54.1}\%. 
Speaking for the whole thesis, this renders the Reddy data a challenging test cohort with a 
hard-to-beat IPI, but -- as we have just seen -- a very helpful training cohort.

Meanwhile on the Schmitz data, the precision of $m^*_\text{Lamis test}$ at \num{75.7}\% exceeds that 
of the IPI by more than \num{10}\%, but falls short of outrivaling the IPI significantly. Still, 
this demonstrates that transferring models between the Schmitz and Lamis data works in both 
directions.

\subsection{Meta analysis}

We again freeze all of our models and unlock the test for all of them to analyze the discrepancy 
between validated and test error and to gain insights on how stable thresholding the models on the 
test cohort is.

\subsubsection{Validated and test errors}

\input{figs/inter_trial_val_test.tex}

Fig. \ref{fig:inter-val-test} compares validation to test error for all candidate models 
participating in the inter-trial experiments; note that again several hyperparameters have already 
been optimized. 

We have emphasized that the risk profiles of the three data sets differ, especially when comparing 
the Schmitz and Reddy to the Lamis test data, so we cannot expect the validation-test-error tuples 
to perfectly align at the identity line. The motivation to increase the sample size was rather 
to observe test errors that are more monotonic in their validation errors. Indeed, Fig. 
\ref{fig:inter-val-test} shows test errors that way more correlated with their validation errors 
than in the intra-trial experiments. As a result, the model with minimal validation error in all 
cases is close to being the one with minimal test error. In more detail, with the exception of 
the models trained on the Lamis test and tested on the Reddy data, the precision of the best 
validated model and that of the best tested model deviate by no more than \num{2}\% points and in 
three of the six cases the best validated and tested model coincide. This is a solid foundation 
to train and validate more models.

As for the model class, we see that no model class prevails clearly. Ignoring the tIPI, the random 
forest always finishes last in validation and never reaches a top position in testing; in fact, 
it claims the last testing place in four out of the six plots. At the top positions, Cox and 
logistic models compete closely: Among the models trained on the Reddy data, a logistic model 
narrowly secures the pole position in validation; testing on both the Schmitz and Lamis test 
data reveals how crucial this victory was as the closest validation competitors of 
$m^*_\text{Reddy}$ cleary lag begind in terms of the test error. Among the models trained on the 
Lamis test data, the same holds true with roles of Cox and logistic swapped. This shows that it 
was crucial to try out both Cox and logistic models although they are closely related; thanks to 
more correlated validation and errors, we need to worry less about overfitted validated 
predictions and might even consider sending analogous Gauss models into the race.

Regarding the question if the predictor should directly contain the gene-expression levels or not,
Fig. \ref{fig:inter-val-test} speaks a plain language. In testing, the top ranking models are 
always such models that do not use gene-expression levels; often the test errors of the models whose 
predictor excludes gene-expression features separate from the rest (cf.\ plots A.1, A.2, B.1). In 
validation however, this distinction can be less obvious, especially for those models trained on 
the Reddy data (cf.\ plots B.1, B.2). In summary, we suffer a lot from the curse of high dimensions 
if we include gene-expression features in the predictor, but gain nothing in terms of test 
performance. Indeed, the models that make the choice of the best validated model on the Reddy data 
close and lucky with regard to testing all use gene-expression features. As the Lamis does a very 
good job at capturing gene-expression information and was trained on an independent data set, we 
conclude that we should just include the Lamis -- and maybe other widely accepted molecular 
signatures -- as an ordinary feature in the predictor and leave aside any other gene-expression 
levels.

\subsubsection{Output thresholding}