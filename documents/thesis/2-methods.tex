\chapter{Methods} \label{chap:methods}

Finding the best possible model for our given task will not be possible from just theoretical 
considerations; we will have to fit several models to our data and demonstrate the performance of 
the chosen one convincingly. Section \ref{sec:train-val-test} will lay out the state-of-the-art 
train-validate-test paradigm for this. In section \ref{sec:candidate-models}, we will introduce 
the candidate models and the hyperparameters governing their fitting process. We will start with 
model-agnostic hyperparameters before we go on to present well-known model classes and finally 
introduce a method that lets us train compositions of multiple models.

\section{Training, validation and testing}\label{sec:train-val-test}

The design of MMML-Predict with its train and test cohort, where the people developing the predictor 
never get to see the test cohort, sets the scene for the standard two-step approach in 
supervised-learning tasks: We use the training cohort to fit multiple models (training) and choose the model
among these models that we have confidence performs best on new data (validation). We then touch 
the test cohort for the first time as we evaluate the chosen model on it to persuade the outside 
world we have come up with model worth deploying.

Since we want to have the same conditions for data used in this thesis as later in MMML-Predict,
we randomly split given data into a train and a test cohort. In more detail, we are initially given 
a cohort of samples $(\mathbf{x}_i, y_i)$, $i = 1, \ldots, n$. Every \textit{observation} or \textit{sample}
$(\mathbf{x}_i, y_i)$ corresponds to a DLBCL patient: $\mathbf{x}_i \in \RR^p$ is a vector of $p$ 
features, the \textit{predictor}, that we want to use to predict the corresponding \textit{response} 
$y_i \in \{0, 1\} \cup (\RR \times \{0, 1\})$; we will explain in section \ref{sec:candidate-models} 
what this odd-looking format for the response encodes, but in this thesis it is always derived from 
the patient's survival. We summarize the 
responses to the response vector $y = (y_i)_{i = 1, \ldots, n}$ and consider the predictors the row 
vectors of the predictor matrix $\mathbf{x} = (\mathbf{x}_i)_{i = 1, \ldots, n} \in \RR^{n \times p}$. We 
call $n$ the \textit{sample size} of the cohort. We randomly partition $I = \{1, \ldots, n\}$ into two 
disjoint subsets $I_\text{train}$ and $I_\text{test}$ and obtain a \textit{training cohort} -- often 
also called \textit{training data} or \textit{training set} --
$(\mathbf{x}_\text{train}, y_\text{train}) = (\mathbf{x}_i, y_i)_{i \in I_\text{train}}$ and a \textit{test 
cohort} -- often also called \textit{test data} or \textit{test set} -- 
$(\mathbf{x}_\text{test}, y_\text{test}) = (\mathbf{x}_i, y_i)_{i \in I_\text{test}}$. The following assumes 
a single, split data set.

\subsection{Training and validation}\label{subsec:train-val}

To be able to discuss some probabilistic caveats of validation later, we introduce some formal 
notation. We start with a set of tuples of hyperparameters $H$, where every $h \in H$ defines a model
up to its parameters. Determining the parameters of a model, by definition, is the job of the 
algorithm optimizing a given loss function; we refer to this as fitting. Every $h \in H$ specifies 
a candidate model and there is a one-to-one mapping between $H$ and the set of candidate models.

For every hyperparameter tuple $h \in H$, we
\begin{itemize}
    \item fit the model to the training cohort subject to $h$ and provide 
        \textit{validated predictions} for 
        every sample in the training cohort.
        A validated prediction for a sample is a prediction made by a model fit 
        subject to $h$ to data that does not include this sample. We obtain a vector of validated 
        predictions $\hat{y}_\text{train} = \text{val}(h)$ with $|I_\text{train}$| entries.
        The most common method to obtain validated predictions is a $k$-fold cross validation 
        \cite{stone74}.
        This means we randomly assign the training samples into $k$ equally sized subsets, called 
        folds, and then fit $k$ models: for every $i = 1, \ldots, k$, we fit a model to all 
        samples that are \textit{not} in the $i$-th fold and obtain its predictions for the samples 
        \textit{in} the $i$-th fold. These predictions serve as (cross-validated) predictions. 
        Doing this for all 
        folds, we obtain a cross-validated prediction for every training sample.
    \item Next, we calculate the validation error 
        $\text{err}(y_\text{train}, \hat{y}_\text{train})$. We will lay out our choice of the 
        error function $\text{err}$ in subsection \ref{subsec:error-function}. For now, its 
        exact definition does not matter.
\end{itemize}
Finally, we select the model $m^*$ with minimal validated error defined by the hyperparameter tuple 
\begin{align}
    h^* = \argmin_{h \in H} \ \text{err}(y_\text{train}, \text{val}(h)).
\end{align}

$\text{val}$ is not a deterministic mapping. The procedure to calculate validated predictions often 
involves randomness, e.g. we randomly assign the training samples into folds in a cross validation 
and many fitting algorithms like that of random forests resort to randomness. The computer does this 
independently between the $\text{val}(h)$. Therefore, we treat the validation errors
$\text{err}(y_\text{train}, \text{val}(h))$ as independent 
real random variables. It is a well known property of independent, identically distributed (i.i.d.) 
real random variables $X_i, i \in \NN$, that their extreme values are notoriously unstable: for all 
$t \in \RR$ with $P(X_1 \geq t) < 1$, 
\begin{align}
    P\left( \min_{1 \leq i \leq n} X_i \geq t \right) = P(X_1 \geq t)^n \to 0 \quad
    \text{as } n \to \infty.
\end{align}

To avoid this disaster -- overfitting the validated predictions to the training 
data --, we need $\text{err}(y_\text{train}, \text{val}(h))$, $h \in H$, with clearly distinct 
distributions. 
How can we achieve this this? First, we can choose $H$ in such a way that the fit models are 
notably distinct. There is, however, a trade-off: the rougher and smaller $H$ is, the more distinct 
the models and their validated errors are, but the higher the chance is that we miss out on a very good 
model. Vice versa, the larger and more granular $H$ is, the more likely one of the $h \in H$ 
specifies a very performant model, but the less likely validation is to reveal this model because,
due to near-i.i.d.\ validation errors in subsets of $H$, another, actually worse model might sneak 
in a greatly underestimated validation error. Second, different methods to calculate validated
predictions can leverage differences in the distribution of the 
$\text{err}(y_\text{train}, \text{val}(h))$ differently. We will not just use cross validation, but 
also so-called out-of-bag predictions in the case of random forests in this thesis. Third, the 
magic bullet of machine learning also helps cure this problem: more training samples will make it 
harder to overfit the validated predictions.

What do we conclude from this for practice? First, we aim to choose $H$ in a smart and lean way. 
We do not want to have big subsets of $H$ that specify more or less the same model. This is hard 
to foresee, but criteria may be theoretical considerations and experience by both us and others 
like default or automatically generated hyperparameters. Second, once we have fit and validated 
all models specified in $H$ and tested the chosen one, we can unlock the test data for all other 
models to analyze the relationship between validated and tested errors. We can look out for subsets 
of $H$ with validated errors strikingly deviating from tested errors or with high test errors and 
exclude them from $H$ in the future. Third, we try to increase the number of samples in training 
and test data by combining data sets. This comes with some caveats and compromises we will deal with 
later, but we hope that more reliable validation errors and better models will compensate for this.

\subsection{Testing}

We calculate the predictions $m^*(\mathbf{x}_{\text{test}, i}) = \hat{y}_{\text{test}, i}$, $i = 
1, \ldots, |I_\text{test}|$, of the 
best validated model $m^*$ on the test cohort and estimate its performance on independent data via 
\begin{align}
    \text{err}(y_\text{test}, \hat{y}_\text{test}).
\end{align}

\subsection{Choice of the error function}\label{subsec:error-function}

As stated in section \ref{sec:intro-mmml}, the MMML-Predictor should detect a high-risk group 
with an as-high-as-possible precision under the constraint that its prevalence must surpass 
\num{10}\%. Since an error should be the lower, the better the model is, we choose 
$\text{err}$ to be the minimum of the negative precisions with a prevalence of at least 17\%. We 
add seven percentage points to the \num{10}\% prevalence as it increases statistical power and makes 
the error function more robust as we will see next. 

Usually, we need 
to take a minimum over \textit{several} precisions because most models do not output the final 
classification.
Instead, they return a continuous score where a higher score means a higher certainty of being
in the positive class, in our case being high-risk. This continuous score needs 
\textit{thresholding} via $\hat{y}_i > t$ for some $t \in \RR$. We call $t$ the \textit{output
threshold} of the model. 
We notice that as we increase $t$ and thereby decrease the prevalence, the obtained 
adjacent precision values fluctuate less and less; for high $t$ and low 
prevalences, fluctuations of up to \num{50} percentage points would be possible, but requiring a 
prevalence of at least 17\% caps such fluctuations at 
\begin{align}
    \frac{1}{\ceil*{\num{0.17} \cdot |I_\text{test}|}}
\end{align}
as an easy calculation shows, cf. also Fig.\ 
\ref{fig:inter-output-prec} in practice. This gives us an error function that is tailored 
for our problem and robust.

As indicated by the notation $\text{err}(y_\text{train}, \hat{y}_\text{train})$ and 
$\text{err}(y_\text{test}, \hat{y}_\text{test})$, we optimize $t$ on the true outcomes 
of the training cohort when validating and we optimize it again on the true outcomes of the test cohort 
when testing. Strictly speaking, $t$ is a hyperparameter, which we optimize on the 
test cohort during testing; this sounds delicate. The results however will show that the optimal 
choice for $t$ (or at least an 
almost optimal choice) on both train and test cohort corresponds to a prevalence of 
slightly above 17\% such that one can agree on the following when it will come to the MMML-Predict 
data: choose $t$ as the 17\%-quantile of the continuous model output on the test cohort.

Similarly, given some cohort, we threshold the IPI in such a way that it maximizes the precision under 
the constraint of a prevalence of at least \num{10}\% on this cohort and call this truly 
binary classifier thresholded IPI, in short tIPI. Usually this amounts to thresholding the IPI via 
$\text{IPI} \geq 4$. The IPI only needs to deliver the minimum 
attention-triggering prevalence of \num{10}\% while its challengers, our models, need a somewhat 
higher prevalence to gain statistical power against the IPI as explained before. Also optimizing 
the IPI's threshold for a given cohort ensures a fair competition.

In general, given data $(\mathbf{x}, y)$ of sample size $n$, we denote the prevalence of a model 
$m$ corresponding to 
$\text{err}(y, m(\mathbf{x}_i)_{i = 1, \ldots, n})$ by $\text{prev}_{\mathbf{x}, y}(m)$ and the corresponding 
precision by $\text{prec}_{\mathbf{x}, y}(m) = -\text{err}(y, m(\mathbf{x}_i)_{i = 1, \ldots, n})$. When it 
is clear from the context which data set or subset of a data set we are talking about, we just write 
$\text{prev}(m)$ and $\text{prec}(m)$.

\subsection{Inter-technical variability} 
One could instead suggest to already optimize the output threshold of our models on the cross-validated 
predictions. While this promises a stricter train-test regime at first glance, fixing the output 
threshold once and for all neglects \textit{inter-technical variability}. 

An always-present problem in bioinformatics, inter-technical variability refers to the fact that 
one and the same sample measured on different platforms, in different labs or at different times, 
in short: under different protocols, may lead to different values for the same feature. 
Values for the same feature and sample measured in different batches, even if the lab, preparation 
and technology are the same, may differ. We will therefore also use the term batch effects for 
inter-technical variability. Every data set adheres to its own protocol, so we need to deal with 
inter-technical variability whenever we combine data sets or have a model predict on a data set 
other than the one it was trained on.

If we 
measure the same $p$ features of the $n$ patients in $\mathbf{x} \in \RR^{n \times p}$ again under 
another protocol, we end up with another predictor 
$\mathbf{z} \in \RR^{n \times p}$.
Following \cite{transplatform17}, for logarithmized gene-expression levels,
we can often well model the discrepancies with two independent biases, sample-wise effects $\theta_i$
and feature-wise effects $\omega_j$, leading to
\begin{align}\label{eq:inter-tech-exact}
    \Delta_{ij} = \mathbf{z}_{ij} - \mathbf{x}_{ij} = \theta_i + \omega_j + \epsilon_{ij}
\end{align}
for residues $\epsilon_{ij}$. Assuming accurate modeling, i.e. small residues, we can well approximate
\begin{align}
    \mathbf{z}_{ij} \approx \tilde{\mathbf{z}}_{ij} = \mathbf{x}_{ij} + \theta_i + \omega_j.
\end{align}
We can now apply an ordinary linear model with parameters $(\beta_0, \beta)$ to $\mathbf{z}$ 
and obtain
\begin{align} \label{eq:inter-tech}
\begin{split}
    \beta_0 + \sum_{j=1}^p \beta_j \mathbf{z}_{ij} &\approx \beta_0 + \sum_{j=1}^p \beta_j \tilde{\mathbf{z}}_{ij} \\
    &= \beta_0 + \sum_{j=1}^p \beta_j \mathbf{x}_{ij} + \sum_{j=1}^p \beta_j \theta_i + \sum_{j=1}^p \beta_j \omega_j.
\end{split}
\end{align}
The third summand is zero if $\sum_{j = 1}^p \beta_j = 0$, a special case of the \textit{zero-sum 
constraint}, which we will introduce in more detail in subsection \ref{subsec:core-models}. 
The fourth summand is constant across all samples and can be absorbed by the intercept. 

Under these assumptions, which are realistic by \cite{transplatform17},
going from one protocol to another just leads to a constant shift of the model 
output; even for generalized linear models, i.e. a Gauss model composed with some monotonic link function,
inter-technical variability does not change the ordering of the output scores belonging to the 
samples. All one needs to do to 
obtain a final classification is calibrating the threshold for the output scores. However, this 
also demonstrates that while generalized linear models can cope with inter-technical variability 
pretty well, there is no point in using the same output threshold across all protocols.

\section{Candidate models}\label{sec:candidate-models}

\subsection{Model-agnostic hyperparameters}\label{subsec:model-agnostic}

Model-agnostic hyperparameters are those hyperparameters we can set and tune for every model. They 
concern the predictor matrix $\mathbf{x} \in \RR^{n \times p}$ and the response vector $y \in 
(\{0, 1\} \cup (\RR \times \{ 0, 1 \}))^n$. The union in the latter set reflects the fact that 
can provide the response in two formats. 
\begin{itemize}
    \item In the first case, the response is a binary vector with 1 encoding the positive class, 
        high-risk DLBCL, and 0 encoding the negative class, low-risk DLBCL. We call this the 
        \textit{binary format}. Here, we need to discard samples censored before two years, but this 
        is typically only a small proportion of the samples in the data set. 
    \item In the second case, the response has two entries per sample: the \textit{time to the 
        event} and \textit{censoring status}. If the censoring status is 1, the event -- in our case 
        cancer progression -- occurs \textit{at} the time to the event. If the censoring status is 
        0, 
        we only know that the event will occur at some point \textit{after} the time to the event.
        We call this the \textit{Cox format}. More on this in subsection \ref{subsec:core-models} 
        when we present the Cox model. 
\end{itemize}

Which of these two formats we provide for a sample 
depends on the model class. Our choice of $\text{err}$ ultimately needs the true response $y$ in 
the binary format, which $\text{err}$ can easily derive from $y$ in the Cox format if necessary.

\subsubsection{A-priori feature selection}

Not every feature in the data set should be part of the predictor matrix $\mathbf{x}$. Including 
features measured toward the end of a patient's therapy or even after it, is cheating and we 
should definitely exclude them. Excluding any of the remaining features is hard to justify: If we 
know for a feature that it alone is associated with the response, we include it in $\mathbf{x}$ 
without doubt.
On the other hand, if a feature alone shows no link to the response, the right model might still be 
able to leverage it in combination with other features; but at this step, we do not know if such 
a model exists and if it is among our candidate models. The only remaining option for handpicking 
features a priori is to brute-force the problem and try out all $2^p$ combinations of features.
Even if we only do this for part of the features, say \num{10} features, this would still
leave us with $2^{10} = 1024$ combinations to try out -- just for this hyperparameter. While this 
may be computationally feasible, validation would be a statistical fiasco as laid out in subsection 
\ref{subsec:train-val}. Therefore, we reduce this quickly exploding number of choices to only 
a handful and let \textit{regularization} handle the large number of features in $\mathbf{x}$. More 
on this in subsection \ref{subsec:elastic-net}.

\paragraph{Gene-expression data}
The first and most important decision to make is: do we want to include the high-dimensional
gene-expression part of the data at all? If we do not, we suddenly have $p \ll n$ instead of 
$p > n$ or even $p \gg n$. This strongly affects the models we fit, hence also $H$ and 
validation.

\paragraph{Features in different formats}
Second, we need to make a couple of more decisions regarding the format of some features.
\begin{itemize}
    \item Concerning the IPI, we can include the five features involved in the IPI in their original
        format, i.e.\ age and LDH level as a continuous feature, Ann Arbor stage and performance 
        status as a categorical feature and the number of extranodal sites as a discrete feature.
        We can also binarize them with the same thresholds as the IPI inventors and include five 
        binary features. We call this the \textit{thresholded format of the IPI features}. 
        Similarly, widely accepted thresholds may exist for other clinical and genetic 
        features.
    \item For already existing models, we can either include their continuous output or threshold 
        it. For the IPI, we can include its output as a single continuous feature. We can also 
        include the so-called IPI group, a partitioning of the IPI output into three groups, which 
        one obtains by thresholding it at \num{1.5} and \num{3.5}. As another example, 
        cell-of-origin signatures, whose output is continuous to begin with, can be thresholded into 
        ABC, GCB and unclassified subtypes. Data sets often only provide this grouping and not the 
        raw, continuous output. 
\end{itemize}

E.g., imagine a data set provides the five IPI features in their original format, from which we can 
easily infer all other mentioned formats, and the continuous output of some gene-expression 
signature for which its inventors also provide a preferred threshold for a binary grouping. 
If we want to decide for 
exactly one format per feature, this leaves us with $4 \cdot 2$ possibilities for this simple case. 
If we want try out every combination of features in any format, the number of possibilities jumps to 
$2^4 \cdot 2^2 = 64$. 

Our solution here again is to be generous and include all formats of a feature in $\mathbf{x}$ a 
certain model may benefit from. This sentence usually 
simplifies to: include all widely used formats of a feature in $\mathbf{x}$. E.g., generalized 
linear models (more in cf.\ \ref{subsec:core-models}) cannot threshold continuous features, so a 
feature in its thresholded format may give 
a rough contribution to the prediction while the same feature in its continuous form may further 
refine it. Moreover, even decision trees can benefit from a feature additionally provided in its 
thresholded format if this threshold has been inferred on a much bigger data set since the decision 
tree may not be able to find the threshold itself.

\subsubsection{Imputation}

If values in $\mathbf{x}$ are missing, i.e. written as \texttt{NA}, actions we can take fall into two 
categories: we discard the part of data affected by missing values or we replace the missing values 
with some realistic estimate.

\paragraph{Discarding part of the data}

In a first step, we discard a feature if it is not available for a large proportion of the samples.
This enables reasonable computing.

\paragraph{Imputing}

At this point, we dichotomize a categorical 
feature with $c$ categories into $c-1$ binary dummy features; if for a sample this categorical feature is not 
available, all $c-1$ dummy features will be \texttt{NA}. This yields a modified $\mathbf{x}$ that 
bears exactly the same information as before. For \textit{every} column in $\mathbf{x}$, we calculate the 
mean of the available features in $\mathbf{x}$ and replace the \texttt{NA} values with it. 
For a missing categorical feature, every dummy feature then holds the marginal Bernoulli 
probability that the feature is in the $k$-th category. 

\subsubsection{Adding combinations of categorical features to the predictor}

Next, we add all combinations of at most $n_\text{combi}$ categorical features that are positive in a 
share of at least $s_\text{min}$ patients to $\mathbf{x}$; e.g., we add a column ``female \& ABC-type 
tumor'' if at least \num{5}\% of patients have this property. We always choose $s_\text{min}
= 5\%$. We set $n_\text{combi} = 1$ for models that facilitate interactions between features 
themselves, otherwise we set $n_\text{combi} = 3$.

Technically, we need to combine the binary features we derived from distinct categorical features 
in the previous step. We realize this by multiplying the corresponding columns in 
$\mathbf{x}$ element-wise. We thereby keep treating the value of a binary feature as a Bernoulli
probability under the assumption that the binary dummy features involved in the combination are 
independent, which is not always justified. The average of a combined (Bernoulli) feature over all 
samples, for which we apply the threshold $s_\text{min}$, therefore approximates the expected value 
of the combined feature.

\subsubsection{Tuning the definion of ``high risk''}

Defining a patient as high-risk if and only if the PFS is less than two years
is a clinically accepted, yet quite arbitrary decision. It may not be the time cutoff that 
separates PFS best from a biological point of view. Therefore, we can provide the fitting 
algorithm a refined response $y$, governed by the \textit{training survival cutoff} $T \in 
\RR_{> 0} \cup \{ +\infty \}$.
\begin{itemize}
    \item For $y$ in the binary format, we set $y_i = 1$ if the PFS 
        of patient $i$ is less than $T$ and $y_i = 0$ otherwise. The higher $T$ is, the more 
        samples we need to discard due to censoring.
    \item For $y$ in the Cox format, we censor all samples with time to the event exceeding $T$ at 
        $T$.
\end{itemize}
Patients may separate much more clearly for some $T \neq 2$ and as long as we come up 
with a positive cohort that comprises at least 15\% of the samples of the test data, this allows us 
to fulfill our task more easily. We stress that $T$ only influences how we train models. For 
validation and testing, a PFS of \num{2} years stays the crucial cutoff.

\subsection{Core models}\label{subsec:core-models}

All models trained, validated and tested in this thesis at the core consist of ordinary linear, 
logistic and Cox proportional-hazards models with additional properties, as well as random forests.
In this chapter, we want to introduce the design of these models and the hyperparameters governing 
their fitting process.

Formally, we deal with a probability space $(\Omega, \mathcal{A}, P)$ that represents the whole 
population of DLBCL patients. We do not and cannot specify this probability space any further because 
we only get in touch with two random variables 
\begin{align}
    X = (X_1, \ldots, X_p): \Omega \to \RR^p \text{ and } Y: \Omega \to \RR,
\end{align}
the predictor and the response, respectively. More 
precisely, we observe independent training samples $(\mathbf{x}_i, y_i) \in \RR^{p+1}, i = 1, \ldots, n$, 
distributed according to $(X, Y)$; $\mathbf{x}_i$ is the $i$-th row of the predictor matrix $\mathbf{x}$ and
$y_i$ is the $i$-th entry of the response vector $y$. The i.i.d.\ test samples follow the same 
distribution as the training samples, but in this subsection everything is about training. If we 
say samples in this subsection, we always refer to the training samples.

\subsubsection{Generalized linear models}\label{subsubsec:glm}

Here, we work with binary response, i.e. $\mathrm{im}(Y) \subset \{0, 1\}$. Both ordinary 
linear models and logistic regression models are generalized linear models (GLMs). In 
a GLM, $Y$ follows an \textit{overdispersed exponential family of distributions} and there is an 
invertible \textit{link function}
$g: \RR \to \RR$ and parameters $(\tbeta_0, \tbeta) \in \RR^{p+1}$ such that
\begin{align}
    g(E(Y \cond X = x)) = \tbeta_0 + \sum_{j=1}^p \tbeta_j x_j \quad 
    \text{for all } x \in \im(X) \subset \RR^p.
\end{align}
where $E(Y \cond X = x)$ is the expected value of $Y$ given $X = x$ \cite{glm72}. We can express the 
\textit{linear predictor} $\sum_{j=1}^p \tbeta_j x_j$ in terms of the Euclidean scalar product 
as $x^T \tbeta$, where $x^T$ denotes the transpose of the column vector $x$. We will only introduce 
the two special cases of an overdispersed exponential family of distributions that matter to us in 
this thesis below. For a general definition, see \cite[section 1.1]{glm72}.

In practice, we do not know the (true) parameters
$(\tbeta_0, \tbeta)$. But we do have our $n$ i.i.d.\ observations $(\mathbf{x}_i, y_i)$
and we will use them together with a likelihood to estimate $(\tbeta_0, \tbeta)$. For an 
arbitrary $(\beta_0, \beta) \in \RR^{p+1}$, we can try to predict the conditional expectation as
\begin{align}\label{eq:glm-mu}
    \mu_i(\beta_0, \beta) = g^{-1}(\beta_0 + \mathbf{x}_i^T \beta), \quad i = 1, \ldots, n.
\end{align}
We have $\mu_i(\tbeta_0, \tbeta) = E(Y \cond X = \mathbf{x}_i)$.

\paragraph{Gauss model}
Here, $g = \text{id}$. $Y$ and $Y \cond X = x$ for all $x \in \RR^p$ follow a normal
distribution with fixed standard deviation $\sigma > 0$ (a property called homoscedasticity), but 
varying mean. For a normal distribution with standard deviation $\sigma$, the log likelihood of an 
expected value -- or equivalently mean -- $\mu$ given an observation $y \in \RR$ is
\begin{align}
\begin{split}
    \ell(\mu; y) &= \log\left( \frac{1}{\sqrt{2\pi}\sigma} 
        \exp \left( -\frac{1}{2\sigma^2}(y - \mu)^2 \right) \right) \\
    &= -\frac{1}{2\sigma^2}(y - \mu)^2 - \log \left( \sqrt{2\pi}\sigma \right).
\end{split}
\end{align}
Up to a constant shift and re-scaling, which does not affect maximizing the log likelihood, this
is the well-known squared error.

\paragraph{Logistic model}
Here, the linear predictor is the log-odds for $Y = 1$ over $Y = 0$ given $X = x$, i.e.
\begin{align}
    g: \mu \mapsto \log\left( \frac{\mu}{1 - \mu} \right), \text{ hence } g^{-1}: \eta \mapsto
    \frac{1}{1 + \exp(-\eta)}.
\end{align}
Y and $Y \cond X = x$ for all $x \in \RR^p$ follow a Bernoulli distribution with varying success 
probability. For the Bernoulli distribution, the log likelihood of an expected value -- or 
equivalently success probability -- $\mu$ given an observation $y \in \{0, 1\}$ is
\begin{align}
    \ell(\mu; y) &= \log\left( \mu^{y} (1 - \mu)^{1 - y} \right) 
    = y \log(\mu) + (1 - y) \log(1 - \mu). 
\end{align}

\paragraph{Loss function}
Using Eq.\ \eqref{eq:glm-mu}, the log likelihood for $(\beta_0, \beta)$ given our i.i.d.\ samples 
$(\mathbf{x}_i, y_i)$, $i = 1, \ldots, n$, for both the Gauss and logistic model therefore is
\begin{align}
    L(\beta_0, \beta) &= \sum_{i=1}^n \ell(\mu_i; y_i) 
    = \sum_{i=1}^n \ell\left( g^{-1}\left( \beta_0 + \mathbf{x}_i^T \beta \right); y_i \right).
\end{align}
We can augment $L$ with two hyperparameters, sample weights $w_i > 0$, $i = 1, \ldots, n$, and the 
zero-sum constraint with respect to zero-sum weights $u_j \geq 0$, $j = 1, \ldots, p$, yielding 
\begin{align} \label{eq:loss-glm-no-lasso}
    L(\beta_0, \beta) &= \sum_{i=1}^n w_i \ell\left( g^{-1}\left( \beta_0 + \mathbf{x}_i^T \beta \right); 
    y_i \right) \quad \text{for } \beta \in \RR^p \text{ with } u^T \beta = 0.
\end{align}
While we do not deviate from the default value $1/n$ for the sample weights in the results chapter 
\ref{chap:results}, we will discuss in chapter \ref{chap:discussion} how we can use them in the 
future in a natural way without getting lost in the vast amount of freedom that one has in 
choosing them. 
With these augmentations, $L$ is no longer a log likelihood, so we call it a \textit{loss function}.

Fitting a Gauss or logistic model to the data means minimizing $L$ with respect to 
$(\beta_0, \beta)$ and coming up with a minimizer $(\hbeta_0, \hbeta)$, which is our estimate for 
$(\tbeta_0, \tbeta)$. Subsection \ref{subsec:elastic-net} will deal with the question how we can 
make $(\hbeta_0, \hbeta)$ a \textit{unique} minimizer.

We want to demonstrate what the zero-sum constraint is good for in a more general setting. Imagine, 
part of the features suffer from sample-wise 
shifts. We denote these features by $J \subset \{1, \ldots, p\}$ and the shifts by $s_i \in R$, 
$i = 1, \ldots, n$. We now set the zero-sum 
weights  
\begin{align}
    u_j =
    \begin{cases}
        c & \text{if } j \in J, \\
        0 & \text{else},
    \end{cases}
\end{align}
for some $c > 0$ (usually $c = 1$); we can express this conveniently with the help of the indicator 
function $\chi_J$ as $u_j = \chi_J(j) c$. By the zero-sum constraint, we demand 
$\sum_{j \in J} c \beta_j = 0$  
and hence $\sum_{j \in J} \beta_j = 0$; apart from that, $(\beta_0, \beta)$ is arbitrary. 
In this situation, the linear predictor 
\begin{align}\label{eq:zerosum-sample-shifts}
    \beta_0 + \sum_{j=1}^p \beta_j (\mathbf{x}_{ij} + s_i \chi_J(j))
    = \beta_0 + \mathbf{x}_i^T \beta + \sum_{j \in J} \beta_j s_i = \beta_0 + \mathbf{x}_i^T \beta,
\end{align}
as $\sum_{j \in J} \beta_j s_i = s_i \sum_{j \in J} \beta_j = 0$, is invariant. If $J$ consists of 
exactly the features holding gene-expression levels, we can transfer our model to other protocols 
and only need to re-calibrate the intercept, as shown in Eq. \eqref{eq:inter-tech}. Moreover, we 
can switch off the zero-sum constraint for all features by setting $u_j = 0$ for all 
$j = 1, \ldots, p$. The zero-sum constraint is cheap in terms of model complexity: it only removes 
one degree of freedom; but it is expensive in terms of computational complexity: enforcing it in
every step of the coordinate descent leads to a considerably higher computation time. 

As we have stressed several times already, the threshold that makes a GLM a binary classifier 
is rather a property of the underlying data set than of the model itself. The essential information 
the modelling function contributes to the classification of a cohort of samples is therefore the 
ordering of the output scores. Because $g$ and therefore also $g^{-1}$ are monotonically increasing, 
the essential part of the model is the linear predictor $\mathbf{x}_i^T \beta$. For this reason, when we 
talk about the output of a GLM, we always mean the output of the linear predictor. For us, this 
makes $g$ more a tweak in the loss function, important for training, than a part of the modelling 
function.

We are not done, yet, with $L$: at the end of the day, a coordinate descent minimizes 
Eq.\ \eqref{eq:loss-glm-no-lasso} with a regularization term added. Before we discuss this, we want 
to introduce a model that is closely related to GLMs.

\subsubsection{Cox proportional-hazards model}\label{subsubsec:cox}

For the Cox proportional-hazards model -- in short: Cox model -- the response variable $Y$ 
measures the time \textit{until} the event of interest occurs (``event'') or the time 
\textit{after} which the event occurs (``censoring''). Another binary random variable 
$\delta: \Omega \to \RR$ encodes 
which of these two options is the case: $\delta(\omega) = 1$ for the event, $\delta(\omega) = 0$ 
for censoring. This pays tribute to an important characteristic of survival trials: some
participants drop out of the trial before the event could happen -- e.g. the trial terminated, the 
patient decided to withdraw or died from another cause -- or the event luckily never happens.

\paragraph{Hazard function} To understand what the Cox model predicts, we need another random 
variable $T: \Omega \to \RR$, the time to the event. Unlike $Y$, $T$ is not affected by censoring. 
We require $T$ to have a density $f_T: \RR \to \RR$ and define its hazard function $h: \RR \to \RR$
via
\begin{align}
    h(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \cond T \geq t)}{\Delta t}.
\end{align}
We can interpret $h(t)$ as the instantaneous rate at which the event is occurring at time $t$, 
given that the event has not occurred before time $t$. Let $F_T$ be the distribution function of 
$T$. Then 
we can write
\begin{align}
    \frac{P(t \leq T < t + \Delta t \cond T \geq t)}{\Delta t} = 
    \frac{F_T(t + \Delta t) - F_T(t)}{\Delta t \cdot (1 - F_T(t))}.
\end{align}
Hence, 
\begin{align}
    h(t) = \lim_{\Delta t \to 0} \frac{F_T(t + \Delta t) - F_T(t)}{\Delta t \cdot (1 - F_T(t))} 
    = \frac{f_T(t)}{1 - F_T(t)} = \frac{f_T(t)}{S_T(t)},
\end{align}
where $S(t) = 1 - F_T(t)$ is the survival function, and $h(t)$ is well-defined as long as $S_T(t)
> 0$.

\paragraph{Conditional hazard}
The Cox model, proposed in \cite{cox72}, wants to get a hand on \textit{conditional} hazards.
Intuitively, it is clear that the population with one value of $X$ can have a vastly different 
hazard function than the population with another value of $X$ if $X$ has predictive power for $T$.
Formally, however, it is hard to define a conditional hazard function: $X$ may very well 
follow a continuous distribution, hence we condition on an event of probability $0$ when we write 
$P(t \leq T < t + \Delta t \cond T \geq t, X = x)$ -- it is hard to define this in a natural
and straightforward way. In practice, the measurements in $X$ often fluctuate around their true 
values anyway, so it is sensible to condition on a small neighborhood of $x$, say an $\epsilon$-ball 
around $x$ with respect to the Euclidean norm $|\cdot|_2$ for some small $\epsilon > 0$. This 
amounts to
\begin{align}
    h(t \cond X = x) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \cond T \geq t, 
    |X - x|_2 < \epsilon)}{\Delta t}
\end{align}
or -- equivalently -- $h(t \cond X = x)$ is the hazard of $T$ restricted to $(\Omega, \mathcal{A}, 
P)$ conditioned on $\Omega_x = \{ \omega \in \Omega: |X(\omega) - x| < \epsilon \}$; unlike before, 
we usually now have $P(\Omega_x) > 0$.

\paragraph{Proportional hazards} The Cox model assumes \textit{proportional} conditional hazards, 
i.e. 
\begin{align}
    h(t \cond X = x) = \lambda_x h_0(t) 
\end{align}
for all $x \in \im(X)$ and some (unspecified) baseline hazard $h_0$ and $\lambda_x > 0$. 
Moreover, it assumes $\lambda_x$ depends on $x$ via 
\begin{align}\label{eq:cox-proportional-factor}
    \lambda_x = \exp(x^T \tbeta),
\end{align}
for parameters $\tbeta \in \RR^p$.

\paragraph{Partial log likelihood}
Given our independent samples $(\mathbf{x}_i, y_i, \delta_i) \in \RR^{p+2}$, $i = 1, \ldots, n$, distributed 
according to $(X, Y, \delta)$, how can we estimate $\tbeta$? The Cox model is a semi-parametric 
model in that it leaves the baseline hazard $h_0$ unspecified. Consequently, estimation with maximum 
likelihood is not possible. This led to another break-through invention in \cite{cox72}, the 
\textit{partial} likelihood, which we will present now. Let $\beta \in \RR^n$ be arbitrary and let 
$E \subset \{ 1, \ldots, n \}$ refer to the non-censored samples ($\delta_i = 1$). For $i \in E$, 
let $R_i$ denote the set of samples 
where no event has occurred until $y_i$, i.e. $y_j \geq y_i$ for all $j \in R_i$. For
fixed $i \in E$ and time to event $y_i$, conditionally on the risk set $R_i$, the probability that 
the event occurs on sample $i$ as observed is
\begin{align}
    \frac{h(y_i \cond X = \mathbf{x}_i)}{\sum_{j \in R_i} h(y_i \cond X = \mathbf{x}_j)} = 
    \frac{\exp(\mathbf{x}_i^T \beta) h_0(y_i)}{\sum_{j \in R_i} \exp(\mathbf{x}_j^T \beta) h_0(y_i)} = 
    \frac{\exp(\mathbf{x}_i^T \beta)}{\sum_{j \in R_i} \exp(\mathbf{x}_j^T \beta)}
\end{align}
according to \cite[Eq. (12)]{cox72}.

Taking all events into account yields the partial likelihood 
\begin{align}\label{eq:cox-partial-lh}
    \tilde{\ell}(\beta) = \prod_{i \in E} \frac{\exp(\mathbf{x}_i^T \beta)}{\sum_{j \in R_i} \exp(\mathbf{x}_j^T 
    \beta)}.
\end{align}
Because $T$ is continuous, there is a zero probability for two distinct samples having the same 
event time, so we need not resort to one of the more sophisticated likelihoods capable of handling ties.
Analogously to Eq. \eqref{eq:loss-glm-no-lasso}, we can equip the partial log likelihood with sample 
weights $w_i > 0$ and zero-sum weights $u_j \geq 0$ such that the refined log partial likelihood, 
which we now call a loss function, reads 
\begin{align}\label{eq:cox-loss}
\begin{split}
    L(\beta) &= \sum_{i \in E} w_i \mathbf{x}_i^T \beta - \log \left( \sum_{j \in R_i} w_j \exp(\mathbf{x}_j^T
    \beta) \right) \quad \text{for } \beta \in \RR^p \text{ with } u^T \beta = 0.
\end{split}
\end{align}
As with GLMs, we can leverage the zero-sum constraint to reach invariance under sample-wise shifts. 
Our estimate for $\tbeta$ is a minimizer of $L$ with respect to $\beta$, denoted by $\hbeta$. 
More on its unique existence in subsection \ref{subsec:elastic-net}.

There are more analogies to GLMs. In Eq.\ \eqref{eq:cox-proportional-factor}, we also compose the 
linear predictor with an increasing function. Given a cohort of samples, we are still most 
interested in the ordering of the model output and the linear predictor $\mathbf{x}_i^T \beta$ 
already uniquely determines this ordering. Again, when we talk about the output of a Cox model, we 
mean the output of the linear predictor. In light of all these similarities,
in this thesis we refer to both GLMs in the typical sense and Cox models as GLMs. Looking closely,
the Cox model gets along without an intercept. We will therefore always ignore $\beta_0$ if 
mentioned in the context of a Cox model in the following.

This paves the way to define some special cases of the zero-sum constraint for the three model 
classes. We say that a subset of the coefficients of a GLM indexed by $J \subset \{1, \ldots, p \}$ 
fulfills the \textit{zero-sum property} if $\sum_{j \in J} \beta_j = 0$. If $J = \{1, \ldots, p \}$, 
we say that the GLM fulfills the zero-sum property. If, additionally, the GLM is a signature, we 
call it a \textit{zero-sum signature}. By Eq.\ \ref{eq:zerosum-sample-shifts}, being a 
\textit{zero-sum} gene-expression signature can be a crucial advantage over just being 
gene-expression signature.

\paragraph{Hazard ratio}

We can use the Cox model to calculate a \textit{hazard ratio} (HR) between two groups of samples. 
Suppose $g_i \in \{0, 1\}$ encodes this grouping. In our case, $g_i = f(\mathbf{x}_i)$ is always 
the output of a binary classifier $f$. We now fit a univariate Cox model to $(g_i, y_i, \delta_i)$, 
$i = 1, \ldots, n$, by maximizing 
the partial likelihood in Eq.\ \eqref{eq:cox-partial-lh} with respect to $\beta$ and obtain an estimate
for the (scalar) model coefficient $\hbeta$. By Eq.\ \eqref{eq:cox-proportional-factor}, 
$\exp(\hbeta)$ is the relative risk increase of being in group $g_i = 1$ compared to group 
$g_i = 0$ and we call it hazard ratio.

We can view the maximum partial-likelihood estimate $\hat{\beta}$ as a random variable that depends 
on i.i.d.\ sampling $n$ training samples. In analogy with maximum-likelihood estimation, there are 
good arguments to believe that the maximum \textit{partial} log-likelihood estimator $\hat{\beta}$
is
\begin{itemize}  
    \item \textit{consistent}, i.e., with growing sample size $n$, $\hbeta$ converges in 
        probability to the true coefficient of the Cox model $\tbeta$, and
    \item \textit{asymptotically normal}, i.e., with growing sample size, $\hbeta$ converges 
        in distribution to a normal distribution with mean $\tbeta$ and a standard deviation we 
        can estimate with the help of the second derivative of the partial log likelihood with 
        respect to $\beta$ evaluated at $\hbeta$ \cite[8.1--8.4]{klein03}.
\end{itemize}
This justifies to treat $\hat{\beta}$ as normal with known variance and allows us to
calculate confidence intervals for the HR $\exp(\tbeta)$ -- the mean of this normal distribution -- 
as well as a two-sided p-value for the null-hypothesis $\exp(\tbeta) = 1$, i.e.\ identical risk 
between the two groups. 

\subsection{Elastic-net regularization} \label{subsec:elastic-net}

Modern molecular measurements are a big factor in our hope that MMML-Predict can succeed and improve 
the IPI. These measurements, like gene-expression levels, show up as hundreds if not thousands of 
features in our data, meaning we usually have $p > n$ if not $p \gg n$. In this situation, we 
cannot uniquely determine the parameters $(\hbeta_0, \hbeta)$ by minimizing 
Eq.\ \eqref{eq:loss-glm-no-lasso} or Eq.\ \eqref{eq:cox-loss}:
the models have way more degrees of freedom than we have samples, they will find biologically 
meaningless and non-reproducible structures in the training data and generalize poorly to new 
data.

A solution to this is to restrict the freedom of the model to the extent that it is forced to 
learn the gist from the data. For that reason, we want to penalize model complexity. Elastic-net 
regularization, a widely adopted method proposed by Zou and Hastie \cite{elasticnet05}, does this 
job for us. The elastic net generalizes two well-established regularization methods,
\begin{itemize}
    \item ridge regularization \cite{ridge70}, which forces the parameter vector $\beta$ to 
        stay inside a ball around the origin by limiting the squared $\ell_2$ norm 
        of the parameters, and 
    \item LASSO (least absolute shrinkage and selection operator) regularization \cite{lasso18}, 
        which forces $\beta$ to stay inside a diamond centered at the origin by limiting the 
        $\ell_1$ norm of the parameters.
\end{itemize}
It does so by combining them with a weight factor $\alpha \in [0, 1]$ and feature-wise penalty 
weights $v \in \RR_{\geq 0}^p$ into
\begin{align}\label{eq:enet-weights}
    p_{\text{enet}, \alpha, v}(\beta) = \sum_{j=1}^p v_j \left( \frac{1-\alpha}{2} \beta_j^2 +
    \alpha |\beta_j| \right).
\end{align}
Multiplied with the regularization strength $\lambda \in \RR_{\geq 0}$, we can add it to the loss 
function $L$ from Eq.\ \eqref{eq:loss-glm-no-lasso} and Eq.\ \eqref{eq:cox-loss} to obtain the 
regularized loss function in its Lagrangian form as 
\begin{align}\label{eq:loss-elastic-net}
    \mathcal{L}(\beta_0, \beta) = -L(\beta_0, \beta) + \lambda p_{\text{enet}, \alpha, v}(\beta) 
    \quad \text{ for } \beta \in \RR^p \text{ with } u^T \beta = 0.
\end{align}

In all three cases -- Gauss, logistic and Cox model -- it is possible to approximately minimize 
$\mathcal{L}$ with respect to $(\beta_0, \beta)$ with a coordinate-descent algorithm 
\cite[section 2.3]{zerosum16}. We have finally arrived at the loss function we will optimize to 
fit models in chapter \ref{chap:results}.

The default value for the penalty weights is $v_j = 1$ for all $i = 1, \ldots, p$. Deviating from 
the default can be useful 
in at least two cases. First, sometimes we are so convinced of the predictive 
power of a feature in our model that we set $v_j = 0$ for it to all but ensure a non-zero 
coefficient for it. The second use case is \textit{standardizing the predictor matrix}, which 
we will deal with further below.

\paragraph{Effect on correlated features}
While both ridge and LASSO regularization shrink the coefficients, they act differently in the case 
of correlated features. Ridge regression tends to force the coefficients of correlated features to 
similar values while the LASSO tends to pick one of them and set the others to zero. E.g, in the 
extreme case of $k$ identical features, the LASSO arbitrarily picks one of them with, say, 
coefficient $a$; the ridge regression, meanwhile, assigns every of the $k$ features the coefficient 
$a/k$. Relying on very few features comes with the advantage of sparse models, for which we 
can cheaply generate new data, but it is very sensitive to measurement errors in one of the picked 
features; in the latter case, ridge regularization would average out errors with the help of other 
correlated features. Elastic-net regularization balances the sparsity of the LASSO with the 
robustness of the ridge regularization, especially for values of $\alpha$ close to $1$ 
\cite{elasticnet05}.


\paragraph{Standardizing the predictor matrix} 
The second application of the penalty weights is an aspect of a bigger topic: standardizing the
predictor matrix. We can use the weights to standardize $\mathbf{x}$ by setting $v_j$ to an estimate 
of the standard deviation of feature $X_j$. Why 
should we do this? Let us consider two features $X_1$ and $X_2$ with standard deviation $\sigma_1$
and $\sigma_2$, respectively, and $\sigma_1 < \sigma_2$. To change the output of the model by $1$ 
when the feature changes by 
one standard deviation and all other features keep their values, $X_1$ needs the coefficient 
$\beta_1 = 1/\sigma_1$ and $X_2$ needs $\beta_2 = 1/\sigma_2$. We have $\beta_1 > \beta_2$. For 
the default penalty weights $v_1 = v_2 = 1$, the elastic net penalizes $X_1$, the feature with a smaller 
standard deviation, which in turn needs a higher coefficient, more. For $\alpha$ close to 
\num{1}, setting $v_1 = \sigma_1, v_2 = \sigma_2$, we approximately offset this difference.
Standardization strives to guarantee equal justice for features 
on different scales. 

Standardization works, however, contrary to the zero-sum idea. Thanks to the 
zero-sum constraint, the fitting process and the final model are invariant under sample-wise 
shifts on a subset of features if we set the zero-sum weights accordingly (cf.\ Eq.\ 
\eqref{eq:zerosum-sample-shifts}). Sample-wise shifts 
change the standard deviation of the affected features, which, with standardization,
affects the cost function and likely leads to another model. 
Consequently, when deploying the zero-sum constraint in some way, one should stick to the default 
value $v_j = 1$ for the features for which we wish to have shift-invariance; these are usually the 
gene-expression levels, which are on comparable scales anyway.

\paragraph{Choosing a $\lambda$ sequence}
As we increase the regularization strength $\lambda$, the set of minimizers for
Eq.\ \eqref{eq:loss-elastic-net} becomes smaller and smaller. For $\lambda$ large enough, there exists 
a unique, trivial solution $(\hbeta_0, \hbeta)$ with $\hbeta = 0$. For the above-mentioned 
coordinate descent, given 
training data, we can approximate the smallest $\lambda$ that still yields a 
trivial solution, denoted by $\lambda_\text{max}$ \cite[section 3.3]{rehberg-thesis18}. Following 
\cite[section 2.5]{regularization-path10}, we choose a minimum value $\lambda_\text{min} = 
\epsilon \lambda_\text{max}$ for a small $\epsilon > 0$ and 
construct a sequence of $n_\lambda$ values decreasing from $\lambda_\text{max}$ to 
$\lambda_\text{min}$ equidistantly on the log scale. Typical values are $\epsilon = \num{0.001}$ and
$n_\lambda = \num{100}$, and we will stick to them throughout chapter \ref{chap:results}. This 
allows us to tune $\lambda$ in a rather natural way.

We minimize Eq.\ \eqref{eq:loss-elastic-net} for $\lambda$ from this sequence in decreasing order. This gives rise 
to two tweaks to tune $\lambda$ efficiently. First, we expect the minimizers of two adjacent $\lambda$ 
values to be quite close and therefore use the minimizer from the previous run of the iterative coordinate 
descent as the starting value for its next run in a so-called \textit{warm start}. Second, we 
\textit{stop early}, i.e., we fit no more models for the remaining $\lambda$ values at the lower end of the 
sequence, if the cross-validated error does not improve for a certain number of consecutive 
$\lambda$ values. In chapter \ref{chap:results}, we will use a typical value of \num{10} for this.

\subsubsection{Random forests}

More precisely, we talk about classification random forests (RFs), hence $Y$ is binary in this 
subsection. To simplify notation, \num{-1} instead of \num{0} encodes the negative class of 
the response, hence $Y$ maps to $\im(Y) \subset \{ \pm 1 \}$. When we say random 
forests in this thesis, we mean classification 
random forests. Random forests, introduced in \cite{breiman01}, are an ensemble of classification 
trees -- in short: trees -- and aggregate the classification of every constituent tree.

\paragraph{Trees}
A tree is a simple function
\begin{align}
    T = \sum_{m=1}^M c_m \chi_{R_m}: \RR^p \to \{ \pm 1 \}
\end{align}
with $c_m \in \{\pm 1\}$ and the $R_m$ being disjoint rectangle sets, i.e. $R_m = \prod_{j=1}^p 
(a_j, b_j]$ for $a_j, b_j \in \RR \cup \{ \pm \infty \}$. By means of such a tree, we want to 
infer the conditional majority class of the response $Y$ from the predictor $X$. In the perfect 
case, we have
\begin{align}\label{eq:rf-major}
    \sgn(E(Y \cond X = x)) = \tilde{T}(x) \quad \text{for all } x \in \im(X)
\end{align}
for a tree $\tilde{T}$. $\sgn$ is the signum function.

How can we use our samples $(\mathbf{x}_i, y_i)$, $i = 1, \ldots, n$, to learn a tree $T$ that does 
good job on Eq.\ \eqref{eq:rf-major}? While we can 
trivially fulfill Eq.\ \eqref{eq:rf-major} exactly for our samples, this would only lead to heavily
overfitted trees. Instead, we confine ourselves with simpler, rougher trees that will make errors on 
training samples, but generalize better. 
In the algorithm further below, we will only need to calculate an error for the samples inside a 
rectangle set $R \subset \RR^p$. Let $n(R) = |\{ 1 \leq i \leq n: \mathbf{x}_i \in R \}|$ be the number of 
samples in $R$ and
\begin{align}
    \hat{p}_R = \frac{|\{ 1 \leq i \leq n: \mathbf{x}_i \in R, y_i = 1 \}|}{n(R)}
\end{align}
be the proportion of samples in $R$ with positive response. The error measure -- in the context of 
trees usually termed impurity measure -- we use in this thesis
is the \textit{Gini impurity} $Q(R) = 2 \hat{p}_R (1 - \hat{p}_R)$. It is low for pure $R$ dominated by 
samples with either positive or negative response and grows quadratically as the response of the 
samples in $R$ gets more and more imbalanced. The impurity measure is a hyperparameter: other 
options include the misclassification error and the cross entropy. To govern the complexity of $T$,
we demand that every rectangle set $R_m$ contain at least $n_\text{min}$ samples. Even this 
constraint still leaves us with a computationally infeasible number of possible trees, and this is 
why we fit $T$ with a greedy algorithm. We recursively split every rectangle set along a 
feature into two sub-rectangles in such a way that the impurity measure gets minimal and stop if 
a rectangle set comprises less than $n_\text{min}$ samples. Formally, for a rectangle set $R$, a 
feature $j \in \{1, \ldots, p\}$ and split point $s \in \RR$, we define 
the pair of sub-rectangles
\begin{align}
    R^{(1)}_{j, s} = \{r \in R: r_j \leq s\} \text{ and } R^{(2)}_{j, s} = \{r \in R: r_j > s\}.
\end{align}
With respect to the impurity measure, there are only finitely many split points of interest since 
we only have finitely many samples in $R$, and hence we need to consider no more than $n(R) \cdot p$ split 
points when 
considering $R$ for partitioning. All of this allows us to compactly describe a procedure to grow a 
tree in Alg.\ \ref{alg:tree}, cf.\ \cite{breiman84}.
\input{algos/tree.tex}

\paragraph{Forests}
Trees are known to be notoriously \textit{noisy}, meaning if we fix $x \in \RR^p$, the variance 
$V_\mathbf{z}(T(x))$ is quite high; the index $\mathbf{z}$ makes explicit that we consider $T(x)$ as 
a random variable that depends on drawing $n$ i.i.d.\ training samples $\mathbf{z}$ (and 
then fitting a tree to them according to Alg.\ \ref{alg:tree}). On the other hand, if grown deep,
trees have pretty low \textit{bias}, meaning for fixed $x \in \RR^p$ the expectation $E_\mathbf{z}(T(x))$ 
is close to $E(Y \cond X = x)$. High variance and low bias makes trees 
ideal candidates for a method called \textit{bagging}, which averages the predictions of many noisy, 
approximately unbiased models with the goal to reduce their variance, cf.\ \cite{breiman96}. Given 
trees $T_b$, $b = 1, 
\ldots, B$, fitted to identically distributed training data $\mathbf{z}_b$, we denote the new model 
by 
\begin{align}
    \overline{T} = \frac{1}{B} \sum_{b=1}^B T_b.
\end{align}

We again fix some $x \in \RR^p$. Since the expected value is linear and the $T_b(x)$ are identically 
distributed, we have $E_\mathbf{z}(\overline{T}(x)) = E_\mathbf{z}(T_1(x))$ and we see that bagging 
leaves the bias unchanged. Consequently, our hope rests on reducing the variance. Say, every 
$T_b(x)$ -- again understood as a random variable dependent on sampling the training data -- has 
variance $\sigma^2$. If the $T_b(x)$ are independent, the variance is additive and we have 
$V_\mathbf{z}(\overline{T}(x)) = \sigma^2/B$, which tends to zero as $B \to \infty$. In practice, 
the $T_b(x)$ are not totally independent as they have training samples in common: we do not 
sample $\mathbf{z}_b$ from the whole population $(X, Y)$, but draw $\mathbf{z}_b$ as a 
\textit{bootstrap} sample of size $n$ from our training 
data $(\mathbf{x}_i, y_i)$, $i = 1, \ldots, n$. Bootstrapping -- uniformly sampling without 
replacement -- is the key concept behind bagging, 
explaining why \textit{bootstrap aggregation} is a synonym for it. If, in this situation, the 
$T_b(x)$ have a pairwise correlation of $\rho > 0$, we have 
\begin{align}
    V_\mathbf{z}\left(\overline{T}(x)\right) = \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2,
\end{align}
which tends to $\rho \sigma^2$ as $B \to \infty$, cf. \cite[Eq. (15.1)]{elem-stat-learn01}.

Thus, in addition to bagging, random forests strive to reduce to correlation between the trees 
without increasing the variance and bias too much. We achieve this by randomly selecting a subset
of $m \leq p$ features every time we split a rectangle into two sub-rectangles when growing the 
tree in Alg.\ \ref{alg:tree}. In more detail, the indices $j$ over which we take the minimum in 
line \ref{alg:tree:split} only range over 
these $m$ features. We denote the modified function with the appended parameter $m$ by 
\textsc{RfTree}. Alg.\ \ref{alg:forest} applies these ideas all together.

\input{algos/forest.tex}

In contrast to GLMs with their continuous output, random forests output a binary classification,
so we cannot control their prevalence. Widely used default values are $m = \floor{\sqrt{p}}$ and 
$n_\text{min} = 1$. As for $B$, we have 
shown above that increasing it reduces the variance at constant bias; in practice, therefore, only 
computation time limits the number of trees in the forest. We can go for a large $B$ as fitting the 
constituent trees is an embarrassingly parallel problem. We can 
also validate random forests very efficiently by facilitating \textit{out-of-bag (OOB)} samples: 
for each sample $(\mathbf{x}_i, y_i)$, we construct its \textit{OOB prediction} by averaging only 
those $T_b(\mathbf{x}_i)$ for which $(\mathbf{x}_i, y_i)$ was not part of the bootstrapped training 
sample -- no need for a time consuming cross validation. For random forests, we use OOB predictions 
as validated predictions.

\subsection{Nested models}\label{subsec:nested-models}

Given some \textit{early} models -- e.g. core models -- $f_i: \RR^p \to \RR$, $i = 1, \ldots, m$, 
we can nest them into another, \textit{late} model $f: \RR^m \to \RR$ and obtain a new model 
$g = f \circ (f_1, \ldots, f_m)$. 

\subsubsection{Early models trained on another data set}
Often, the early models have been trained on another, independent data set, so we observe their 
output as features in our data set. Examples are the cell-of-origin signature, the IPI score or the 
LAMIS. Such $f_i$ are merely projections onto a feature.

\subsubsection{Early models trained on the same data set}
If we need to fit some of the early models to the training data ourselves, getting reliable 
cross-validated predictions
for $f$ becomes trickier. Without loss of generality, we assume the only early model left to be 
fitted 
is $f_1$. We want to fit $f_1$ according to a hyperparameter tuple $h_1$ and $f$ according to $h_2$, 
giving us the hyperparameter tuple $h = (h_1, h_2) \in H$ for the nested model. How do 
we get realistic cross-validated predictions $\hat{y}_i$, $i \in F_\ell$, for the $\ell$-th fold? 
We start by fitting $f_1$ to $(\mathbf{x}_i, y_i)$, $i \in \{ 1, \ldots, n \} \setminus F_\ell = F_\ell^c$ 
subject to $h_1$.
The question reduces to: which values for the output of $f_1$, denoted $o$, do we provide the 
algorithm that fits $f$ to $(o_i, f_2(\mathbf{x}_i), \dots, f_m(\mathbf{x}_i); y_i)$, $i \in 
F_\ell^c$?

The choice $o_i = f_1(\mathbf{x}_i)$ is most likely too optimistic since $f_1$ overfits its training 
data to some extent. Consequently, $f$ will put too much trust in $o$ and will generalize poorly, 
as the cross-validated predictions of $f$ will already show. We want to tackle this problem by 
using cross-validated predictions for $f_1$, instead. The theoretically cleanest approach 
is to do an \textit{inner} cross validation for $f_1$, i.e. another cross-validation for fitting 
$f_1$ to $(\mathbf{x}_i, y_i)$, $i \in F_\ell^c$, and use the cross-validated predictions from \textit{this} 
cross validation for $o$. However, the additional loop increases time complexity by a factor of $k$ 
for a $k$-fold cross-validation. Because we fit $f_1$ to $(\mathbf{x}_i, y_i)$, $i = 1, \ldots, n$, 
in a cross validation anyway, we prefer to use the cross-validated predictions we get in this 
process for $o$. This even allows to formulate the approach for arbitrary validated predictions 
for $f_1$ and $f$. Alg.\ \ref{alg:nested-pcv} writes this down clearly.

\input{algos/nested_pseudo_cv.tex}

The gain in time complexity compared to the approach involving an inner cross validation comes at a 
little price: a not thoroughly clean validation of $g$. To see this, we assume that the validated 
predictions for $f_1$ and $f$ are both cross-validated predictions with the same
assignment of samples into folds -- a very 
realistic assumption as we 
usually choose $k = n$. We fix a sample $i$ and another sample $j$ that is in another fold. 
Sample $i$'s cross-validated prediction $\hat{y}_i$ for $g$ -- or equivalently its cross-validated 
prediction for $f$ -- depends on the cross-validated 
prediction $o_j$ of sample $j$ for $f_1$. $o_j$ in turn depends on sample 
$i$. All in all, $\hat{y}_i$ very slightly depends on $(\mathbf{x}_i, y_i)$.

Next, we want to talk about how we tune the hyperparameters of a nested model. In practice, one thinks of 
the hyperparameter tuples $H$ as partitioned into subsets. We summarize the
hyperparameters of a model that make it most distinct to other models in its \textit{architecture}. 
For a core model, the architecture concerns its class -- Gauss, logistic, Cox model or random forest 
-- and the a-priori selected features. For a nested model, its architecture is defined by the 
architectures of the involved core models and the hierarchy that nests them into one another.

For a given architecture of a nested model, we need to tune the hyperparameter tuple $h = (h_1, h_2)$ 
of 
Alg.\ \ref{alg:nested-pcv}, i.e., we repeatedly call \textsc{NestedPseudoCV} as we vary $h \in H_0$
with $H_0$ being the set of tried out hyperparameter tuples for this model architecture. What 
should $H_0$ look like? We have candidate 
hyperparameter tuples $H_{0, 1}$ for $f_1$ and $H_{0, 2}$ for $f$. An unprejudiced strategy is to 
set $H = H_{0, 1} \times H_{0, 2}$. Compared to non-nested models, we try out a considerably 
higher number of hyperparamter tuples -- roughly a factor of $|H_{0, 1}| \approx |H_{0, 2}|$ -- and 
the validated errors of nested models have a higher chance 
of being overestimated, cf.\ subsection \ref{subsec:train-val}. We therefore settle on a greedier 
strategy: we first tune $h_1 \in H_{0, 1}$, yielding a best validated hyperparameter tuple 
$\hat{h}_1$, and then go on to tune $h_2 \in H_{0, 2}$. This means $H_0 = \{ \hat{h}_1 \} 
\times H_{0, 1}$.

$f_1$ often deals with high-dimensional input. E.g., $f_1$ is a Gauss model 
predicting from 
several thousand gene-expression levels. Hence, we undoubtedly need elastic-net 
regularization in its training and need to tune the regularization strength $\lambda$. 
With this alone, we quickly end up with several hundred hyperparameters to be tuned for $f_1$. 
With $H_{0,1}$ being so big, good validated performances have one thing in common: they and the 
corresponding cross-validated predictions are too optimistic. This means, we still 
have not gotten entirely rid of the problem that $f$ puts too much trust into $f_1$'s output 
misled by too optimistic values in the training. This problem will propagate into another
problem: too optimistic cross-validated predictions for $g$ and an overestimated 
cross-validated performance of $g$. It is hard to quantify the effect, so we will have a very close 
eye on models nested according to Alg.\ \ref{alg:nested-pcv} in chapter \ref{chap:results}. 

\subsubsection{Nested models and inter-technical variability}

Considering a nested model $g = f \circ (f_1, \ldots, f_m)$, what happens if the output of some of 
the early models $f_i$ is sensitive to inter-technical variability? Without loss of generality, let 
only $f_1$ be such an early model. Moreover, let both $f_1$ and $f$ be a GLM, which we describe 
with the linear predictor as $f_1: \RR^p \to \RR, x \mapsto x^T \beta$ and $f: \RR^m \to \RR, 
x \mapsto x^T \gamma$. 
$\mathbf{z} \in \RR^{n \times p}$ holds the same features for the same samples as the 
predictor matrix $\mathbf{x}$, but measured under a different protocol. Let $J = \{1, \ldots, p\}$ 
denote the features that are affected by the change of protocol, e.g. the gene-expression levels.
Let $\beta$ fulfill the zero-sum constraint $\sum_{j \in J} \beta_j = 0$.
We assume that Eq.\ \eqref{eq:inter-tech-exact} holds with small residues $\epsilon_{ij}$. For $j 
\notin J$, we this is trivial since we even have $\epsilon_{ij} = 0$. For $j \in J$, this is also 
a realistic assumption since, in this thesis, $f_1$ is always a gene-expression signature.

With Eq.\ \eqref{eq:inter-tech}, we obtain
\begin{align}\label{eq:inter-tech-nested}
\begin{split}
    g(\mathbf{z}_i) &= (f \circ (f_1, \ldots, f_m))(\mathbf{z}_i) \\ 
    &= (\mathbf{z}_i^T \beta, f_2(\mathbf{z}_i), \ldots, f_m(\mathbf{z}_i))^T \gamma \\
    &\approx (\mathbf{x}_i^T \beta + c, f_2(\mathbf{x}_i), \ldots, f_m(\mathbf{x}_i))^T \gamma \\ 
    &= c \gamma_1 + (f \circ (f_1, \ldots, f_m))(\mathbf{x}_i) = c \gamma_1 + g(\mathbf{x}_i)
\end{split}
\end{align}
for some $c \in \RR$ and all samples $i = 1, \ldots, n$. The sample-wise shift in the output of 
$f_1$ propagates into a sample-wise shift in the output of $g$ -- qualitatively this is the same 
as in Eq.\ \ref{eq:inter-tech}. The residuals $\epsilon_{ij}$, $\beta$ and $\gamma$ impact the 
above approximation. From our 
perspective, where output ordering matters most, we can say: the better this approximation, the more 
monotonic the $g(\mathbf{z}_i)$ are in $g(\mathbf{x}_i)$. We do not elaborate on this theoretically any further; 
in our practice in section \ref{sec:inter-trial}, we will see that the approximation is indeed
very good.

While $f_1$ is always a GLM in this thesis, one might consider a random forest for $f$. Random 
forests, which predict solely by thresholding single features, are highly sensitive to shifts in 
the output of $f_1$. In this situation, we should rather make $f_1$ agnostic to inter-technical 
variability once and 
for all by thresholding its output on the respective batch of samples, but this risks losing the 
nuanced information a continuous output provides. These considerations turn the simplicity of GLMs 
into an asset against more complicated models: for GLMs we can better foresee how they will react 
to inter-technical variability.

\section{Software}

We have designed the software that generates the results of this thesis in such a way that it 
can be the foundation of the machine-learning part of MMML-Predict as the project develops and 
other researchers take over. Beyond this, we wrote code, wherever it was possible, in such 
a general way that one can apply it not just for the MMML-Predict problem, but for all equivalent 
and closely related problems. We ran all code in the statistical-computing environment \texttt{R}
\cite{r-language}.

\subsection{The R package \texttt{patroklos}}
The R package \texttt{patroklos} holds all of the highly reusable code of this thesis. It brings 
all the methods described in this chapter to life and is available on GitHub \cite{patroklos-gh} 
together with a 
\href{https://lgessl.github.io/patroklos/}{website}\footnote{\url{https://lgessl.github.io/patroklos}}. 
In fact, one can deploy 
\texttt{patroklos} to solve any binary classification problem in a supervised-learning manner.
\texttt{patroklos} provides some extra functionality for those problems among them where the 
response is a thresholded survival time, high-dimensional data is involved and one wants to maximize 
the precision of the classifier for a sufficiently large positive cohort.

\paragraph{Training} 
\texttt{patroklos} allows the user to quickly specify model architectures and 
attach hyperparameters tuples to them, including model-agnostic hyperparameters to all of them. It 
provides a decorator to transform a function that fits merely one model for a single hyperparameter 
tuple into a function that fits and validates models for a variety of hyperparameter tuples. 
It supports fitting core models with the support of the R packages \texttt{zeroSum} 
\cite{zerosumR} and \texttt{ranger} \cite{ranger17} out of the box. \texttt{zeroSum} 
augments the popular \texttt{glmnet} package by the zero-sum constraint: it fits and 
tunes Gauss, logistic and Cox models in a cross validation and endows their loss function 
with elastic-net regularization and -- unlike \texttt{glmnet} -- the zero-sum 
constraint. The time-critical work of \texttt{zeroSum} happens in a fast, well-structured C++ 
backbone. \texttt{ranger} fits random forests fast, also thanks to a performant C++ backbone. 

\paragraph{Validation and testing} 
If fitting functions from existing R packages calculate a validated error of 
their fitted models at all, they do so in a plethora of ways, usually using an error measure derived 
from the loss function they minimize. E.g., \texttt{zeroSum} uses the binomial deviance for 
logistic models and \texttt{ranger} uses the classification error for classification random 
forests. \texttt{patroklos} takes 
full control of validation and unifies it
by calculating the same error measure across all models and even more statistics of the model
with the help of the \texttt{AssScalar} R6 class. With the \texttt{Ass2d} R6 class, the 
user can plot one property of a truly binary classifier against another, which guides 
thresholding models with continuous output as in Fig.\ \ref{fig:inter-output-prec}. 
\texttt{patroklos} calculates logrank p-values and hazard ratios together with their confidence
intervals by means of the R package \texttt{survival} \cite{survival-cran}.

\paragraph{Modular, extendible design} 
\texttt{patroklos} defines function interfaces so the user 
can easily wrap fitting functions implemented in other R packages or by the user to make 
them a part of the \texttt{patroklos} pipeline. The two R6 classes \texttt{Model} and \texttt{Data}
strive to abstract models from the data in such a way that the user can easily apply $H$ to new 
data.

\paragraph{Meta analysis} 
\texttt{patroklos} provides tools to analyze how the validated error depends on the test error and 
allows to highlight subgroups of model architectures to unveil systematic flaws in validation 
and test performance.

\subsection{\texttt{patroklos} in action}

The Git repository that brings \texttt{patroklos} into action for this thesis is available in a 
public version on GitHub \cite{thesis-gh}. We try our best at presenting the results in a 
comprehensive and understandable way in the following chapter \ref{chap:results}. Yet, since code 
is more precise than freely written text, the Git repository rigorously defines everything we have 
done to obtain the results.

The public Git repository does not contain the data because part of it must not be publicly 
available. To reproduce the results of this thesis, the reader may request access to the scientific 
computing servers of the Chair of Statistical Bioinformatics, University of Regensburg, from 
\href{mailto:christian.kohler@ur.de}{Christian Kohler}\footnote{Email: 
\href{mailto:christian.kohler@ur.de}{\texttt{christian.kohler@ur.de}}}; on these servers, a mounted
volume holds all data sets.