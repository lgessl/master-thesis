\chapter{Methods} \label{chap:methods}

Finding the best possible model for our given task will not be possible from just theoretical 
considerations; we will have to fit several models to our data and demonstrate the performance of 
the chosen one convincingly. Section \ref{sec:train-val-test} will lay the state-of-the-art 
train-validate-test paradigm for this. In section \ref{sec:candidate-models}, we will introduce 
the candidate models and the hyperparameters governing their fitting process. We will start with 
model-agnostic hyperparameters before we go on to present well-known model types and finally 
introduce a method that lets us train compositions of multiple models.

\section{Training, validation and testing}\label{sec:train-val-test}

The design of MMML-Predict with its train and test cohort, where the people developing the predictor 
never get to see the test cohort, pays tribute to the standard two-step approach in supervised-
learning tasks: We use the train cohort to fit multiple models (training) and choose the model
among these models that we have confidence performs best on new data (validation). We then touch 
the test cohort for the first time as we evaluate the chosen model on it to persuade the outside 
world we have come up with model worth deploying.

Since we want to have the same conditions for data used in this thesis as later in MMML-predict,
we randomly split given data $(\mathbf{x}, y)$ with predictor $\mathbf{x}$ and response $y$ into 
a train cohort $(\mathbf{x}_\text{train}), y_\text{train}$ and a test cohort $(\mathbf{x}_\text{test}, y_\text{test})$.
The following assumes a single, split data set.

\subsection{Training and validation}\label{subsec:train-val}

To be able to discuss some probabilistic caveats of validation later, we introduce some formal 
notation. We start with a set of tuples of hyperparameters $H$, where every $h \in H$ defines a model
up to its parameters; determining the parameters of a model, by definition, is the job of the 
algorithm optimizing a given loss function, we refer to this as fitting. There is a one-to-one 
mapping between $H$ and the set of candidate models.

For every hyperparameter tuple $h \in H$, we
\begin{enumerate}
    \item fit the model to the train cohort subject to $h$ in $k$-fold cross-validation. This means 
        we randomly assign the training samples into $k$ subsets, called folds, and then actually 
        fit $k+1$ models, one model on all samples and for every $i = 1, \ldots, k$, we train a model
        on all samples except the $i$-th fold and obtain its cross-validated predictions on the 
        $i$-th fold; these are predictions on new data. Doing this for all folds, we obtain a 
        cross-validated prediction for every sample and hence a vector of cross-validated predictions
        $\hat{y}_\text{train} = \text{cv}(h)$ of the same shape as $y_\text{train}$. There may be 
        other methods than ordinary cross
        validation in this step, but their purpose is always the same: yield a prediction for every 
        sample that looks like it was made on new data.
    \item We use the cross-validated predictions to calculate the validation error 
        $\text{err}(y_\text{train}, \hat{y}_\text{train})$.
\end{enumerate}

Finally, we select the model $m^*$ defined by the hyperparameter tuple 
\begin{align}
    h^* = \argmin_{h \in H} \ \text{err}(y_\text{train}, \text{cv}(h)).
\end{align}

We note that for all $h$, the validation error is a random variable, which e.g. depends on the 
random assignment of the fold, the train cohort and possibly random involved in the loss-function
optimizer. It is a well known property of independent,
identically distributed (i.i.d.) real random variables $X_i, i \in \NN$, that their extreme values 
are notoriously unstable in the sense that for all $t \in \RR$ with $P(X_1 \geq t) < 1$, 
\begin{align}
    \Pr\left( \min_{1 \leq i \leq n} X_i \geq t \right) = \Pr(X_1 \geq t)^n \to 0 \quad
    \text{as } n \to \infty.
\end{align}

While not all $\text{err}(y_\text{train}, \text{cv}(h)), h \in H$, are i.i.d. -- after all, they 
all depend on 
$(\mathbf{x}_\text{train}, y_\text{train})$ and some model are better suited for the problem than others --,
we can have situations where at least a subset of $H$ delivers validation errors close to being i.i.d.

\paragraph{Models merely guessing}
Imagine models not suited to deal with the problem at all. Their cross-validated predictions are 
merely guesses on $y_\text{train}$ and therefore i.i.d. There is a non-zero probability to guess
all true outcomes on the train cohort correctly and hence it is only a matter of the number of 
such merely guessing models until which get the minimal possible error for one of them. The number of 
such models can indeed grow large e.g. if we keep trying ever new hyperparameters for a model class 
that can per se not describe the dependence between $\mathbf{x}$ and $y$ at all.
As a practical advice, we should not spend too much time und put too much hope into improving 
bad models by hyperparamter tuning.

\paragraph{General scenario}
In general, the validation errors of the candidate models are independent to some extent because we fit them
subject to differing hyperparameters. As a result, the validation error of every model can 
fluctuate freely around its expected value to some extent. The larger $H$ gets, the 
more severely underestimated validation errors we will have in the ranking. This 
increases the odds that $m^*$ is just a model with a tremendously underestimated validation error, 
leading to a bad surprise on the test cohort. As a practical advice, we should be careful with 
trying too many model families and vastly differing hyperparameters for a given base model 
because both may give us quite independent models.

\paragraph{}
This underscores the need for a smart and lean choice of the candidate models or hyperparameter 
tuples, respectively. Exclusion criteria for candidate models may be theoretical considerations
and experience by both us and others. Concerning experience, this means we will not move away too 
far from default hyperparameters and we will develop heuristics in chapter \ref{chap:results} as 
move from data set to the next.

\subsection{Testing}

We calculate the predictions $m^*(X_\text{test}) = \hat{y}_\text{test}$ of the best validated model 
$m^*$ on the test cohort and estimate its performance on independent data via 

\begin{align}
    \text{err}(y_\text{test}, \hat{y}_\text{test}).
\end{align}

\subsection{Choice of the error function}\label{sec:error-function}

Taking into account the calculations in section \ref{sec:intro-mmml},
we want to optimize the precision of the high-risk group, which must comprise at least 15\% of the 
samples in our data. Since an error should be the lower, the better the model is, we choose 
$\text{err}$ to be the minimum of the negative precisions with a prevalence of at least 17\%. 

Usually, we need 
to take a minimum over \textit{several} precisions because most models do not output the final 
classification.
Instead they return a continuous score where a higher score means a higher probability of being
in the positive class, in our case being high-risk, and this continuous score needs thresholding
via $\hat{y}_i > t$ for some $t \in \RR$.
We notice that as we increase $t$ and thereby decrease the prevalence, the obtained 
adjacent precision values more and more depend on one another; for high $t$ and low 
prevalences, fluctuations of up to 50 percentage points would be possible, but requiring a 
prevalence of at least 17\% caps such fluctuations at 
\begin{align}
    \frac{1}{\ceil*{\num{0.17} \cdot n_\text{test}}}
\end{align}
for $n_\text{test}$ samples in the test cohort. This gives us an error function that is tailored 
for our problem, sufficiently far-sighted and robust.

As indicated by the notation $\text{err}(y_\text{train}, \hat{y}_\text{train})$ and 
$\text{err}(y_\text{test}, \hat{y}_\text{test})$, we optimize $t$ on the true outcomes 
of the train cohort when validating and we optimize it again on the true outcomes of the test cohort 
when testing. Strictly speaking, $t$ is a hyperparameter, which we optimize on the 
test cohort during testing; this sounds delicate. The results however will show that the optimal 
choice for $t$ (or at least an 
almost optimal choice) on both train and test cohort correponds to a prevalence of 
slightly above 17\% such that one can agree on the following when it comes to the MMML-Predict data: 
choose $t$ as the 17\%-quantile of the coninuous model output on the test cohort.

\subsection{Inter-technical variability} 
One could instead suggest to already optimize the output threshold on the cross-validated 
predictions. While this promises a stricter train-test regime at first glance, fixing the output 
threshold once and for all neglects inter-technical variability. An always-present problem in 
Bioinformatics, inter-technical variability refers to the fact that one and the same sample 
measured on different platforms or in different labs, in short: with different protocols, may 
lead to different values for the same feature. If we measure the same $p$ features of the $n$ 
patients in $\mathbf{x} \in \RR^{n \times p}$ again with another protocol, we end up with another predictor 
$\mathbf{z} \in \RR^{n \times p}$.
For gene-expression levels,
we can often well model the discrepancies with two indepedent biases, sample-wise effects $\theta_i$
and feature-wise effects $\omega_j$, leading to
\begin{align}
    \Delta_{ij} = \mathbf{z}_{ij} - \mathbf{x}_{ij} = \theta_i + \omega_j + \epsilon_{ij}
\end{align}
for residues $r_{ij}$. Assuming accurate modeling, i.e. small residues, we can well approximate
\begin{align}
    \mathbf{z}_{ij} \approx \tilde{\mathbf{z}}_{ij} = \mathbf{x}_{ij} + \theta_i + \omega_j.
\end{align}
We can now apply an ordinary linear -- or Gaussian -- model to $\mathbf{z}$ obtaining
\begin{align} \label{eq:inter-tech}
\begin{split}
    \beta_0 + \sum_{j=1}^p \beta_j \mathbf{z}_{ij} &\approx \beta_0 + \sum_{j=1}^p \beta_j \tilde{\mathbf{z}}_{ij} \\
    &= \beta_0 + \sum_{j=1}^p \beta_j \mathbf{x}_{ij} + \sum_{j=1}^p \beta_j \theta_i + \sum_{j=1}^p \beta_j \omega_j.
\end{split}
\end{align}
The second summand is zero under the zero-sum constraint $\sum_{j = 1}^p \beta_j = 0$ and the third 
summand is constant across all samples and can be absorbed by the intercept \cite{transplatform17}. 

Under these (not very 
strict) conditions, going from one protocol to another just leads to a constant shift of the model 
output; even for generalized linear models, i.e. a Gauss model composed with some monotic link function,
inter-technical variability does not change the ordering of the samples. All one needs to do to 
obtain a final classification is calibrating the threshold for the output scores. However, this 
also demonstrates that while generalized linear models can cope with inter-technical variability 
pretty well, there is no point in using the same output threshold across all protocols.

\section{Candidate models}\label{sec:candidate-models}

\subsection{Model-agnostic hyperparameters}\label{subsec:model-agnostic}

Model-agnostic hyperparameters are those hyperparameters we can set and tune for every model. They 
concern the predictor matrix $\mathbf{x} \in \RR^{n \times p}$ and the response vector $y \in 
(\RR \times \{ 0, 1 \})^n \cup \{ 0, 1 \}^n$. We can provide the response in two formats. In the 
first case, the response has two entries per sample: the first one is the time to event, in our 
case progression-free survival, and the second one indicates if the event has occured (1) or 
if the patient was censored before it could occur (0); more on this in subsection 
\ref{subsec:core-models} when we present the Cox model. In the second case, the response is a binary 
vector with 1 indicating the positive class (high-risk DLBCL)
and 0 indicating the negative class (low-risk DLBCL). Note that in the latter case we need 
to discard samples censored before two years, but this is typically only a small proportion of the 
samples in the data set.

\subsubsection{A-priori feature selection}

Of course, not every feature in the data set should be part of the predictor matrix $\mathbf{x}$. Including 
features measured to toward the end of a patinent's therapy or even after it, is cheating and we 
should definitely exclude them. Excluding any of the remaining features is hard to justify: If we 
know for a feature that it alone is associated with the response, we of course include it in $\mathbf{x}$.
On the other hand, if a feature alone shows no link to the response, the right model might still be 
able to leverage it in combination with other features; but at this step, we do not know if such 
a model exists and if it is among our candiate models. The only remaining option for handpicking 
features a priori is to brute-force the problem and try out all $2^p$ combinations of features.
Even if we trust on regularization to do its job for the high-dimensional part (typically several 
hundreds or even thousands of gene expression levels), just \num{10} remaining fetures would still
leave us with $2^{10} = 1024$ combinations to try out -- per model class. While this may be 
computationally feasible, validation would be a statistical fiasco as laid out in subsection 
\ref{subsec:train-val}. Therefore, we decided to reduce this quickly exploding problem to only 
a couple of decisions to 
make and have all the other feature selection happen during model fitting. 

\paragraph{Gene-expression data}
The first and most important decision to make is: do we want to include the high-dimensional
gene-expression part of the data at all? If we do not, we suddenly have $p \ll n$ instead of 
$p \gg n$ and can use way more complex models. 

\paragraph{Features in different formats}
Second, we need to make a couple of more decisions regarding the format of some features.
\begin{itemize}
    \item Concerning the IPI, one can include the five IPI features in their original, continuous 
        form; discretized as by the IPI inventors; the IPI score as a single continuous feature; 
        or the so-called IPI group, a partioning of the IPI score into three groups. Similar widely 
        accepted thresholds may exist for other clinical and genetic features.
    \item For already existing models, we can include their continuous output or also threshold 
        it. E.g., cell-of-origin signatures can be -- and most of the time in data sets already 
        have been -- thresholded into ABC, GCB and unclassified subtypes. The above-mentioned IPI 
        group thresholds the IPI model. 
\end{itemize}

Imagine, a data set provides the five IPI features in continuous format and the continuous output 
of some gene-expression signature for which 
its inventors also provide a preferred threshold. If we want to decide for exactly one format 
per feature, this leaves us with $5 \cdot 2$ possibilities for this simple case. If we want try out 
every combination of non-empty subsets, the number of possibilities jumps to $2^5 \cdot 2^2 = 
128$. 

Our solution here again is to be generous and include all formats of a feature in $\mathbf{x}$ a 
certain model may benefit from. This sentence usually 
simplifies to do: include all widely used formats of a feature in $\mathbf{x}$. E.g., generalized linear models
cannot threshold continuous features, so a feature in its thresholded format may give a rough 
contribution to the prediction while the same feature in its continuous form may further refine it.
Moreover, even decision trees can benefit from a feature additionally provided in its thresholded 
format if this threshold has been inferred on a much bigger data set since the decision tree 
may not be able to find the threshold itself.

\subsubsection{Imputation}

If values in $\mathbf{x}$ are missing, i.e. written as \texttt{NA}, actions we can take fall into two 
categories: we discard the part of data affected by missing values or we replace the missing values 
with some realistic estimate.

\paragraph{Discarding part of the data}

In a first step, we discard a feature if it is not available for a large proportion of the samples.
This enables reasonable computing.

\paragraph{Imputing}

We then mean-impute the remaining missing values. At this point, we model a categorical feature 
with $c$ categories as $c-1$ binary dummy features; if for a sample this categorical feature is not 
available, all $c-1$ dummy features will be \texttt{NA}. For every column in $\mathbf{x}$, we calculate the 
mean of the available features in $\mathbf{x}$ and replace the \texttt{NA} values with it. 
For a missing categorical feature, every dummy feature therefore holds the marginal probability 
that the feature is in the $k$-th category. This is efficient, transparent and easily applicable to 
new data.

\subsubsection{Adding combinations of discrete features to the predictor}

We add all combinations of at most $n_\text{combi}$ categorical features that are positive in a 
share of at least $s_\text{min}$ patients to $\mathbf{x}$; e.g., we add a column ``female \& ABC-type 
tumor'' if at least \num{5}\% of patients have this property. We always choose $s_\text{min}
= 5\%$. We set $n_\text{combi} = 1$ for models that facilitate interactions between features 
themselves, otherwise we set $n_\text{combi} = 3$.

\subsubsection{Tuning the definion of ``high-risk''}

Defining a patient as high-risk if and only if the progression-free survival is less than two years
is clinically accepted, yet quite arbitrary decision. It may be not be the time cutoff that 
separates patients' survival 
best from a biological point of view. Therefore we can provide the fitting 
algorithm a modified response $y$, governed by the time cutoff $T > 0$.
\begin{itemize}
    \item For binary response $y \in \{0, 1\}^n$, we set $y_i = 1$ if the progression-free survival 
        of patient $i$ is less than $T$ and $y_i = 0$ otherwise.
    \item For response $y \in (\RR \times \{0, 1\})^n$ with time to event and censoring, we censor 
        all samples with time to event exceeding $T$ at $T$.
\end{itemize}

\subsection{Core models}\label{subsec:core-models}

All models trained, validated and tested in this thesis at the core consist of ordinary linear, 
logistic and Cox proportional-hazards models with additional properties, as well as random forests.
In this chapter, we want to introduce the design of these models and the hyperparameters governing 
their fitting process.

Formally, we deal with a probability space $(\Omega, \mathcal{A}, P)$ that we do not and cannot 
specify any further because we only get in touch with two random variables 
\begin{align}
    X = (X_1, \ldots, X_p): \Omega \to \RR^p \text{ and } Y: \Omega \to \RR,
\end{align}
the predictor and the response, respectively. More 
precisely, we observe independent training samples $(x_i, y_i) \in \RR^{p+1}, i = 1, \ldots, n$, 
distributed according to $(X, Y)$; $x_i$ is the $i$-th row of the predictor matrix $\mathbf{x}$ and
$y_i$ is the $i$-th entry of the response vector $y$. The i.i.d. test samples follow the same 
distribution as the training samples, but in this subsection everything is about the training and 
hence the training samples; if we say samples in this subsection, we always refer to the training   
samples.

\subsubsection{Generalized linear models}\label{subsubsec:glm}

Here, we work with binary response, i.e. $im(Y) \subset \{0, 1\}$. Both ordinary 
linar models and logistic regression models are generalized linear models (GLMs). In 
a GLM, $Y$ follows an exponential-family distribution and there is an invertible link function 
$g: \RR \to \RR$ and parameters $(\beta_0, \beta) \in \RR^{p+1}$ such that
\begin{align}
    g(E(Y \cond X = x)) = \beta_0 + \sum_{j=1}^p \beta_j x_j \quad \text{for all } x \in \RR^p,
\end{align}
where the $E(Y \cond X = x)$ is the expected value of $Y$ given $X = x$. For the above samples 
$(x_i, y_i)$, we obtain
\begin{align}
    \mu_i = E(Y \cond X = x_i) = g^{-1}\left(\beta_0 + \sum_{j=1}^p \beta_j x_{ij}\right).
\end{align}
Relating $\mu_i$ to $y_i$ via a log-likelihood, will allow us to fit $(\beta_0, \beta)$ to 
$(x_i, y_i)_{i = 1, \ldots, n}$. We can express $\sum_{j=1}^p \beta_j x_{ij}$ in terms of the 
Euclidean scalar product as $x_i^T \beta$, where $x_i^T$ denotes the transpose of the column vector 
$x_i$.

\paragraph{Ordinary linear model}
Here, $g = \text{id}$. $Y$ and $Y \cond X = x_i$ for all $i = 1, \ldots, n$ follow a normal
distribution with fixed standard deviation $\sigma > 0$ (a property called homoscadasticity), but 
varying mean. The log-likelihood of $\mu_i$ is
\begin{align}
\begin{split}
    \ell(\mu_i; y_i) &= \log\left( \frac{1}{\sqrt{2\pi}\sigma} 
        \exp \left( -\frac{1}{2\sigma^2}(y_i - \mu_i)^2 \right) \right) \\
    &= -\frac{1}{2\sigma^2}(y_i - \mu_i)^2 - \log \left( \sqrt{2\pi}\sigma \right).
\end{split}
\end{align}
Up to a constant shift and re-scaling, which does not affect maximizing the log-likelihood, this
is the well-known squared error.

\paragraph{Logistic model}
Here, the linear predictor is the log-odds for $Y = 1$ over $Y = 0$ given $X = x$, i.e.
\begin{align}
    g: \mu \mapsto \log\left( \frac{\mu}{1 - \mu} \right), \text{ hence } g^{-1}: \eta \mapsto
    \frac{1}{1 + \exp(-\eta)}.
\end{align}
Y and $Y \cond X = x_i$ for all $i = 1, \ldots, n$ follow a Bernoulli distribution with parameter 
$p$ and $\mu_i$, respectively. The log-likelihood of $\mu_i$ is
\begin{align}
    \ell(\mu_i; y_i) &= \log\left( \mu_i^{y_i} (1 - \mu_i)^{1 - y_i} \right) 
    = y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i). 
\end{align}

\paragraph{}
Using $\mu_i = g^{-1}(\beta_0 + x_i^T \beta)$, in both cases we can express the 
log-likelihood 
for $(\beta_0, \beta)$. For our i.i.d. samples $(x_i, y_i)_{i = 1, \ldots, n}$, we therefore obtain 
\begin{align}
    L(\beta_0, \beta) &= \sum_{i=1}^n \ell(\mu_i; y_i) 
    = \sum_{i=1}^n \ell\left( g^{-1}\left( \beta_0 + x_i^T \beta \right); y_i \right).
\end{align}
We can augment $L$ with two hyperparameters, sample weights $w_i > 0$, $i = 1, \ldots, n$, and the 
zero-sum constraint with respect to zero-sum weights $u_j \geq 0$, $j = 1, \ldots, p$, yielding 
\begin{align} \label{eq:loss-glm-no-lasso}
    L(\beta_0, \beta) &= \sum_{i=1}^n w_i \ell\left( g^{-1}\left( \beta_0 + x_i^T \beta \right); 
    y_i \right) \quad \text{subject to } \sum_{j=1}^p \beta_j u_j = 0.
\end{align}
While we do not deviate from default value $1/n$ for the sample weights in the results chapter 
\ref{chap:results}, we will discuss in chapter \ref{chap:discussion} how we can use them in the 
future in a natural way without getting lost in the vast amount of freedom one has to choose them.

What is the zero-sum constraint good for? Imagine, part of the features suffer from sample-wise 
shifts. We denote these features by $J \subset \{1, \ldots, p\}$ and the shifts by $s_i \in R$, 
$i = 1, \ldots, n$. We now set the zero-sum weights  
\begin{align}
    u_j =
    \begin{cases}
        c & \text{if } j \in J, \\
        0 & \text{else},
    \end{cases}
\end{align}
for some $c > 0$ (usually $c = 1$); we can express this conviently with the help of the indicator 
function $\chi_J$ as $u_j = \chi_J(j) c$. By the zero-sum constraint, $\sum_{j \in J} c \beta_j = 0$  
and hence $\sum_{j \in J} \beta_j = 0$. In this situation, the linear predictor 
\begin{align}
    \beta_0 + \sum_{j=1}^p \beta_j (x_{ij} + s_i \chi_J(j))
    = \beta_0 + x_i^T \beta + \sum_{j \in J} \beta_j s_i = \beta_0 + x_i^T \beta,
\end{align}
as $\sum_{j \in J} \beta_j s_i = s_i \sum_{j \in J} \beta_j = 0$, is invariant. If $J$ consists of 
exactly the features holding gene-expression levels, we can transfer our model to other protocols 
and only need to re-calibrate the intercept, as shown in eq. (\ref{eq:inter-tech}). Moreover, we 
can switch off the zero-sum constraint for all features by setting $u_j = 0$ for all 
$j = 1, \ldots, p$. The zero-sum constraint is cheap in terms of model complexity: it only removes 
one degree of freedom; but it is expensive in terms of computational complexity: enforcing it in
every step of the coordinate descent leads to a considerably higher computation time. However, the 
coordinate descent minimizes eq. (\ref{eq:loss-glm-no-lasso}) with a regularization term add. Before 
we discuss this, we want to introduce a model that is closely related to GLMs.

\subsubsection{Cox proportional-hazards model}\label{subsubsec:cox}

For the Cox proportional-hazards model -- or in short: Cox model -- the response variable $Y$ 
measures the time until the event of interest occurs (``event'') or the time after which at some 
point the 
event occurs (``censoring''). Another binary random variable $\delta: \Omega \to \RR$ encodes 
which of these two options is the case: $\delta(\omega) = 1$ for the event, $\delta(\omega) = 0$ 
for censoring. This pays tribute to an important characteristic of survival trials: some
participants drop out of the trial before the event could happen -- e.g. the trial terminated, the 
patient decided to withdraw or died from another cause -- or the event luckily never happens.

\paragraph{Hazard function} To understand what the Cox model predicts, we need another random 
variable $T: \Omega \to \RR$, the time to the event (this time not affected by censoring), and 
require $T$ to have a density $f_T: \RR \to \RR$. We define the hazard function $h: \RR \to \RR$
via
\begin{align}
    h(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \cond T \geq t)}{\Delta t}.
\end{align}
We can interprete $h(t)$ as the instantaneous rate at which the event is occurring at time $t$, 
given that the event has not occured before time $t$. Let $F_T$ be $T$'s distribution function. Then 
we can write
\begin{align}
    \frac{P(t \leq T < t + \Delta t \cond T \geq t)}{\Delta t} = 
    \frac{F_T(t + \Delta t) - F_T(t)}{\Delta t \cdot (1 - F_T(t))}.
\end{align}
Hence, 
\begin{align}
    h(t) = \lim_{\Delta t \to 0} \frac{F_T(t + \Delta t) - F_T(t)}{\Delta t \cdot (1 - F_T(t))} 
    = \frac{f_T(t)}{1 - F_T(t)} = \frac{f_T(t)}{S_T(t)},
\end{align}
where $S(t) = 1 - F_T(t)$ is the survival function, and $h(t)$ is well-defined as long as $S_T(t)
> 0$.

\paragraph{Conditional hazard} The Cox model wants to get a hand on \textit{conditional} hazards.
Intuitively, it is clear that the population with one value of $X$ can have a vastly different 
hazard function than the population with another value of $X$ if $X$ has predictive power for $T$.
Formally, however, it is hard to define a conditional hazard function: $X$ may very well 
have a continuous distribution, hence we condition on an event of probability $0$ when we write 
$P(t \leq T < t + \Delta t \cond T \geq t, X = x)$ -- it is hard to define this in a natural
and straightforward way. In practice, the measurements in $X$ often fluctuate around their true 
values anyway, so it is sensible to condition on a small neighborhood of $x$, say an $\epsilon$-ball 
around $x$ with respect to the Euclidean norm $|\cdot|_2$ for some small $\epsilon > 0$. This 
amounts to
\begin{align}
    h(t \cond X = x) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \cond T \geq t, 
    |X - x|_2 < \epsilon)}{\Delta t}
\end{align}
or -- equivalently -- $h(t \cond X = x)$ is the hazard of $T$ restricted to $(\Omega, \mathcal{A}, 
P)$ conditioned on $\Omega_x = \{ \omega \in \Omega: |X(\omega) - x| < \epsilon \}$; unlike before, 
we usually now have $P(\Omega_x) > 0$.

\paragraph{Proportional hazards} The Cox model assumes \textit{proportional} conditional hazards, 
i.e. 
\begin{align}
    h(t \cond X = x) = \lambda_x h_0(t) 
\end{align}
for some (unspecified) baseline hazard $h_0$ and $\lambda_x > 0$. Moreover, it assumes $\lambda_x$ depends 
on $x$ via 
\begin{align}
    \lambda_x = \exp(x^T \beta),
\end{align}
for parameters $\beta \in \RR^p$.

\paragraph{Partial log-likelihood}
Given our independent samples $(x_i, y_i, \delta_i) \in \RR^{p+2}$, $i = 1, \ldots, n$, distributed 
according to $(X, Y, \delta)$, let $E \subset \{ 1, \ldots, n \}$ refer to the non-censored samples 
($\delta_i = 1$) and for $i \in E$, let $R_i$ denote the set of samples where no event has occurred 
until $y_i$, i.e. $y_j \geq y_i$ for all $j \in R_i$. For fixed $i \in E$ and time to event $y_i$,
conditionally on the risk set $R_i$, the probability that the event occurs on sample $i$ as 
observed is
\begin{align}
    \frac{h(y_i \cond X = x_i)}{\sum_{j \in R_i} h(y_i \cond X = x_j)} = 
    \frac{\exp(x_i^T \beta) h_0(y_i)}{\sum_{j \in R_i} \exp(x_j^T \beta) h_0(y_i)} = 
    \frac{\exp(x_i^T \beta)}{\sum_{j \in R_i} \exp(x_j^T \beta)}.
\end{align}
Taking all events into account yields the partial likelihood 
\begin{align}
    \tilde{\ell}(\beta) = \prod_{i \in E} \frac{\exp(x_i^T \beta)}{\sum_{j \in R_i} \exp(x_j^T 
    \beta)}.
\end{align}
Because $T$ is continuous, there is a zero probability for two distinct samples having the same 
event time, so need not deploy one of the more sophisticated likelihoods capable of handling ties.
Analogously to eq. (\ref{eq:loss-glm-no-lasso}), we can equip the log-likelihood with sample 
weights $w_i > 0$ and zero-sum weights $u_j \geq 0$ such that the loss function reads 
\begin{align}
\begin{split}
    L(\beta) &= \sum_{i \in E} w_i x_i^T \beta - \log \left( \sum_{j \in R_i} w_j \exp(x_j^T
    \beta) \right) \\
    & \text{subject to } \sum_{j=1}^p \beta_j u_j = 0.
\end{split}
\end{align}
As with GLMs, we can leverage the zero-sum constraint to reach shift-invariance.

\subsection{Elastic-net regularization}

Modern molecular measurements are a big factor in our hope that MMML-Predict can succeed and improve 
the IPI. These measurements, like gene expression levels, show up as hundreds if not thousands of 
features in our data, meaning we usually have $p > n$ if not $p \gg n$. In this situation, we 
cannot uniquely determine the parameters of any of the models by maximzing the log-likelihood: 
the models have way more degress of freedom than we have samples, find biologically meaningless 
and non-reproducible structures in the training cohort and generalize to poorly to new data.

The solution to this is to restrict the freedom of the model to the extent that it is forced to 
learn the gist from the data. For that reason, we want to penalize model complexity. Elastic-net 
regularization, a widely adopted method proposed by Zou and Hastie \cite{elasticnet05}, does this 
job for us. The elastic net generalizes two well-established regularization methods,
\begin{itemize}
    \item ridge regularization \cite{ridge70}, which forces the parameter vector $\beta$ to 
        stay inside a ball around the origin by limiting the squared $\ell_2$ norm 
        of the parameters, and 
    \item LASSO (least absolute shrinkage and selection operator) regularization \cite{lasso18}, 
        which forces $\beta$ to stay inside a diamont centered at the origin by limiting the 
        $\ell_1$ norm of the parameters,
\end{itemize}
by combining them with weight factor $\alpha \in [0, 1]$ into 
\begin{align}
    p_{\text{enet}, \alpha}(\beta) = \frac{1-\alpha}{2} |\beta|_2^2 + \alpha |\beta|_1.
\end{align}
Muliplited with the regularization hyperparameter $\lambda \geq 0$ we can add it to the negative 
log-likelihood to obtain the regularized loss function
\begin{align}
    \mathcal{L}(\beta_0, \beta) = -L(\beta_0, \beta) + \lambda p_{\text{enet}, \alpha}(\beta).
\end{align}
Note that in the Cox case, $\mathcal{L}$ does not depend on $\beta_0$ and we just ignore $\beta_0$ 
in the above equation. 

\paragraph{Effect on correlated features}
While both ridge and LASSO regularization shrink the coefficients, they act differently in the case 
of correlated features. Ridge regression tends to force the coefficients of correlated features to 
similar values while LASSO tends to pick one of them and set the others to zero. E.g, in the 
extreme case of $k$ identical features, the LASSO arbitrarily picks one of them with, say, 
coefficient $a$; the ridge regression, meanwhile, assigns every of the $k$ features the coefficient 
$a/k$. Relying a very few features comes with the advantage of sparse models, for which we 
can cheaply generate new data, but it is very sensitive to measurement errors in one of the picked 
features; in the latter case, ridge regularization would average out errors with the help of other 
correlated features. Elastic-net regularization balances the sparsity of the LASSO with the 
robustness of the ridge regularization, especially for values of $\alpha$ close to $1$ 
\cite{elasticnet05}.

\paragraph{Feature-wise penalty weights}
We refine the elastic-net regularization one last time and introduce \textit{a priori} defined 
feature weights $v_i \geq 0$ yielding
\begin{align}
    p_{\text{enet}, \alpha}(\beta) = \sum_{j=1}^p v_j \left( \frac{1-\alpha}{2} \beta_j^2 +
    \alpha |\beta_j| \right).
\end{align}
The default value is $w_j = 1$ for all $i = 1, \ldots, p$. Deviating from the default can be useful 
in at least two scenarios. First, sometimes we are so convinced of the predicitve 
power of a feature in our model that we set $v_j = 0$ for it to all but ensure a non-zero 
coefficient for it; the IPI (in some format) could be such a feature. 

\paragraph{Standardizing $X$} 
The second application of the penalty weights is an aspect of a bigger topic: standardizing the
predictor. We can use the weights to standardize $X$ by setting $v_j$ to an estimate of the standard 
deviation of feature $j$. Why 
should we do this? Let us consider two features $X_1$ and $X_2$ with standard deviation $\sigma_1$
and $\sigma_2$, respectively. To change the output of the model by $1$ when the feature changes by 
one standard deviation, $X_1$ needs the coefficient $\beta_1 = 1/\sigma_1$ and $X_2$ needs 
$\beta_2 = 1/\sigma_2$. For $\alpha = 1$, $p_{\text{enet}, \alpha}$ penalizes both coefficients 
equally and it still does so approximately for $\alpha$ slightly below $1$. Standardization strives 
to garuantee equal justice for features on different scales.
Standardization works, however, contrary to the zero-sum idea: thanks to the zero-sum constraint, 
the fitting process and the final model both are invariant under sample-wise shifts on a subset of 
features if we set the zero-sum weights accordingly. Standardization, in contrast, actively changes 
the cost function, which probably results in a different model. Consequently, one should stick to 
the default value $v_j = 1$ for the features for which we wish to have shift-invariance; these 
are usually the gene expression levels, which are on comparable scales anyway.

\subsubsection{Random forests}

More precisely, we talk about classification random forests, hence $Y$ is binary in this subsection 
with $\im(Y) \subset \{ \pm 1 \}$, where $1$ still encodes the positive class. When we say random 
forests in this thesis, we mean classification 
random forests. Random forests are an ensemble of classification trees -- in short: 
trees -- and aggregate the classification of every constituent tree.

\paragraph{Trees}
A tree is a simple function
\begin{align}
    T = \sum_{i=m}^M c_m \chi_{R_m}: \RR^p \to \{0, 1\}
\end{align}
with $c_m \in \{\pm 1\}$ and the $R_m$ being disjoint rectangle sets, i.e. $R_m = \prod_{j=1}^p 
(a_j, b_j]$ for $a_j, b_j \in \RR \cup \{ \pm \infty \}$. It aims to predict the conditional 
majority class 
\begin{align}\label{eq:rf-major}
    \sgn(E(Y \cond X)) = T(X);
\end{align}
$\sgn$ the signum function.

How can we use our samples $(x_i, y_i)$, $i = 1, \ldots, n$, to learn a tree $T$? While we can 
trivially fulfill eq. (\ref{eq:rf-major}) exactly for the samples, this would only lead to terribly 
overfit trees. Instead, we confine ourselves with simpler, rougher trees that will make errors on 
training samples, but generalize better. 
In the algorithm further below, we will only need to calculate an error for the samples inside a 
rectangle set $R \subset \RR^n$. Let $n(R) = |\{ 1 \leq i \leq n: x_i \in R \}|$ be the number of 
samples in $R$ and
\begin{align}
    \hat{p}_R = \frac{|\{ 1 \leq i \leq n: x_i \in R, y_i = 1 \}|}{n(R)}
\end{align}
be the proportion of samples in $R$ with positive outcome. The error measure -- in the context of 
trees usually termed impurity measure -- we use in this thesis
is the Gini impurity $Q(R) = 2 \hat{p}_R (1 - \hat{p}_R)$; it is low for pure $R$ dominated by 
samples with either positive or negative outcome and grows quadratically as the outcome of the 
samples in $R$ gets more and more imbalanced. The impurity measure is a hyperparameter: other 
options include the misclassification error and the cross entropy. To govern the complexity $T$,
we demand that every rectangle set $R_m$ contain at least $n_\text{min}$ samples. Even this 
constraint still leaves with a computationally infeasible number of possible trees, and this is 
why we fit $T$ with a greedy algorithm: it recursively partitions every rectangle set along a 
feature in two subsets in such a way that the impurity measure gets minimal until every resulting 
rectangle set comprises less than $n_\text{min}$ samples. Formalizing binary partioning, for a 
rectangle set $R$, a feature $j \in \{1, \ldots, p\}$ and split point $s \in \RR$, we define 
the pair of half planes
\begin{align}
    R^{(1)}_{j, s} = \{x \in R: x_j \leq s\} \text{ and } R^{(2)}_{j, s} = \{x \in R: x_j > s\}.
\end{align}
With respect to the impurity measure, there are only finitely many split points of interest since 
we only have finitely many samples, and hence we need to consider no more than $n \cdot p$ when 
considering $R$ for partioning. All of this allows us to compactly describe a procedure to grow a 
tree in Alg. \ref{alg:tree}.
\input{algos/tree.tex}

\paragraph{Forests}
Trees are known to be notoriously noisy, meaning if we fix $x \in \RR^p$ the variance 
$V_\mathbf{z}(T(x))$ is quite high; the index $\mathbf{z}$ makes explicit that we consider $T(x)$ as 
a function -- or better: random variable -- of i.i.d. drawing training samples $\mathbf{z}$ (and 
then fitting a tree to them according to Alg. \ref{alg:tree}). On the other hand, if grown deep,
trees have pretty low bias, meaning for fixed $x \in \RR^p$ the expectation $E_\mathbf{z}(T(x))$ in 
most cases is exactly what we want, $\sgn(E(Y \cond x))$. High variance and low bias makes trees 
ideal candidates for a method called bagging, which averages the predictions of many noisy, 
approximately unbiased models with the goal to reduce their variance. Given trees $T_b$, $b = 1, 
\ldots, B$, we denote the new model by 
\begin{align}
    \overline{T}(x) = \frac{1}{B} \sum_{b=1}^B T_b(x).
\end{align}

Since the expected value is linear, we have 


\subsection{Nested models}\label{subsec:nested-models}

\section{Software}