\chapter{Discussion} \label{chap:discussion}

The previous chapter showed that our best models can significantly outperform the IPI in both 
a high-risk and representative-risk setting. Moreover, we demonstrated that can reliably validate 
our trained models and pick a high-performing one if we restrict the methods from chapter 
\ref{chap:methods} appropriately.

\section{Heuristics for candidate models}

Key to ensuring realistic validated errors is excluding those candidate models from $H$ that sneak 
in a validated error considerably below their test error. In this thesis, we developed a series 
of heuristics that aim to do precisely that.

\paragraph{Nesting models}
We have seen that we should nest models predicting from hundreds or even thousands of 
gene-expression levels together with more features into another model with care. Our way to do 
this, which uses the cross-validated predictions of the early model to train the late model, 
delivers way too optimistic validated errors for the late model. The early model, usually involving 
much more features than training samples, has plenty of freedom to overfit and it will exploit this 
freedom when we tune its hyperparameters in cross-validation. The algorithm of the late model then 
recognizes the output of the early model as a very predictive features and is misled to make it 
a prominent feature in the final model. On independent test data, the input for the late model 
systematically differs from the training, causing it to generalize poorly.

Instead, we need to present the late model training data identically distributed to the test 
data. An easy and successful way to this is to not train and tune the early model ourselves, but have 
other people do this on an independent data set. In other words, we use already existent signatures. 
Specifically, we did this with cell of origin and the Lamis. The Lamis signature fulfills the 
zero-sum constraint such that transferring it to a new data set with gene-expression levels 
measured under another protocol only results in a constant shift. When using the output of such 
a signature as a feature for one of our models, we have two options regarding its format.
First, we can threshold the signature -- as with cell of origin into ABC and GCB or the Lamis into 
a high and low group --, thereby rendering the signature protocol-independent and ready to input 
into any late model; this, of course, also works with violated zero-sum constaint and 
$m^*_\text{Schmitz}$ from section \ref{sec:inter-trial} 
does this quite successfully. Second, we can leave the signature output untouched and provide it as a 
continuous feature to a GLM, for whose linear predictor a constant shift in an input feature results 
in constant shift of its output. The most convincing model of this thesis, $m^*_\text{Reddy}$ does 
this with the Lamis score.

\paragraph{Gene-expression levels}
As a result, we refrain from incorporating gene-expression levels directly into the models trained 
by us, but only use their valuable information condensed into the output of alredy-existent models. 
With the curse of high dimensions gone, we have observed much more trustworthy validated errors. 
Furthermore, this reduces the training time and enables us to use more complex models.

\paragraph{More heuristics}

The inter-trial experiments suggest that model performance benefits from using a lower training 
survival cutoff $T$. Random forests, whose OOB predictions proved to yield a very accurate estimate 
of the test error, fare worse than GLMs; unlike GLMs, they cannot deal with systemically shifted 
features, which makes them unsuited to handle the continuous output of gene-expression signatures
across protocols.

\section{Increasing the sample size}

In addition to restricting the hyperparameter-tuple space $H$ carefully, we raised the number of 
samples by combining the three data sets. We hoped to get both more representative train and test 
cohorts with less systematic differences and hence turn validated errors into better estimates 
for test errors. Whatever the reason, in the latter we succeeded. To combine the data, we had to 
make sacrifices: we discarded the features that are 
not present in all threee data -- and this included valuable features like MYC translocations -- 
and we often trained models to predict PFS and then tested them on OS or vice versa. Still, our 
best validated models defied this together with systematic, especially technological and prognostic 
differences between the data sets and, at roughly \num{15}\% prevalence, outperformed the IPI 
significantly on the prospective Staiger data.

In the inter-trial experiments, with more samples in the test cohort, we could lower the prevalence 
of our selected models to \num{10}\%, drastically raise the precision and still retain enough 
statistical power to significantly defeat the IPI in even more cases. For the best validated models, 
we have seen an overwhelmingly monotonic relationship between the prevalence and the precision, 
which made the \num{15}\%-quantile or -- even better -- \num{10}\%-quantile of the model output a 
near-optimal threshold, respectively. 

The results of this thesis strongly suggest that sample size is a crucial factor 
for our problem. Furthermore, we should be ready to make sacrifices -- less features, 
even a different kind of the response -- to increase it. 

\section{The future of MMML-Predict}

With $m^*_\text{Schmitz}$ and even more $m^*_\text{Reddy}$, this thesis presents two 
models that meet all requirements of MMML-Predict: a prevalence of at least \num{10}\% and a 
precision above \num{50}\% and significantly above the IPI on an independent, prospective data set. 

And still, every percentage point we gain in prevalence will convince more clinicians, researchers 
and pharmaceutical companies to pay attention to the high-risk group identified by the MMML-Predictor.
Every percentage point we gain in precision will not just spur interest of the above people and 
companies, but will also convince more patients to let the MMML-Predictor guide their treatment and 
will avoid both unnecessary and failed therapies. This thesis did not suggest that model performance 
has already saturated, so we can go for even more in the future.

How might we do this? This thesis demonstrated that we can use data from already-existent trials, 
even if they are not prospective and differ technologically, to train and validated a model and 
then successfully test it on data from a prospective trial as MMML-Predict is about to conduct one.
In the intra-trial experiments, we saw validation and test performance out of touch and presume 
that a too small, therefore non-representative and systematically differing train and test cohorts 
could have played a key role in this. For the final MMML-Predictor, one might thus consider 
using more than \num{100} of the \num{300} samples registered for MMML-Predict, maybe even all 
\num{300}, for a sufficiently large, statistially more powerful test cohort. One might combine 
a series of available data sets into a big training cohort. With combining data sets come two 
caveats. 

First, we need to intersect over the sets of features, which amounts to discarding plenty 
of features. Nevertheless, many modern DLBCL data sets include gender, age, the five thresholded 
IPI features as clinical and gene-expression levels as molecular features. More modern data sets 
also hold the double and triple hit status, which refers to the simultaneous translocation of MYC
and either BCL2 or BCL6, or MYC, BCL2 and BCL6, respectively. From gene-expression 
features, we can calculate already-existent molecular signatures as well as the double-expressor and 
triple-expressor, which describe the phenomena anaogous to double- and triple-hit status with 
overexpression instead of translocation of the three involved genes.

Second, we need to take care of batch effects. Among the features mentioned above, this concerns 
the gene-expression levels. The first option is to get rid of protocol effects by once and for all
thresholding the output of molecular signatures. Most signatures come with canonical thresholds, 
such as the group for the LAMIS, and we can calculate them on the respective data set even before 
combining. In this case, we can deploy quite any model as we no longer need to fight the curse 
of high dimensions or batch effects. The second option is to only apply zero-sum signatures (or 
stand)